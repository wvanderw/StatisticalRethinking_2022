[["index.html", "Rethinking Companion Chapter 1 The Golem of Prague 1.1 Statistical golems 1.2 Statistical Rethinking 1.3 Tools for golem engineering 1.4 Summary Session Info", " Rethinking Companion Wade VanderWright 2022-09-29 Chapter 1 The Golem of Prague This is a companion book written in Markdown for McElreath’s Statistical Rethinking (2020). You can set up your R console by running: install.packages(c(&quot;coda&quot;,&quot;mvtnorm&quot;,&quot;devtools&quot;,&quot;dagitty&quot;)) library(devtools) devtools::install_github(&quot;rmcelreath/rethinking&quot;) 1.1 Statistical golems The Golem of Prague and statistical golems (models) are powerful but lack wisdom. As McElreath tells us, there are many kinds of golems and figuring out how to build the one you need to carry out the task at hand can be tricky. Figure 1.1 In addition, novel research often requires novel methods and the researchers may have to stray from the common tests to engineer their own golems. 1.2 Statistical Rethinking A lot can go wrong with statistical inference, and this is one reason that beginners are so anxious about it. When the goal is to choose a pre-made test from a flowchart, then the anxiety can mount as one worries about choosing the “correct” test. More work is needed to ensure researchers understand all the moving parts of their golems and how to interpret their results. 1.2.1 What are we trying to do with the golems? The popular belief is that we need to create models that use statistical means to test the null hypothesis. Two reasons why deductive falsification doesn’t work: Hypotheses are not models. The relations among hypothese and different kinds of models are complex. Many models correspond to the same hypothesis, and many hypotheses corresponf to asingle model. This makes strict falsification impossible. All models are false, but some are useful. Figure 1.2 Two opposing hypothesis for evolutionary change: H0: Neutral theory (random mutation and drift) H1: Natural selection (fitness leads to observed change) Process models for each hypothesis: P0a: steady state in time (null) P0b: fluctuations in population size through time P1a: selection favours the same alleles through time P1b: selection preference fluctuates through time (different alleles) Statistical Models: Mi: unique to P0b Mii: Power law in the data (frequency) shared expectation of P0a and P1b Miii: unique to P1a Note that all process models contain time, solidifying directionality Measurements matter. Even when we think the data falsify a model, another observer will debate our methods and measures. They don’t trust the data. Sometime sthey are right. The colour of swans Before Australia was discovered, all swans were white and no number of observations could prove this fact to be true. H0: All swans are white Australia had black swans, which instantly makes H0 false. Remember, observations are prone to error and hypotheses are quantitative rather than discrete. “At the edges of scientific knowledge, the ability to measure a hypothetical phenomenon is often in question as much as the phenomenon itself.” 1.3 Tools for golem engineering You’ll wreck Prague eventually, you just need to notice the destruction. We want our models to be able to design inquiry, extract information from data, and make predictions. To do this we will need: Bayesian data analysis Model comparison Multilevel Models Graphical causal models 1.3.1 Bayesian data analysis Bayesian data analysis takes questions in the form of a model and produces logical probability distributions of the answer. This represents plausibility. 1.3.2 Model comparison and predictions Model comparison is often thought of in terms of ‘which model will make the best predictions?’ Two tools for this are Cross-validation and Information Criteria. Complex models usually make worse predictions than simple ones due to overfitting. The smarter the golem, the dumber its predictions. Fitting is easy; prediction is hard. 1.3.3 Multilevel models 1.3.4 Graphical causal models 1.4 Summary Session Info sessionInfo() ## R version 4.1.1 (2021-08-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.26 png_0.1-7 digest_0.6.29 R6_2.5.1 ## [5] jsonlite_1.8.0 magrittr_2.0.3 evaluate_0.14 highr_0.9 ## [9] stringi_1.7.3 rlang_1.0.6 cli_3.4.1 rstudioapi_0.13 ## [13] jquerylib_0.1.4 bslib_0.3.1 rmarkdown_2.14 tools_4.1.1 ## [17] stringr_1.4.0 xfun_0.30 yaml_2.2.1 fastmap_1.1.0 ## [21] compiler_4.1.1 htmltools_0.5.2 knitr_1.33 sass_0.4.0 "],["small-worlds-and-large-worlds.html", "Chapter 2 Small worlds and large worlds 2.1 The garden of forking data 2.2 Building a model 2.3 Components of the model 2.4 Maiking the model go", " Chapter 2 Small worlds and large worlds Every model has two parts: small world and large world. The small world is within the model itself and the large world is the broader world we want the model to be applied to. In the small world, everything is defined and there isn’t much room for pure surprises. The large world has more opportunities for unforeseen events and because the small world is an incomplete representation of the large world, mistakes are expected. The goal is to create small worlds (models) that approximate reality so that they perform well in the large world. 2.1 The garden of forking data Bayesian inference is the counting and comparing of possibilities. At each point where a ‘decision’ may split the path, bayesian inference evaluates each path and eliminates the paths that are not congruent with the data fed into the model. 2.1.1 Counting possibilities Marble Example There is a bag of four marbles of two colours (blue and white). This means that there could be 5 possibilities (conjectures); 4:0 white, 3:1 white, 2:2 split, 3:1 blue, and 4:0 blue. A sequence of three marbles is pulled from the bag, one at a time, and returned to the bag (repeated sampling) We get blue, white, blue. Considering a 3:1 white scenario, on the first draw you could get a blue marble or three white marble draws Expanding out one more draw (layer) we can expect the same possibilities because the first marble is replaced before the second draw Expanding one more time gives us the final garden of 64 possibilities (43; 4 marbles with 3 draws) Now recall our draws were blue, white, blue so we can trim the paths that are not congruent with the draws We can also trim other possibilities like all white marbles or all blue marbles because we drew both colours from the bag. Putting our 3:1 white, 2:2 split, and 3:1 blue possibilities together would look something like this You can see that there are different numbers of unique paths to get our observed result 3:1 white has 3 paths 2:2 split has 8 paths 3:1 blue has 9 paths We will call these counts our priors. 2.1.2 Combining other information Suppose we make another marble draw and it is blue. We then count the ways each of our marble possibilities could create this new result 3:1 white has 1 path 2:2 split has 2 paths 3:1 blue has 3 paths Mutiplying by the prior counts gives us: 3:1 white has (3x1) 3 paths 2:2 split has (8x2) 16 paths 3:1 blue has (9x3) 27 paths and suggests that our 3:1 blue possibility is more plausible with the new information. Note that prior data and new data don’t have to be of the same type If we knew that the marble factory made the bags of marbles at fixed rates (i.e. there are 3x more 3:1 white bags as there are 3:1 blue bags and 2x as many 2:2 split bags than 3:1 blue bags) we could update our prior knowledge 3:1 white has 3 paths x 3 factory rate = 9 2:2 split has 16 paths x 2 factory rate = 32 3:1 blue has 27 paths x 1 factory rate = 27 Now the 2:2 split bag seems to be the most plausible outcome (by a small margin) 2.1.3 From counts to probability To avoid observation counts from getting quickly out of hand (over a million possible sequences after 10 data points) we need to collapse the information in a way that is easy to manipulate with the data. Continuing our marble example The plausibility of 3:1 white after seeing blue, white, blue is proportional to the ways 3:1 white can produce blue, white, blue * prior plausibility of 3:1 white. In other words if \\(p\\) is the proportion of blue marbles then in a 3:1 white bag \\(p\\) = 0.25 (1/4) And if we call our data (blue, white, blue) \\(D~new~\\) we can write: The plausibility of \\(p\\) after \\(D~new~\\) \\(\\propto\\) ways \\(p\\) can produce \\(D~new~\\) \\(\\times\\) prior plausibility of \\(p\\) We then standardize the plausibility of \\(p\\) after \\(D~new~\\) by dividing by the sum of the products to make the sum of plausibility to equal 1 \\[\\begin{equation} \\text{The plausibility of } p \\text{ after } D~new~ = \\frac{\\text{ways } p \\text{ can produce } D~new~ \\times \\text{prior plausibility of } p}{\\text{sum of products}} \\end{equation}\\] If you recall our first count of the paths to obtain our observations we had: 3:1 white has 3 paths 2:2 split has 8 paths 3:1 blue has 9 paths To illustrate the standardization of plausibility in R: ways &lt;- c(3, 8, 9) ways/sum(ways) ## [1] 0.15 0.40 0.45 We can now think of these plausibilities as probabilities that sum to 1 New terms: Parameter : \\(p\\) or the conjectured proportion of blue marbles. (indexing ways to explain the data) Likelihood : The relative number of ways that a parameter value (\\(p\\)) can produce the data Prior probability : The prior plausibility of any \\(p\\) value Posterior probability : The new, updated plausibility of and \\(p\\) value 2.2 Building a model Globe example You have a globe and want to know how much surface is covered in water. You throw it in the air multiple times and when you catch it, what lies under your right index finger is recorded. The first nine throws are: W L W W W L W L W with W meaning water and L meaning land. The ratio of water to land is 6:3 (2:1). This will be our data Steps to design your model: Data story: Motivate the model by narrating how the data might arise Update: Educate your model by feeding it data Evaluate: All statistical models require supervision, leading to model revision 2.2.1 A data story You can be descriptive in your data story by defining associations that can be used to predict outcomes. Or you can have a causal story where some events produce others. Typically casual stories are also descriptive. A good place to start is restating the sampling process: The true proportion of water covering the globe is \\(p\\) A single toss of the globe has probability \\(p\\) of producing a water (W) observation. Land probability is \\(1 - p\\) Each toss is independent of the others 2.2.2 Bayesian updating A Bayesian model must start with one set of plausibilities for each possible scenario or Prior plausibilities. The model then updates the plausibilities using the input of the data and creates the Postierior plausibilties. For the globe example, the plausibility for each \\(p\\) value is set to be the same. So before the first draw, the model assumes an equal uniform plausibility for any proportion of water (\\(p\\)) shown as the dashed line below. After the first draw (W), the model updates its plausibility of water proportion to have a very unlikely chance that there is almost no water and a high chance there is a lot of water (solid line below) because there is no known land For each additional data point (toss) we see the model adjust its plausibility expectations. After the second toss (L), the model adjusts the plausibility of \\(p\\) to be highest at 0.5 or 1/2 as we have seen a proportion of 0.5 in the data so far As we add observations, W observations will shift the plausibility peak of \\(p\\) to the right and L observations will pull it back to the left Note that as observations are added, the height of the peak increases as fewer values of \\(p\\) gain plausibility 2.2.3 Evaluate To ensure that the model is behaving as it should to be applicable to the large world, it should be always be checked. The model’s certainty is not the same as accuracy. As you increase the amount of tosses of the globe the model will become more and more certain of the plausibility of a \\(p\\) value (a narrow and tall curve). This could be an artifact of the model and could look very different under another model. Also, be sure to supervise and critique your model’s work as it can’t supervise itself. 2.3 Components of the model Recall that we have counted a few different things already The number of ways each conjecture (mix of marbles) could produce an observation The accumulated number of ways each conjecture could produce the entire data The initial plausibility of each conjectured cause of the data Here are some components related to these things that will help us understand what is happening within the models we build 2.3.1 Variables Variables are the things we want to infer. They can be proportions, rates, even the data itself. In the globe example we had three variables \\(p\\), the proportion of water, our target of inference This value was ‘unobservable’ so it could be labled as a paramter Observed counts of water (W) Observed counts of land (L) 2.3.2 Definitions We must define our variables and how they relate to each other. 2.3.2.1 Observed variables To determine the plausibility of the data to give the target of inference (\\(p\\)), we must define a distribution function for each observed variable which we will call a likelihood. For the globe example we are only concerned with W and L as they are the only events that can occur for each toss. We also need to make assumptions: Each toss (sample) is independent of the others The probability of getting W is the same for each sample (allowing re-sampling) For instances that are either TRUE or FALSE (0 or 1) we can use something called the binomial distribution. (Also called the ‘coin tossing’ distribution). The probability of observing can be expressed as: \\(\\text{Pr}(W, L|p) = \\frac{(W + L)! }{W!L!} p^{W}(1 - p)^{L}\\) In words: The counts of ‘water’ (W) and ‘land’ (L) are distributed binomially with probability p of ‘water’ for each toss. To calculate the likelihood in R you can use the dbinom function dbinom(6, size = 9, prob = 0.5) ## [1] 0.1640625 Recall we had 6 W’s from 9 tosses. Here \\(p\\) is held at 0.5 but you can play around with this to see how the likelihood changes. Hint: plot(dbinom(6, size = 9, prob = seq(from = 0, to = 1, by = 0.1))) 2.3.2.2 Unobserved variables Because we do not observe the \\(p\\) (probability of sampling water) directly, we would define it as a parameter. There are other frequent questions that models will try to answer with parameters such as: - What is the differece between group A and group B? - How strong is the association between group and outcome? - Does the effect of the grouping vary with a covariate? - How much variation is there among the groups? Each parameter of your model must have its own distribution of plausibility that you provide the model before it runs. These are your priors. Flat priors are common (as in figure X above) but are hardly representative of real world observations. If you don’t have good prior information to help set your prior assumptions of your paramters, then you should try a variety of priors to see how it affects your model’s performance. 2.3.3 A model is born Now we can write our model as: \\(W \\text{ ~ } \\text{Binomial}(N,p)\\) With the prior for \\(p\\) as: \\(p \\text{ ~ } \\text{Uniform}(0,1)\\) So we can say that the amount of water on the globe is a function with the binomial distribution where \\(N = W + L\\) and the probability of water (\\(p\\)) is a uniform distribution ranging from 0 to 1. 2.4 Maiking the model go read up to PDF page 59 (grid approx) ###grid approx #define grid p_grid &lt;- seq(from = 0, to = 1, length.out = 20) #define prior prior &lt;- rep(1,20) #compute likelihood at each value in the grid likelihood &lt;- dbinom(6 , size = 9, prob = p_grid) #compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior #standardize posterior so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid, posterior, type = &#39;b&#39;, xlab = &quot;probability of water&quot;, ylab = &quot;posterior probability&quot;) mtext(&quot;20 points&quot;) 5 points (left side of figure 2.7) #define grid p_grid &lt;- seq(from = 0, to = 1, length.out = 5) #define prior prior &lt;- rep(1,5) #compute likelihood at each value in the grid likelihood &lt;- dbinom(6 , size = 9, prob = p_grid) #compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior #standardize posterior so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid, posterior, type = &#39;b&#39;, xlab = &quot;probability of water&quot;, ylab = &quot;posterior probability&quot;) mtext(&quot;5 points&quot;) 100 points #define grid p_grid &lt;- seq(from = 0, to = 1, length.out = 100) #define prior prior &lt;- rep(1,100) #compute likelihood at each value in the grid likelihood &lt;- dbinom(6 , size = 9, prob = p_grid) #compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior #standardize posterior so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid, posterior, type = &#39;b&#39;, xlab = &quot;probability of water&quot;, ylab = &quot;posterior probability&quot;) mtext(&quot;20 points&quot;) 1000 points #define grid p_grid &lt;- seq(from = 0, to = 1, length.out = 1000) #define prior prior &lt;- rep(1,1000) #compute likelihood at each value in the grid likelihood &lt;- dbinom(6 , size = 9, prob = p_grid) #compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior #standardize posterior so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid, posterior, type = &#39;b&#39;, xlab = &quot;probability of water&quot;, ylab = &quot;posterior probability&quot;) mtext(&quot;20 points&quot;) Changing priors (recreating fig 2.6) #define grid p_grid &lt;- seq(from = 0, to = 1, length.out = 20) #define new priors prior &lt;- ifelse(p_grid &lt; 0.5, 0, 1) #compute likelihood at each value in the grid likelihood &lt;- dbinom(6 , size = 9, prob = p_grid) #compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior #standardize posterior so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid, posterior, type = &#39;b&#39;, xlab = &quot;probability of water&quot;, ylab = &quot;posterior probability&quot;) mtext(&quot;20 points&quot;) #define grid p_grid &lt;- seq(from = 0, to = 1, length.out = 20) prior &lt;- exp(-5*abs(p_grid - 0.5)) #compute likelihood at each value in the grid likelihood &lt;- dbinom(6 , size = 9, prob = p_grid) #compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior #standardize posterior so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid, posterior, type = &#39;b&#39;, xlab = &quot;probability of water&quot;, ylab = &quot;posterior probability&quot;) mtext(&quot;20 points&quot;) 2.4.1 Quadratic Nine tosses (n = 9) library(rethinking) ## Loading required package: rstan ## Warning: package &#39;rstan&#39; was built under R version 4.1.2 ## Loading required package: StanHeaders ## Loading required package: ggplot2 ## Warning: package &#39;ggplot2&#39; was built under R version 4.1.2 ## rstan (Version 2.21.7, GitRev: 2e1f913d3ca3) ## For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()). ## To avoid recompilation of unchanged Stan programs, we recommend calling ## rstan_options(auto_write = TRUE) ## Loading required package: cmdstanr ## This is cmdstanr version 0.5.3 ## - CmdStanR documentation and vignettes: mc-stan.org/cmdstanr ## - CmdStan path: /Users/DulvyLab/Desktop/.cmdstan/cmdstan-2.30.1 ## - CmdStan version: 2.30.1 ## Loading required package: parallel ## rethinking (Version 2.21) ## ## Attaching package: &#39;rethinking&#39; ## The following object is masked from &#39;package:rstan&#39;: ## ## stan ## The following object is masked from &#39;package:stats&#39;: ## ## rstudent globe.qa &lt;- quap( alist( W ~ dbinom(W+L, p) , #binomial likelihood p ~ dunif(0,1) #uniform prior ) , data = list(W=6, L=3) ) #display the summary of the quadratic approximation precis(globe.qa) ## mean sd 5.5% 94.5% ## p 0.666667 0.1571337 0.4155371 0.917797 plot analytical vs. the quadratic approximation #analytical calculation W &lt;- 6 L &lt;- 3 curve( dbeta(x , W+1, L+1), from = 0, to = 1) #quadratic calculation curve( dnorm(x, 0.67, 0.16), lty = 2, add = TRUE) compare 9 tosses to 18 and 36 (via Kurs, fig 2.8) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(ggplot2) library(stringr) n_grid &lt;- 100 tibble(p_grid = seq(from = 0, to = 1, length.out = n_grid) %&gt;% rep(., times = 3), prior = 1, w = rep(c(6, 12, 24), each = n_grid), n = rep(c(9, 18, 36), each = n_grid), m = .67, s = rep(c(.16, .11, .08), each = n_grid)) %&gt;% mutate(likelihood = dbinom(w, size = n, prob = p_grid)) %&gt;% mutate(unstd_grid_posterior = likelihood * prior, unstd_quad_posterior = dnorm(p_grid, m, s)) %&gt;% group_by(w) %&gt;% mutate(grid_posterior = unstd_grid_posterior / sum(unstd_grid_posterior), quad_posterior = unstd_quad_posterior / sum(unstd_quad_posterior), n = str_c(&quot;n = &quot;, n)) %&gt;% mutate(n = factor(n, levels = c(&quot;n = 9&quot;, &quot;n = 18&quot;, &quot;n = 36&quot;))) %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = grid_posterior), color = &quot;blue&quot;) + geom_line(aes(y = quad_posterior), color = &quot;black&quot;) + labs(x = &quot;proportion water&quot;, y = &quot;density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~n, scales = &quot;free&quot;) 2.4.2 MCMC n_samples &lt;- 1000 p &lt;- rep(NA, n_samples) p[1] &lt;- 0.5 W &lt;- 6 L &lt;- 3 for(i in 2:n_samples){ p_new &lt;- rnorm(1, p[i-1], 0.1) if(p_new &lt; 0) p_new &lt;- abs(p_new) if(p_new &gt; 1) p_new &lt;- 2 - p_new q0 &lt;- dbinom(W, W+L, p[i-1]) q1 &lt;- dbinom(W, W+L, p_new) p[i] &lt;- ifelse(runif(1) &lt; q1/q0, p_new, p[i-1]) } #dens(p, xlim=c(0,1)) d &lt;- density(p) plot(d) curve(dbeta(x, W+1, L+1), lty = 2, add = TRUE) #quadratic library(brms) ## Loading required package: Rcpp ## Warning: package &#39;Rcpp&#39; was built under R version 4.1.2 ## Loading &#39;brms&#39; package (version 2.16.3). Useful instructions ## can be found by typing help(&#39;brms&#39;). A more detailed introduction ## to the package is available through vignette(&#39;brms_overview&#39;). ## ## Attaching package: &#39;brms&#39; ## The following objects are masked from &#39;package:rethinking&#39;: ## ## LOO, stancode, WAIC ## The following object is masked from &#39;package:rstan&#39;: ## ## loo ## The following object is masked from &#39;package:stats&#39;: ## ## ar globe_qa_brms &lt;- brm(data = list(w = 24), family = binomial(link = &quot;identity&quot;), w | trials(36) ~ 1, prior(beta(1, 1), class = Intercept), iter = 4000, warmup = 1000, control = list(adapt_delta = .9), seed = 4) ## Compiling Stan program... ## Start sampling ## ## SAMPLING FOR MODEL &#39;32a7f51a2d1717ef80875b648bc79f41&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 2.1e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 1: Iteration: 400 / 4000 [ 10%] (Warmup) ## Chain 1: Iteration: 800 / 4000 [ 20%] (Warmup) ## Chain 1: Iteration: 1001 / 4000 [ 25%] (Sampling) ## Chain 1: Iteration: 1400 / 4000 [ 35%] (Sampling) ## Chain 1: Iteration: 1800 / 4000 [ 45%] (Sampling) ## Chain 1: Iteration: 2200 / 4000 [ 55%] (Sampling) ## Chain 1: Iteration: 2600 / 4000 [ 65%] (Sampling) ## Chain 1: Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 1: Iteration: 3400 / 4000 [ 85%] (Sampling) ## Chain 1: Iteration: 3800 / 4000 [ 95%] (Sampling) ## Chain 1: Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.017364 seconds (Warm-up) ## Chain 1: 0.049054 seconds (Sampling) ## Chain 1: 0.066418 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;32a7f51a2d1717ef80875b648bc79f41&#39; NOW (CHAIN 2). ## Chain 2: Rejecting initial value: ## Chain 2: Error evaluating the log probability at the initial value. ## Chain 2: Exception: binomial_lpmf: Probability parameter[1] is -1.16817, but must be in the interval [0, 1] (in &#39;modeld7136c0fed_32a7f51a2d1717ef80875b648bc79f41&#39; at line 22) ## ## Chain 2: Rejecting initial value: ## Chain 2: Error evaluating the log probability at the initial value. ## Chain 2: Exception: binomial_lpmf: Probability parameter[1] is 1.21464, but must be in the interval [0, 1] (in &#39;modeld7136c0fed_32a7f51a2d1717ef80875b648bc79f41&#39; at line 22) ## ## Chain 2: ## Chain 2: Gradient evaluation took 7e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 2: Iteration: 400 / 4000 [ 10%] (Warmup) ## Chain 2: Iteration: 800 / 4000 [ 20%] (Warmup) ## Chain 2: Iteration: 1001 / 4000 [ 25%] (Sampling) ## Chain 2: Iteration: 1400 / 4000 [ 35%] (Sampling) ## Chain 2: Iteration: 1800 / 4000 [ 45%] (Sampling) ## Chain 2: Iteration: 2200 / 4000 [ 55%] (Sampling) ## Chain 2: Iteration: 2600 / 4000 [ 65%] (Sampling) ## Chain 2: Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 2: Iteration: 3400 / 4000 [ 85%] (Sampling) ## Chain 2: Iteration: 3800 / 4000 [ 95%] (Sampling) ## Chain 2: Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.01714 seconds (Warm-up) ## Chain 2: 0.045432 seconds (Sampling) ## Chain 2: 0.062572 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;32a7f51a2d1717ef80875b648bc79f41&#39; NOW (CHAIN 3). ## Chain 3: Rejecting initial value: ## Chain 3: Error evaluating the log probability at the initial value. ## Chain 3: Exception: binomial_lpmf: Probability parameter[1] is 1.3474, but must be in the interval [0, 1] (in &#39;modeld7136c0fed_32a7f51a2d1717ef80875b648bc79f41&#39; at line 22) ## ## Chain 3: Rejecting initial value: ## Chain 3: Error evaluating the log probability at the initial value. ## Chain 3: Exception: binomial_lpmf: Probability parameter[1] is -0.920633, but must be in the interval [0, 1] (in &#39;modeld7136c0fed_32a7f51a2d1717ef80875b648bc79f41&#39; at line 22) ## ## Chain 3: Rejecting initial value: ## Chain 3: Error evaluating the log probability at the initial value. ## Chain 3: Exception: binomial_lpmf: Probability parameter[1] is 1.24986, but must be in the interval [0, 1] (in &#39;modeld7136c0fed_32a7f51a2d1717ef80875b648bc79f41&#39; at line 22) ## ## Chain 3: ## Chain 3: Gradient evaluation took 7e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 3: Iteration: 400 / 4000 [ 10%] (Warmup) ## Chain 3: Iteration: 800 / 4000 [ 20%] (Warmup) ## Chain 3: Iteration: 1001 / 4000 [ 25%] (Sampling) ## Chain 3: Iteration: 1400 / 4000 [ 35%] (Sampling) ## Chain 3: Iteration: 1800 / 4000 [ 45%] (Sampling) ## Chain 3: Iteration: 2200 / 4000 [ 55%] (Sampling) ## Chain 3: Iteration: 2600 / 4000 [ 65%] (Sampling) ## Chain 3: Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 3: Iteration: 3400 / 4000 [ 85%] (Sampling) ## Chain 3: Iteration: 3800 / 4000 [ 95%] (Sampling) ## Chain 3: Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.01648 seconds (Warm-up) ## Chain 3: 0.051965 seconds (Sampling) ## Chain 3: 0.068445 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;32a7f51a2d1717ef80875b648bc79f41&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 1.2e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 4: Iteration: 400 / 4000 [ 10%] (Warmup) ## Chain 4: Iteration: 800 / 4000 [ 20%] (Warmup) ## Chain 4: Iteration: 1001 / 4000 [ 25%] (Sampling) ## Chain 4: Iteration: 1400 / 4000 [ 35%] (Sampling) ## Chain 4: Iteration: 1800 / 4000 [ 45%] (Sampling) ## Chain 4: Iteration: 2200 / 4000 [ 55%] (Sampling) ## Chain 4: Iteration: 2600 / 4000 [ 65%] (Sampling) ## Chain 4: Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 4: Iteration: 3400 / 4000 [ 85%] (Sampling) ## Chain 4: Iteration: 3800 / 4000 [ 95%] (Sampling) ## Chain 4: Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.016888 seconds (Warm-up) ## Chain 4: 0.043018 seconds (Sampling) ## Chain 4: 0.059906 seconds (Total) ## Chain 4: print(globe_qa_brms) ## Family: binomial ## Links: mu = identity ## Formula: w | trials(36) ~ 1 ## Data: list(w = 24) (Number of observations: 1) ## Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup draws = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.66 0.08 0.50 0.80 1.00 3569 4105 ## ## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). posterior_samples(globe_qa_brms) %&gt;% mutate(n = &quot;n = 36&quot;) %&gt;% ggplot(aes(x = b_Intercept)) + geom_density(fill = &quot;black&quot;) + labs(x = &quot;proportion water&quot;) + xlim(0, 1) + theme(panel.grid = element_blank()) + facet_wrap(~n) ## Warning: Method &#39;posterior_samples&#39; is deprecated. Please see ?as_draws for ## recommended alternatives. ## Select practice questions 2M1. Compute and plot grid approximations for the following: W, W, W #define grid p_grid &lt;- seq(from = 0, to = 1, length.out = 20) #define prior prior &lt;- rep(1,20) #compute likelihood at each value in the grid likelihood &lt;- dbinom(3 , size = 3, prob = p_grid) #compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior #standardize posterior so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid, posterior, type = &#39;b&#39;, xlab = &quot;probability of water&quot;, ylab = &quot;posterior probability&quot;) mtext(&quot;20 points&quot;) 2. W, W, W, L #define grid p_grid &lt;- seq(from = 0, to = 1, length.out = 20) #define prior prior &lt;- rep(1,20) #compute likelihood at each value in the grid likelihood &lt;- dbinom(3 , size = 4, prob = p_grid) #compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior #standardize posterior so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid, posterior, type = &#39;b&#39;, xlab = &quot;probability of water&quot;, ylab = &quot;posterior probability&quot;) mtext(&quot;20 points&quot;) L, W, W, L, W, W, W #define grid p_grid &lt;- seq(from = 0, to = 1, length.out = 20) #define prior prior &lt;- rep(1,20) #compute likelihood at each value in the grid likelihood &lt;- dbinom(5 , size = 7, prob = p_grid) #compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior #standardize posterior so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid, posterior, type = &#39;b&#39;, xlab = &quot;probability of water&quot;, ylab = &quot;posterior probability&quot;) mtext(&quot;20 points&quot;) 2M2. Compute the above again but with priors that change from 0 when p&lt;0.5 to a constant when p&gt;0.5. W, W, W #define grid p_grid &lt;- seq(from = 0, to = 1, length.out = 20) #define prior prior &lt;- ifelse(p_grid &lt; 0.5, 0, 3) #compute likelihood at each value in the grid likelihood &lt;- dbinom(3 , size = 3, prob = p_grid) #compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior #standardize posterior so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid, posterior, type = &#39;b&#39;, xlab = &quot;probability of water&quot;, ylab = &quot;posterior probability&quot;) mtext(&quot;20 points&quot;) 2. W, W, W, L #define grid p_grid &lt;- seq(from = 0, to = 1, length.out = 20) #define prior prior &lt;- ifelse(p_grid &lt; 0.5, 0, 3) #compute likelihood at each value in the grid likelihood &lt;- dbinom(3 , size = 4, prob = p_grid) #compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior #standardize posterior so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid, posterior, type = &#39;b&#39;, xlab = &quot;probability of water&quot;, ylab = &quot;posterior probability&quot;) mtext(&quot;20 points&quot;) L, W, W, L, W, W, W #define grid p_grid &lt;- seq(from = 0, to = 1, length.out = 20) #define prior prior &lt;- ifelse(p_grid &lt; 0.5, 0, 3) #compute likelihood at each value in the grid likelihood &lt;- dbinom(5 , size = 7, prob = p_grid) #compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior #standardize posterior so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid, posterior, type = &#39;b&#39;, xlab = &quot;probability of water&quot;, ylab = &quot;posterior probability&quot;) mtext(&quot;20 points&quot;) 2M4. Three cards have two sides that can be black or white. one card is B/B, one is B/W, and one is W/W. They are placed in a bag and shuffled with one drawn and placed on the table black side up. Show the probability of the other side of the card being black. # p = probability of black (initial 6 sides; 3W, 3B; p = 0.5) # W/W = 0 paths to observed black; p = 0 # B/W = 1 paths to observed black; p = 1/2 # B/B = 1 paths to observed black; p = 1 # W/W = 0 x 0 = 0 # B/W = 1 x 1/2 = 1/2 # B/B = 1 x 1 = 1 ways &lt;- c(0 ,0.5, 1) ways/sum(ways) ## [1] 0.0000000 0.3333333 0.6666667 2M5. A fourth card is added to the example above and it is B/B. Calculate the new probability of the other side of the drawn card being black. # p = 5/8 (eight card sides, 3 W, 5 B) # W/W = 0 paths to observed black; p = 0 # B/W = 1 path to observed black; p = 1/2 # B/B = 1 path to observed black; p = 1 # B/B = 1 path to observed black; p = 1 # W/W = 0 x 0 = 0 # B/W = 1 x 1/2 = 1/2 # B/B = 1 x 1 = 1 # B/B = 1 x 1 = 1 ; add two black cards together (2) ways &lt;- c(0 ,0.5, 2) ways/sum(ways) ## [1] 0.0 0.2 0.8 "],["sampling-the-imaginary.html", "Chapter 3 Sampling the imaginary 3.1 Sampling from a grid-approximate posterior 3.2 Sampling to summarize 3.3 Sampling to simulate prediction", " Chapter 3 Sampling the imaginary 3.0.1 Probabilities vs. Frequency counts Vampires in the population Probabilities There is a blood test that can correctly identify a vampire 95% of the time. Or mathematically, Pr(positive|vampire) = 0.95. 1% of the time the test gives a false positive or Pr(positive|mortal) = 0.01. Also, vampires are rare in the population only making up 0.1% of the population so, Pr(vampire) = 0.001 If someone tests poisitve, what is the probability that they are actually a vampire? Using Bayes’ theorem, Pr(vampire|positive) can be inverted as: \\[\\begin{equation} \\text{Pr(vampire|positive)} = \\frac{\\text{Pr(positive|vampire)} \\times \\text{Pr(vampire)} {\\text{Pr(positive)}} \\end{equation}\\] Here, Pr(positive) is the average probability of a positive test result or: \\begin{eqaution} = + (1 - ) \\end{equation} Pr_Positive_Vampire &lt;- 0.95 Pr_Positive_Mortal &lt;- 0.01 Pr_Vampire &lt;- 0.001 Pr_Positive &lt;- Pr_Positive_Vampire * Pr_Vampire + Pr_Positive_Mortal * (1 - Pr_Vampire) (Pr_Vampire_Positive &lt;- Pr_Positive_Vampire * Pr_Vampire / Pr_Positive) ## [1] 0.08683729 There is an 8.7% chance that a positive test result is actually a vampire. Frequency counts In 100,000 people, 100 are vampires of the 100 vampires, 95 will test positive of the 99,900 mortals, 999 will test positive Pr_Positive_Vampire &lt;- 95 / 100 Pr_Positive_Mortal &lt;- 999 / 99900 Pr_Vampire &lt;- 100 / 100000 Pr_Positive &lt;- 95 + 999 (Pr_Vampire_Positive &lt;- Pr_Positive_Vampire * 100 / Pr_Positive) ## [1] 0.08683729 OR: Pr_Positive_Vampire &lt;- 95 #positive results from vampires Pr_Positive &lt;- 95 + 999 #all positive results (Pr_Vampire_Positive &lt;- Pr_Positive_Vampire / Pr_Positive) ## [1] 0.08683729 3.1 Sampling from a grid-approximate posterior Let’s recreate the grid approximation for the globe tossing example: p_grid &lt;- seq(from = 0, to = 1, length.out = 1000) #create 1000 values between 0 and 1 prob_p &lt;- rep(1, 1000) #the uniform prior of 1 prob_data &lt;- dbinom(6, size = 9, prob = p_grid) #The observed tosses (data) posterior &lt;-prob_data*prob_p #calculate the posterior posterior &lt;-posterior/sum(posterior) #standardize by dividing by the sum Let’s pull some samples from our posterior (n = 10000) samples &lt;- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE) #notice replace set to true as there are only 1000 values in p_grid #find the mode of your samples getmode &lt;- function(x) { uniqv &lt;- unique(x) uniqv[which.max(tabulate(match(x, uniqv)))] } samples_mode &lt;- getmode(samples) And plot them plot(samples) #left panel of figure 3.1 abline(h = samples_mode, col = &#39;red&#39;) View the samples as a density plot(density(samples), main = &quot;Density of samples from posterior&quot;) polygon(density(samples), col = &#39;black&#39;, border = &#39;blue&#39;) abline(v = samples_mode, col = &#39;red&#39;) #library(rethinking) #dens(samples) 3.2 Sampling to summarize Common questions about your posterior: 1. intervals of defined boundaries 2. intervals of defined probability mass 3. point estimates 3.2.1 Intervals of defined boundaries What is the posterior probability that the proportion of water is &gt;0.5? sum(posterior[p_grid &lt; 0.5]) ## [1] 0.1718746 about 17% Doing the same using the samples from the posterior sum(samples &lt; 0.5) / 1e4 #divide by the number of samples you gathered ## [1] 0.1729 what about between 0.5 and 0.75? sum(samples &gt; 0.5 &amp; samples &lt; 0.75) / 1e4 ## [1] 0.6065 Recreating figure 3.2 (upper left panel) library(ggplot2) library(dplyr) df &lt;- tibble(p_grid, posterior) df %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = df %&gt;% filter(p_grid &lt; 0.5), aes(ymin = 0, ymax = posterior)) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) + theme_bw() upper right panel df %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = df %&gt;% filter(p_grid &lt; 0.75 &amp; p_grid &gt; 0.5), aes(ymin = 0, ymax = posterior)) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) + theme_bw() 3.2.2 intervals of defined mass finding the lower 80% of the probability mass using samples (q_80 &lt;- quantile(samples, 0.8)) ## 80% ## 0.7607608 finding the middle 80% (10-90%) (q_10_90 &lt;- quantile(samples, c(0.1, 0.9))) ## 10% 90% ## 0.4474474 0.8108108 Bottom panels to figure 3.2 lower left: df %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = df %&gt;% filter(p_grid &lt; q_80), aes(ymin = 0, ymax = posterior)) + annotate(geom = &#39;text&#39;, x = 0.25, y = 0.0025, label = &#39;lower 80%&#39;) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) + theme_bw() df %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + geom_ribbon(data = df %&gt;% filter(p_grid &gt; q_10_90[1] &amp; p_grid &lt; q_10_90[2]), aes(ymin = 0, ymax = posterior)) + annotate(geom = &#39;text&#39;, x = 0.25, y = 0.0025, label = &#39;middle 80%&#39;) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) + theme_bw() example of three tosses and three water observations: p_grid &lt;- seq(from = 0, to = 1, length.out = 1000) prior &lt;- rep(1, 1000) likelihood &lt;- dbinom(3, size = 3, prob = p_grid) posterior &lt;- likelihood * prior posterior &lt;- posterior / sum(posterior) samples &lt;- sample(p_grid, size = 1e4, replace = TRUE, prob = posterior) Getting the 50% interval quantile(samples, c(0.25, 0.75)) ## 25% 75% ## 0.7057057 0.9299299 #PI(samples, prob = 0.5) #rethinking package Tidybayes package: library(tidybayes) ## Warning: package &#39;tidybayes&#39; was built under R version 4.1.2 ## ## Attaching package: &#39;tidybayes&#39; ## The following objects are masked from &#39;package:brms&#39;: ## ## dstudent_t, pstudent_t, qstudent_t, rstudent_t median_qi(samples, .width = 0.5) ## y ymin ymax .width .point .interval ## 1 0.8418418 0.7057057 0.9299299 0.5 median qi Notice that all methods above give us the same interval from ~70 - ~93 With tidybayes, you can also look for multiple intervals at once: median_qi(samples, .width = c(0.5, 0.8, 0.99)) ## y ymin ymax .width .point .interval ## 1 0.8418418 0.7057057 0.9299299 0.50 median qi ## 2 0.8418418 0.5685686 0.9749750 0.80 median qi ## 3 0.8418418 0.2792743 0.9989990 0.99 median qi Finding the Highest Posterior Density Interval (HPDI) #HPDI(samples, prob = 0.5) #rethinking mode_hdi(samples, .width = 0.5) #tidybayes ## y ymin ymax .width .point .interval ## 1 0.9565123 0.8418418 1 0.5 mode hdi Note: you can get just the points of refernce by using qi() for quantiles and hdi() for highest density intervals. Useful for plotting recreate figure 3.3 left panel df &lt;- tibble(p_grid, posterior) df %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(data = df %&gt;% filter(p_grid &gt; qi(samples, .width = 0.5)[1] &amp; p_grid &lt; qi(samples, .width = 0.5)[2]), aes(ymin = 0, ymax = posterior)) + geom_line(aes(y = posterior)) + labs(subtitle = &#39;50% Percentile interval&#39;, x = &#39;proportion of water (p)&#39;, y = &#39;density&#39;)+ theme_bw() right panel df %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(data = df %&gt;% filter(p_grid &gt; hdi(samples, .width = 0.5)[1] &amp; p_grid &lt; hdi(samples, .width = 0.5)[2]), aes(ymin = 0, ymax = posterior)) + geom_line(aes(y = posterior)) + labs(subtitle = &#39;50% HPDI&#39;, x = &#39;proportion of water (p)&#39;, y = &#39;density&#39;)+ theme_bw() 3.2.3 Point estimates How to get a single useful (?) point estimate for your parameter. First option is the maximum a posteriori (MAP). p_grid[which.max(posterior)] ## [1] 1 With samples: Mode(samples) #tidybayes ## [1] 0.9565123 #chainmode(samples, adj = 0.01) #rethinking what about mean or median? mean(samples) ## [1] 0.801547 median(samples) ## [1] 0.8418418 Visualize the mean, median, and mode (figure 3.4) create a small data frame ( point_estimates &lt;- bind_rows( mean_qi(samples), median_qi(samples), mode_qi(samples) ) %&gt;% select(y, .point) %&gt;% mutate(x = y + c(-0.03, 0.03, -0.03), z = c(0.001, 0.0015, 0.0025)) ) ## y .point x z ## 1 0.8015470 mean 0.7715470 0.0010 ## 2 0.8418418 median 0.8718418 0.0015 ## 3 0.9565123 mode 0.9265123 0.0025 plot df %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(aes(ymin = 0, ymax = posterior), fill = &quot;grey75&quot;) + geom_vline(xintercept = point_estimates$y) + geom_text(data = point_estimates, aes(x = x, y = z, label = .point), angle = 90) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) + theme(panel.grid = element_blank()) How do we choose between the point estimates? Loss functions If we assume that p = 0.5 then the expected loss would be: sum(posterior * abs(0.5 - p_grid)) ## [1] 0.3128752 applying this method to all values of p_grid: loss &lt;- sapply(p_grid, function(d) sum(posterior * abs(d - p_grid))) Now find the p value with the lowest loss p_grid[which.min(loss)] ## [1] 0.8408408 Visualize the loss function min_loss_x &lt;- p_grid[which.min(loss)] min_loss_y &lt;- loss[which.min(loss)] df &lt;- tibble(p_grid, loss) df %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(aes(ymin = 0, ymax = loss), fill = &#39;grey75&#39;) + geom_point(aes(x = min_loss_x, y = min_loss_y), size = 3, shape = 21, color = &#39;blue&#39;) + labs(x = &#39;decision&#39;, y = &#39;expected proportional loss&#39;) + theme(panel.grid = element_blank()) 3.3 Sampling to simulate prediction McElreath’s 5 reasons for posterior simulation: 1. Model design - We can sample from both the posterior and the priors to see how the model behaves 2. Model checking - simulating implied observations to check the model fit 3. Software validation - To double check that the software is running as expected, it helps to simulate observations for a known model and try to recover the parameter values 4. Research design - you can test observations from your hypothesis to test your design. similar to power analysis 5. Forecasting - Estimates can be used to simulate new predictions, for new cases and future observations 3.3.1 Dummy data From the globe tossing example we can use the likelihood function to create dummy data \\[\\begin{equation} \\text{Pr}(W|N,p) = \\frac{N!}{ W!(N-W)!}p^W\\left(1-p\\right)^{N-W} \\end{equation}\\] If we had two tosses (N = 2), there are 3 possibilities: 0 W, 1W, 2W. So we can compute the probabilities of each with the p value set to 0.7 dbinom(0:2, size = 2, prob = 0.7) ## [1] 0.09 0.42 0.49 So we see a 9% chance for 0 W, 42% chance of 1 W, and 49% 2 W. So we can sample from the binomial distribution. For example: rbinom(1 , size = 2, prob = 0.7) ## [1] 2 This is a single random draw from the described distribution. You can also sample multiples: rbinom(10, size = 2, prob = 0.7) ## [1] 1 1 2 2 1 2 2 2 2 0 So we can create a large selection of random draws as dummy data and see if the 0s, 1s, and 2s appear in the same proportions as the probabilities above (9%, 42%, and 49%) dummy_w &lt;- rbinom(1e5, size = 2, prob = 0.7) table(dummy_w) / 1e5 ## dummy_w ## 0 1 2 ## 0.08972 0.41847 0.49181 Now, let’s update the tosses to match all the previous examples (N = 9). dummy_w &lt;- rbinom(1e5, size = 9, prob = 0.7) n &lt;- c(1:1e5) df_w &lt;- tibble(n, dummy_w) df_w %&gt;% ggplot(aes(x = dummy_w)) + geom_histogram(binwidth = 1) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 2)) + ylab(&quot;frequency&quot;) + theme(panel.grid = element_blank()) #simplehist(dummy_w, xlab = &#39;dummy water count&#39;) #rethinking 3.3.2 Model checking 3.3.2.1 Did the software work? There actually is no way to check if the software is working correctly. You just have to set yourself an acceptable amount of correspondence between the observations (data) and implied predictions. 3.3.2.2 Is the model adequate? We need to incorporate the model’s posterior distribution (and its uncertainty) with the implied predictions (and their uncertainty) to create a Posterior Predictive Distribution. To visulize this, we can recreate McElreath’s figure 3.6 below. # number of grid point in p_grid n &lt;- 1001 # number of W in 9 tosses n_w &lt;- 6 # number of tosses n_t &lt;- 9 # make a table that contains the p_grid, prior, and posterior df &lt;- tibble( p_grid = seq(from = 0, to = 1, length.out = n), #prior is still flat prior = 1) %&gt;% mutate(likelihood = dbinom(n_w, size = n_t, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior)/sum(likelihood*prior)) #visualize the posterior distribution with 9 p values to sample from df %&gt;% ggplot(aes(x = p_grid)) + geom_ribbon(aes(ymin = 0, ymax = posterior), color = &#39;grey70&#39;, fill = &#39;grey70&#39;) + geom_segment(data = . %&gt;% filter(p_grid %in% c(seq(from = 0.1, to = 0.9, by = 0.1), 3 / 10)), aes(xend = p_grid, y = 0, yend = posterior, size = posterior), color = &#39;grey35&#39;, show.legend = FALSE) + geom_point(data = . %&gt;% filter(p_grid %in% c(seq(from = 0.1, to = 0.9, by = 0.1), 3 / 10)), aes(y = posterior)) + annotate(geom = &#39;text&#39;, x = 0.08, y = 0.0025, label = &#39;Posterior probability&#39;) + scale_size_continuous(range = c(0,1)) + scale_x_continuous(&#39;probability of water&#39;, breaks = c(0:10) / 10) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) #show the sampling distributions for each p value above library(tidyr) ## ## Attaching package: &#39;tidyr&#39; ## The following object is masked from &#39;package:rstan&#39;: ## ## extract #number of simulated draws n_draws &lt;- 1e5 #simulation function simulate_binom &lt;- function(probability){ set.seed(11) #reproducible rbinom(n_draws, size = 9, prob = probability) } # make a table of simulated draws for each probability df_small &lt;- tibble(probability = seq(from = 0.1, to = 0.9, by = 0.1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom)) %&gt;% unnest(draws) %&gt;% mutate(label = str_c(&quot;p = &quot;, probability)) #create a histogram for each simulated p value df_small %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &#39;grey90&#39;, size = 1/10) + scale_x_continuous(NULL, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &#39;Sampling distributions&#39;) + #coord_cartesian(xlim = 0:9) + theme(panel.grid = element_blank()) + facet_wrap(~ label, ncol = 9) #show the newly calculated PPD #number of samples n_samples &lt;- 1e4 #make sure it can be replicated set.seed(11) # sample rows of the original dataframe and calculate a W value for each p_grid value samples &lt;- df %&gt;% sample_n(size = n_samples, weight = posterior, replace = TRUE) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9)) #plot the newly created PPD samples %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &#39;grey90&#39;, size = 1/10) + scale_x_continuous(&#39;number of water samples&#39;, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&#39;Posterior predictive distribution&#39;) + #coord_cartesian(xlim = 0:9, ylim = 0:3000) + theme(panel.grid = element_blank()) The advantage here is that the predictive distribution is still quite spread out compared to the observed data (w = 6). This is much more informative than if we were to just pick out the mode of the posterior and make implied predictions from that value. This would look like the sampling distribution of p = 0.6 above which would be overconfident. 3.3.3 Practice with brms Let’s create a PPD with brms Load the package library(brms) brms_3 &lt;- brm(data = list(w = 6), family = binomial(link = &#39;identity&#39;), w | trials(9) ~ 1, prior(beta(1, 1), class = Intercept), seed = 11, control = list(adapt_delta = 0.999)) ## Compiling Stan program... ## recompiling to avoid crashing R session ## Start sampling ## ## SAMPLING FOR MODEL &#39;32a7f51a2d1717ef80875b648bc79f41&#39; NOW (CHAIN 1). ## Chain 1: Rejecting initial value: ## Chain 1: Error evaluating the log probability at the initial value. ## Chain 1: Exception: binomial_lpmf: Probability parameter[1] is -0.232403, but must be in the interval [0, 1] (in &#39;modeld712023842a_32a7f51a2d1717ef80875b648bc79f41&#39; at line 22) ## ## Chain 1: ## Chain 1: Gradient evaluation took 2.1e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.037163 seconds (Warm-up) ## Chain 1: 0.048304 seconds (Sampling) ## Chain 1: 0.085467 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;32a7f51a2d1717ef80875b648bc79f41&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 1.8e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.039711 seconds (Warm-up) ## Chain 2: 0.061793 seconds (Sampling) ## Chain 2: 0.101504 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;32a7f51a2d1717ef80875b648bc79f41&#39; NOW (CHAIN 3). ## Chain 3: Rejecting initial value: ## Chain 3: Error evaluating the log probability at the initial value. ## Chain 3: Exception: binomial_lpmf: Probability parameter[1] is -0.794659, but must be in the interval [0, 1] (in &#39;modeld712023842a_32a7f51a2d1717ef80875b648bc79f41&#39; at line 22) ## ## Chain 3: ## Chain 3: Gradient evaluation took 1e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.039656 seconds (Warm-up) ## Chain 3: 0.035382 seconds (Sampling) ## Chain 3: 0.075038 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;32a7f51a2d1717ef80875b648bc79f41&#39; NOW (CHAIN 4). ## Chain 4: Rejecting initial value: ## Chain 4: Error evaluating the log probability at the initial value. ## Chain 4: Exception: binomial_lpmf: Probability parameter[1] is -0.0368002, but must be in the interval [0, 1] (in &#39;modeld712023842a_32a7f51a2d1717ef80875b648bc79f41&#39; at line 22) ## ## Chain 4: Rejecting initial value: ## Chain 4: Error evaluating the log probability at the initial value. ## Chain 4: Exception: binomial_lpmf: Probability parameter[1] is 1.49284, but must be in the interval [0, 1] (in &#39;modeld712023842a_32a7f51a2d1717ef80875b648bc79f41&#39; at line 22) ## ## Chain 4: ## Chain 4: Gradient evaluation took 1.2e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.039805 seconds (Warm-up) ## Chain 4: 0.027747 seconds (Sampling) ## Chain 4: 0.067552 seconds (Total) ## Chain 4: Posterior summary of the probability of w posterior_summary(brms_3)[&#39;b_Intercept&#39;, ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 0.63 0.14 0.34 0.88 Now we can sample draws with fitted() in the brms package from the posterior library(purrr) ## ## Attaching package: &#39;purrr&#39; ## The following object is masked from &#39;package:rethinking&#39;: ## ## map f &lt;- fitted(brms_3, summary = FALSE, scale = &#39;linear&#39;) %&gt;% as_tibble() %&gt;% set_names(&#39;p&#39;) ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0. ## Using compatibility `.name_repair`. glimpse(f) ## Rows: 4,000 ## Columns: 1 ## $ p &lt;dbl&gt; 0.6109911, 0.5994330, 0.6465488, 0.6851288, 0.5909901, 0.6042266, 0.… As a density: f %&gt;% ggplot(aes(x = p)) + geom_density(fill = &#39;grey50&#39;, color = &#39;grey50&#39;) + annotate(geom = &#39;text&#39;, x = 0.08, y = 2.5, label = &#39;Posterior probability&#39;) + scale_x_continuous(&#39;probability of water&#39;, breaks = c(0, 0.5, 1), limits = 0:1) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Now we can use this distribution to simulate samples #make reproducible set.seed(11) #simulate samples f &lt;- f %&gt;% mutate(w = rbinom(n(), size = n_t, prob = p)) #plot PPD f %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &#39;grey90&#39;, size = 1/10) + scale_x_continuous(&#39;number of water samples&#39;, breaks = seq(from = 0, to = 9, by =3)) + scale_y_continuous(NULL, breaks = NULL, limits = c(0, 1200)) + ggtitle(&#39;Posterior predictive distribution&#39;) + theme(panel.grid = element_blank()) "],["geocentric-models.html", "Chapter 4 Geocentric models 4.1 Why normal distributions are normal 4.2 A language for describing models 4.3 Gaussian model of height 4.4 Linear Prediction 4.5 Curves from lines", " Chapter 4 Geocentric models This chapter introduces linear regression as a Bayesian procedure. Under a probability interpretation, which is necessary for Bayesian work, linear regression uses a Gaussian (normal) distribution to describe our golem’s uncertainty about some measurement of interest. This type of model is simple, flexible, and commonplace. Like all statistical models, it is not universally useful. But linear regression has a strong claim to being foundational, in the sense that once you learn to build and interpret linear regression models, you can more easily move on to other types of regression which are less normal. &gt;p. 71 4.1 Why normal distributions are normal Imagine there are 1000 people standing on the center line of a soccer field and they each have a coin. They each toss their coin 16 times and with each toss, heads means one step left, and tails means one step right. After everyone has done their 16 tosses, can you imagine the distribution of where they would be on the field? It would be roughly normal centered on the center of the field. 4.1.1 Normal by addition To simulate this in R we can assign random values that are either left (-1) or right (1) for each coin toss and then add them together. pos &lt;- replicate(1000, sum(runif(16,-1,1))) plot(density(pos)) Now lets visualize what each person’s walk #load packages library(dplyr) library(tidyr) library(ggplot2) #set seed for reproducible plots set.seed(11) #create dataframe pos_df &lt;- replicate(100, runif(16, -1, 1)) %&gt;% #simulation (turned down to 100 people) as_tibble() %&gt;% # make it a tibble rbind(0, .) %&gt;% # add a row of zeros above the coin tosses as a starting point mutate(step = 0:16) %&gt;% #this is the step index gather(key, value, -step) %&gt;% #this will convert the data into long format mutate(person = rep(1:100, each = 17)) %&gt;% #this adds a person index (17 times for the 0 and 16 coins) group_by(person) %&gt;% #for each person value mutate(position = cumsum(value)) %&gt;% #create a cumulative sum for each person ungroup() #create plot pos_df %&gt;% ggplot(aes(x = step, y = position, group = person)) + geom_vline(xintercept = c(4, 8, 16), linetype = 2) + geom_line(aes(color = person &lt; 2, alpha = person &lt; 2)) + scale_color_manual(values = c(&#39;skyblue4&#39;,&#39;black&#39;)) + scale_alpha_manual(values = c(1/5, 1)) + scale_x_continuous(&#39;step number&#39;, breaks = c(0, 4, 8, 12, 16)) + theme(legend.position = &#39;none&#39;, panel.grid = element_blank()) You can see that the more steps you add, the more normal the distribution becomes. 4.1.2 Normal by multiplication Suppose that an organism’s growth rate is influenced by 12 loci each with several alleles that code for more growth and that these loci can interact with each other causing multiplicative effects set.seed(11) prod(1 + runif(12, 0, 0.1)) ## [1] 1.448587 Or more explicitly: set.seed(11) a = 1 b = runif(12, 0, 0.1) c = a + b prod(c) ## [1] 1.448587 as a density: set.seed(11) growth &lt;- replicate(10000, prod(1 + runif(12, 0, 0.1))) plot(density(growth)) Now see how the distribution behaves if you change the effect size of the loci set.seed(11) big &lt;- replicate(10000, prod(1 + runif(12, 0, 0.5))) plot(density(big)) set.seed(11) small &lt;- replicate(10000, prod(1 + runif(12, 0, 0.01))) plot(density(small)) 4.1.3 Normal by log-multiplication See how skewed the big distribution was above? If we log transform the product, what do you think we will see? set.seed(11) log_big &lt;- replicate(10000, log(prod(1 + runif(12, 0, 0.5)))) plot(density(log_big)) 4.1.4 Using Gaussian distributions define ontological define epistemological 4.2 A language for describing models Learning the language 1. Recognize the variables you are working with. Observable variables are data and unobservable variables are parameters 2. Define the variable in terms of the other variables or as probability distributions 3. The combination of variables and probability distributions are joint generative models that can be used to simulate hypothetical observations as well as analyze real ones Summarize the model in a mathy way: \\[\\begin{equation} y_{i} ~ \\text{Normal}(\\mu_{i}, \\sigma)\\ \\mu_{i} ~ \\beta x_{i}\\ \\beta ~ \\text{Normal}(0, 10)\\ \\sigma ~ \\text{Exponential}(1)\\ x_{i} ~ \\text{Normal}(0,1)\\ \\end{equation}\\] 4.2.1 Re-describing the globe tossing model. Recall that the globe tossing model had the variables defined as: \\[\\begin{equation} W ~ \\text{Binomial}(N, p)\\ p ~ \\text{Uniform}(0, 1) \\end{equation}\\] These definitions have implied meanings. For example, the Binomial distribution of \\(W\\) implies that each toss (\\(N\\)) is independent of the others. The first line of these simple models are the likelihood function from Bayes’ theorem. The other lines will outline priors for variables if defined. 4.3 Gaussian model of height In this section we will build the beginnings of a regression model that will follow a gaussian distribution with a mean \\(\\mu\\) and standard deviation \\(\\sigma\\). The model will take in data or a predictor variable and evaluate all possible values of \\(\\mu\\) and \\(\\sigma\\) to produce a distribution of distributions basically. 4.3.1 The data Let’s load up McElreath’s data library(rethinking) data(Howell1) d &lt;- Howell1 Now we can see the new data frame str(d) ## &#39;data.frame&#39;: 544 obs. of 4 variables: ## $ height: num 152 140 137 157 145 ... ## $ weight: num 47.8 36.5 31.9 53 41.3 ... ## $ age : num 63 63 65 41 51 35 32 27 19 54 ... ## $ male : int 1 0 0 1 0 1 0 1 0 1 ... precis(d, hist=FALSE) #rethinking package ## mean sd 5.5% 94.5% ## height 138.2635963 27.6024476 81.108550 165.73500 ## weight 35.6106176 14.7191782 9.360721 54.50289 ## age 29.3443934 20.7468882 1.000000 66.13500 ## male 0.4724265 0.4996986 0.000000 1.00000 And for this model we will just be using height of adults d2 &lt;- d[d$age &gt;= 18 ,] #create a new data frame that is all rows where age is 18 or greater 4.3.2 The Model We want to model these heights with a Gaussian distribution. Let’s see the data first dens(d2$height) This is pretty normal looking so we can go ahead and set the model up to use a Gaussian distirbution to describe the probability distribution of the data. Because we are still unsure about which distribution is the right one, we will leave the mean and standard deviation as variables for now. \\[\\begin{equation} h_i ~ \\text{Normal}(\\mu, \\sigma) \\end{equation}\\] This is our likelihood function for the model. Next we will need to define our \\(\\mu\\) and \\(\\sigma\\) variables. \\[\\begin{equation} \\mu ~ \\text{Normal}(178, 20) \\sigma ~ \\text{Uniform}(0, 50) \\end{equation}\\] Here McElreath uses 178 cm as the mean because that is his height and spreads the 95% interval 40 cm in either direction (138 - 218cm). Let’s see what this would look like curve(dnorm(x, 178, 20), from = 100, to = 250) Here we are looking across heights from 100cm to 250cm and as you can see the normal distribution is putting the the mean between 140 and 220. \\(\\sigma\\) has a uniform prior that holds its value somewhere between 0 and 50 cm. curve(dunif(x, 0, 50), from = -10, to = 60) Here you can see that any value from 0 to 50 has an equal probability of being ‘correct.’ Now we can create something called a Prior Predictive by using the priors we’ve defined to see how they relate to the observed heights. sample_mu &lt;- rnorm(1e4, 178, 20) #create randome draws from the mu prior sample_sigma &lt;- runif(1e4, 0, 50) #create random draws from the sigma prior prior_h &lt;- rnorm(1e4, sample_mu, sample_sigma) #use the priors of mu and sigma to infulence h dens(prior_h) 4.3.3 Grid approximation of the posterior distribution Here we will approximate the posterior in detail with grid approximation for learning purposes. First will be the methods presented by McElreath and second I will add a tidyverse version adapted from Kurz mu_list &lt;- seq(from = 150, to = 160, length.out = 100) #100 equally spaced values from 150 to 160 sigma_list &lt;- seq(from = 7, to = 9, length.out = 100) #100 equally spaced values from 7 to 9 post &lt;- expand.grid(mu = mu_list, sigma = sigma_list) #expand to a grid of all combinations of the above (100 x 100) post$LL &lt;- sapply(1:nrow(post), function(i) sum( dnorm(d2$height, post$mu[i], post$sigma[i], log = TRUE))) #calculate the log likelihood for each row of post across the data values of d2$height post$prod &lt;- post$LL + dnorm(post$mu, 178, 20, TRUE) + dunif(post$sigma, 0, 50, TRUE) #create a product by adding priors to log likelihood post$prob &lt;- exp(post$prod - max(post$prod)) #Create the posterior probability by taking each product and subtracting the maximum product (while also back transforming to cm) Now we can see the posterior with: contour_xyz(post$mu, post$sigma, post$prob) or: image_xyz(post$mu, post$sigma, post$prob) In tidyverse language n &lt;- 100 d_grid &lt;- as_tibble(mu &lt;- seq(from = 150, to = 160, length.out = n), sigma &lt;- seq(from = 7, to = 9, length.out = n)) d_grid &lt;- d_grid %&gt;% tidyr::expand(mu, sigma) head(d_grid) ## # A tibble: 6 × 2 ## mu sigma ## &lt;dbl&gt; &lt;dbl&gt; ## 1 150 7 ## 2 150 7.02 ## 3 150 7.04 ## 4 150 7.06 ## 5 150 7.08 ## 6 150 7.10 grid_function &lt;- function(mu, sigma){ dnorm(d2$height, mean = mu, sd = sigma, log = TRUE) %&gt;% sum() } d_grid &lt;- d_grid %&gt;% mutate(log_likelihood = purrr::map2(mu, sigma, grid_function)) %&gt;% unnest() %&gt;% mutate(prior_mu = dnorm(mu, mean = 178, sd = 20, log = TRUE), prior_sigma = dunif(sigma, min = 0, max = 50, log = TRUE)) %&gt;% mutate(product = log_likelihood + prior_mu + prior_sigma) %&gt;% mutate(probability = exp(product - max(product))) ## Warning: `cols` is now required when using unnest(). ## Please use `cols = c(log_likelihood)` head(d_grid) ## # A tibble: 6 × 7 ## mu sigma log_likelihood prior_mu prior_sigma product probability ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 150 7 -1299. -4.89 -3.91 -1308. 1.96e-35 ## 2 150 7.02 -1298. -4.89 -3.91 -1307. 3.78e-35 ## 3 150 7.04 -1298. -4.89 -3.91 -1307. 7.20e-35 ## 4 150 7.06 -1297. -4.89 -3.91 -1306. 1.36e-34 ## 5 150 7.08 -1296. -4.89 -3.91 -1305. 2.53e-34 ## 6 150 7.10 -1296. -4.89 -3.91 -1305. 4.65e-34 d_grid %&gt;% ggplot(aes(x = mu, y = sigma, z = probability)) + geom_contour() + labs(x = expression(mu), y = expression(sigma)) + xlim(range(d_grid$mu)) + ylim(range(d_grid$sigma)) + theme(panel.grid = element_blank()) d_grid %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_raster(aes(fill = probability), interpolate = TRUE) + scale_fill_viridis_c(option = &#39;A&#39;) + labs(x = expression(mu), y = expression(sigma)) + theme(panel.grid = element_blank()) 4.3.4 Sampling from the posterior Now that we have a posterior calculated, we can pull samples from it. sample_rows &lt;- sample(1:nrow(post), size = 1e4, replace = TRUE, prob = post$prob) #sample rows by their probability sample_mu &lt;- post$mu[sample_rows] #index samples of mu by the rows sampled sample_sigma &lt;- post$sigma[sample_rows] #index samples of sigma by rows sampled Now we can plot these samples to see where the most common values for \\(\\mu\\) and \\(\\sigma\\) might overlap plot(sample_mu, sample_sigma, cex = 1, pch = 16, col = col.alpha(rangi2, 0.1)) We can also look at the samples individually dens(sample_mu) dens(sample_sigma) And we can get summary points from these sample distributions just like before PI(sample_mu) ## 5% 94% ## 153.9394 155.2525 PI(sample_sigma) ## 5% 94% ## 7.323232 8.252525 (tidyverse) d_grid_samples &lt;- d_grid %&gt;% sample_n(size = 1e4, replace = TRUE, weight = probability) d_grid_samples %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_point(size = 0.9, alpha = 1/15) + scale_fill_viridis_c() + labs(x = expression(mu[samples]), y = expression(sigma[samples])) + theme(panel.grid = element_blank()) d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% ggplot(aes(x = value)) + geom_density(fill = &#39;grey35&#39;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &#39;free&#39;) library(tidybayes) d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) ## # A tibble: 2 × 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu 155. 154. 156. 0.95 mode hdi ## 2 sigma 7.75 7.22 8.35 0.95 mode hdi d_grid_samples %&gt;% select(mu, sigma) %&gt;% gather() %&gt;% group_by(key) %&gt;% median_qi(value, .width = 0.5) ## # A tibble: 2 × 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu 155. 154. 155. 0.5 median qi ## 2 sigma 7.77 7.57 7.97 0.5 median qi 4.3.5 Finding the posterior distribution with quap Recall that quadratic approximation uses the maximum a posteriori (MAP) as the peak of the distribution and creates the shape of the distribution as a negative quadratic based on the slope near the peak. To set up, lets rerun the data input data(Howell1) d &lt;- Howell1 d2 &lt;- d[d$age &gt;= 18,] Now we have to define our model parameters in R code flist &lt;- alist( height ~ dnorm(mu, sigma), mu ~ dnorm(178, 20), sigma ~ dunif(0, 50) ) And then we can fit the model to the data m4.1 &lt;- quap(flist, data = d2) and the posterior is available to view here precis(m4.1) ## mean sd 5.5% 94.5% ## mu 154.606930 0.4120200 153.948443 155.265418 ## sigma 7.731809 0.2914308 7.266046 8.197571 Changing the prior for \\(\\mu\\) to be quite narrow m4.2 &lt;- quap( alist( height ~ dnorm(mu, sigma), mu ~ dnorm(178, 0.1), sigma ~ dunif(0, 50) ), data = d2) precis(m4.2) ## mean sd 5.5% 94.5% ## mu 177.86375 0.1002354 177.70356 178.02395 ## sigma 24.51748 0.9289156 23.03289 26.00207 notice here that because the prior was so narrow for \\(\\mu\\), the mean did not change much from the prior value. \\(\\sigma\\) on the other hand has vastly changed from the previous example even though its prior remained the same. 4.3.6 Sampling with quap Because quadratic approximation is inherently multidimensional, we need to access to the different covariances between pairs of parameters. Here is the variance-covariance matrix for the first model. vcov(m4.1) ## mu sigma ## mu 0.1697604695 0.0002160376 ## sigma 0.0002160376 0.0849319320 The above contains both the variance of each parameter but also the correlated change expected in the other parameters. To see them separately: diag(vcov(m4.1)) #variance of each parameter (sqrt = sd) ## mu sigma ## 0.16976047 0.08493193 cov2cor(vcov(m4.1)) #Correlation matrix of the parameters ## mu sigma ## mu 1.000000000 0.001799184 ## sigma 0.001799184 1.000000000 The correlation matrix above indicates that \\(\\mu\\) and \\(\\sigma\\) are very far from correlated. Now let’s pull vectors of values from a multi-dimenstional distribution (made easy with rethinking) post &lt;- extract.samples(m4.1, n = 1e4) head(post) ## mu sigma ## 1 154.7748 7.358637 ## 2 155.5361 7.869098 ## 3 154.4503 7.648488 ## 4 154.9172 7.283003 ## 5 153.7071 7.978019 ## 6 154.6316 8.848559 Now we can compare to the posterior samples to the MAP values precis(post) ## mean sd 5.5% 94.5% histogram ## mu 154.610172 0.4093737 153.948552 155.26072 ▁▁▅▇▂▁▁ ## sigma 7.728621 0.2873045 7.267368 8.18118 ▁▁▁▂▅▇▇▃▁▁▁▁ 4.4 Linear Prediction Now that we have a model for height, we can get to the ‘regression’ part where we add a predictor variable. Let’s add weight to our height model and see what that looks like data(Howell1); d &lt;- Howell1; d2 &lt;- d[d$age &gt;= 18, ] plot(d2$height ~ d2$weight) Seems to be a pretty clear relationship between height and weight. The linear model strategy will take the predictor variable (weight) and assume that it has a constant additive relationship to the mean of the outcome (height). Now we have to put our predictor into the model structure. \\[\\begin{equation} h_{i} ~ \\text{Normal}(\\mu_{i}, \\sigma) \\mu_{i} = \\beta (x_{i} - \\overline{x}) \\alpha ~ \\text{Normal}(178, 20) \\beta ~ \\text{Uniform}(0, 10) \\sigma ~ \\text{Uniform}(0, 50) \\end{equation}\\] 4.4.0.1 Probability of the data (first line of the model) So now that both height \\(h_{i}\\) and the mean \\(\\mu_{i}\\) are indexed by \\(i\\) it means that each data point or row in the dataframe influences the value of \\(\\mu\\) and therefore \\(h\\). 4.4.0.2 Linear model (second line of the model) The value of \\(\\mu_{i}\\) is now made from a combination of \\(\\alpha\\), \\(\\beta\\), and observed weight data (\\(x\\)) where \\(\\overline{x}\\) describes the mean of all weight values observed. There are two parts to this line in the model that make it ‘linear’ 1. When \\(x_{i}\\) = \\(\\overline{x}\\) then the \\(\\beta\\) value is 0 which makes \\(\\alpha\\) an ‘intercept.’ 2. When \\(x_{i}\\) increases by one unit, the expected change in height (\\(h_{i}\\)) is explained by \\(\\beta\\) or the ‘slope.’ 4.4.0.3 Priors (lines 3-5 of the model) The \\(\\alpha\\) and \\(\\sigma\\) priors is basically the same as before (except now the \\(\\mu\\) prior is now \\(\\alpha\\)). The \\(\\beta\\) prior is set to have a mean of 0 on purpose because we don’t know for sure what the relationship is between weight and height (even though the plot suggests a positive slope value). To illustrate this point, McElreath simulates the prior predictive distribution using observed weights. set.seed(2971) N &lt;- 100 # 100 lines a &lt;- rnorm(N, 178, 20) # alpha prior b &lt;- rnorm(N, 0, 10) # beta prior Let’s see how this random selection from the priors looks plot(NULL, xlim = range(d2$weight), ylim = c(-100, 400), xlab = &#39;weight&#39;, ylab = &#39;height&#39;) abline(h = 0, lty = 2) #dashed line at height = 0 abline(h = 272, lty = 1, lwd = 0.5) #solid line at world&#39;s tallest person mtext(&#39;b ~ dnorm(0, 10)&#39;) #plot title xbar &lt;- mean(d2$weight) #average weight for(i in 1:N){ curve(a[i] + b[i]*(x - xbar), #linear equation for each pair from = min(d2$weight), to = max(d2$weight), add = TRUE, col = col.alpha(&#39;black&#39;,0.2)) } This is clearly absurd as no adult should be near 0 height and weigh 30 kg. We can fix this by holding \\(\\beta\\) positive with a log-normal distribution b &lt;- rlnorm(1e4, 0, 1) dens(b, xlim = c(0,5), adj = 0.1) Now we can use this new \\(\\beta\\) prior in the last plot set.seed(2971) N &lt;- 100 # 100 lines a &lt;- rnorm(N, 178, 20) # alpha prior b &lt;- rlnorm(N, 0, 1) # NEW beta prior plot(NULL, xlim = range(d2$weight), ylim = c(-100, 400), xlab = &#39;weight&#39;, ylab = &#39;height&#39;) abline(h = 0, lty = 2) #dashed line at height = 0 abline(h = 272, lty = 1, lwd = 0.5) #solid line at world&#39;s tallest person mtext(&#39;log(b) ~ dnorm(0, 1)&#39;) #plot title xbar &lt;- mean(d2$weight) #average weight for(i in 1:N){ curve(a[i] + b[i]*(x - xbar), #linear equation for each pair from = min(d2$weight), to = max(d2$weight), add = TRUE, col = col.alpha(&#39;black&#39;,0.2)) } This new plot seems far more plausible for an adult population. 4.4.1 Finding the posterior distribtuion Just like before we can use the quap function to get a quadratic approximation or alternatively, we can use brms to fit the model. quap method #re load the data data(Howell1) ; d &lt;- Howell1; d2 &lt;- d[d$age &gt;= 18,] #define the mean weight of observed values xbar &lt;- mean(d2$weight) #fit the model m4.3 &lt;- quap( alist( height ~ dnorm(mu, sigma), #height likelihood distribution mu &lt;- a + b*(weight - xbar), #linear equation a ~ dnorm(178, 20), #alpha prior b ~ dlnorm(0, 1), #beta prior sigma ~ dunif(0, 50) #sigma prior ), data = d2 ) precis(m4.3) ## mean sd 5.5% 94.5% ## a 154.6013671 0.27030766 154.1693633 155.0333710 ## b 0.9032807 0.04192363 0.8362787 0.9702828 ## sigma 5.0718809 0.19115478 4.7663786 5.3773831 brms method library(brms) b4.3_priors &lt;- get_prior(data = d2, formula = height ~ 1 + weight) b4.3 &lt;- brm(data = d2, #data to be used family = gaussian, #likelihood of height height ~ 1 + weight, #linear equation prior = b4.3_priors, iter = 41000, warmup = 40000, chains = 4, cores = 4, seed = 11) #model mechanics ## Compiling Stan program... ## Start sampling The plot plot(b4.3) The alpha values (b_intercept in brms) appear to be quite different here. This is because we haven’t centered the weight values yet (\\(x - \\overline{x}\\)), so we can do that and run it again. d2 &lt;- d2 %&gt;% mutate(weight_c = weight - mean(weight)) b4.3b_priors &lt;- get_prior(data = d2, formula = height ~ 1 + weight_c) b4.3b &lt;- brm(data = d2, #data to be used family = gaussian, #likelihood of height height ~ 1 + weight_c, #linear equation prior = b4.3b_priors, iter = 41000, warmup = 40000, chains = 4, cores = 4, seed = 11) #model mechanics ## Compiling Stan program... ## recompiling to avoid crashing R session ## Start sampling plot(b4.3b) 4.4.2 Interpreting the posterior distribution The model can only report the posterior distribution. This is the right answer to the question you asked. But it’s your responsibility to process the answer and make sense of it. —McElreath, p.98 You can make sense of model outputs in two ways: 1. read summary tables 2. plot simulations Reading summary tables becomes rather complicated as the models become more complex. 4.4.2.1 Tables of marginal distributions precis(m4.3) ## mean sd 5.5% 94.5% ## a 154.6013671 0.27030766 154.1693633 155.0333710 ## b 0.9032807 0.04192363 0.8362787 0.9702828 ## sigma 5.0718809 0.19115478 4.7663786 5.3773831 In this table, the first row is the quadratic approximation for \\(\\alpha\\), second is \\(\\beta\\), and finally \\(\\sigma\\). repeated for our brms model round(posterior_summary(b4.3b, probs = c(0.055,0.945))[1:3,], 2) ## Estimate Est.Error Q5.5 Q94.5 ## b_Intercept 154.59 0.27 154.16 155.03 ## b_weight_c 0.91 0.04 0.84 0.97 ## sigma 5.10 0.19 4.81 5.41 Now let’s see the variance-covariance matrix round(vcov(m4.3), 3) ## a b sigma ## a 0.073 0.000 0.000 ## b 0.000 0.002 0.000 ## sigma 0.000 0.000 0.037 repeated for brms model posterior_samples(b4.3b) %&gt;% select(-lp__) %&gt;% cor() %&gt;% round(digits = 2) ## Warning: Method &#39;posterior_samples&#39; is deprecated. Please see ?as_draws for ## recommended alternatives. ## b_Intercept b_weight_c sigma ## b_Intercept 1.00 0.03 0.05 ## b_weight_c 0.03 1.00 -0.05 ## sigma 0.05 -0.05 1.00 Visual approach pairs(m4.3) *very little covariance here 4.4.2.2 Plotting the posterior inference against the data plot(height ~ weight, data = d2, col = rangi2) #plot the data points post &lt;- extract.samples(m4.3) #extract samples from the posterior a_map &lt;- mean(post$a) #get the mean of the samples of alpha b_map &lt;- mean(post$b) #get the mean of the samples of beta curve(a_map + b_map*(x - xbar), add = TRUE) This line is very plausible but ther are others that are equally plausible d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_abline(intercept = fixef(b4.3)[1], slope = fixef(b4.3)[2]) + geom_point(shape = 1, size = 2, color = &#39;royalblue&#39;) + theme_bw() + theme(panel.grid = element_blank()) 4.4.2.3 Adding uncertainty around the mean There are many combinations of \\(\\alpha\\) and \\(\\beta\\) that could be plausible post &lt;- extract.samples(m4.3) post[1:5,] ## a b sigma ## 1 154.5789 0.9376825 5.220756 ## 2 154.4067 0.8937310 4.752735 ## 3 154.4622 0.9150822 5.341227 ## 4 154.2649 0.9236067 5.160423 ## 5 155.1258 0.9495934 5.108891 We will also alter the amount of observations to see how it influences the spread of these combinations. N &lt;- 10 #lets pull 10 data points dN &lt;- d2[1:N, ] mN &lt;- quap( alist( height ~ dnorm(mu, sigma), mu &lt;- a + b*(weight - mean(weight)), a ~ dnorm(178, 20), b ~ dlnorm(0, 1), sigma ~ dunif(0, 50) ), data = d2) #and plot 20 samples post &lt;- extract.samples(mN, n=20) #display data and sample size plot(dN$weight, dN$height, xlim = range(d2$weight), ylim = range(d2$height), col = rangi2, xlab = &#39;weight&#39;, ylab = &#39;height&#39;,) mtext(concat(&#39;N = &#39;,N)) #add sample lines for(i in 1:20){ curve(post$a[i] + post$b[i]*(x - mean(dN$weight)), col=col.alpha(&#39;black&#39;,0.3), add = TRUE) } You can see that the extreme weight values are far more spread out. This pattern disappears when the model has more data to work with and becomes more confident about the means. N &lt;- 300 #lets pull 300 data points dN &lt;- d2[1:N, ] mN &lt;- quap( alist( height ~ dnorm(mu, sigma), mu &lt;- a + b*(weight - mean(weight)), a ~ dnorm(178, 20), b ~ dlnorm(0, 1), sigma ~ dunif(0, 50) ), data = d2) #and plot 20 samples post &lt;- extract.samples(mN, n=25) #display data and sample size plot(dN$weight, dN$height, xlim = range(d2$weight), ylim = range(d2$height), col = rangi2, xlab = &#39;weight&#39;, ylab = &#39;height&#39;,) mtext(concat(&#39;N = &#39;,N)) #add sample lines for(i in 1:25){ curve(post$a[i] + post$b[i]*(x - mean(dN$weight)), col=col.alpha(&#39;black&#39;,0.3), add = TRUE) } 4.4.2.4 Plotting regression intervals and contours Let’s use 50 kg as a focal weight and grab some samples post &lt;- extract.samples(m4.3) mu_at_50 &lt;- post$a + post$b * (50 - xbar) Now we can look at the distribution of plausible heights for a person of 50kg weight dens(mu_at_50, col = rangi2, xlab = &#39;mu|weight = 50kg&#39;) The height of someone that weighs 50 kg is almost certainly between 158 and 160.5 cm. PI(mu_at_50, prob = 0.89) ## 5% 94% ## 158.5706 159.6649 and 89% of the ways the model can produce the data suggests that the average height of a 50kg person is between 159 and 160cm. Now what can we do about the average slope (\\(\\beta\\)) from the model? mu &lt;- link(m4.3) str(mu) ## num [1:1000, 1:352] 157 157 158 157 157 ... Now we have a large matrix that has 1000 samples (rows) for 352 data points (observations). But we want to have increments of 1 kg steps across the x axis. weight.seq &lt;- seq(from = 25, to = 70, by =1) mu &lt;- link(m4.3, data = data.frame(weight = weight.seq)) str(mu) ## num [1:1000, 1:46] 137 136 136 137 135 ... Now our new matrix has 1000 samples across 46 weight steps. Let’s visualize the uncertainty of each weight value. plot(height ~ weight, d2, type = &#39;n&#39;) #type &#39;n&#39; hides the data for (i in 1:100){ points(weight.seq, mu[i,], pch = 16, col = col.alpha(rangi2, 0.1)) } Now we can compute a mean \\(\\mu\\) value and the distribution for each weight value mu_mean &lt;- apply(mu, 2, mean) mu_PI &lt;- apply(mu, 2, PI, prob = 0.89) And add them to the plot plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.5)) shade(mu_PI, weight.seq, col = &#39;grey80&#39;) lines(weight.seq, mu_mean) 4.4.2.5 Prediction Intervals Now that we have a good handle on \\(\\mu_{i}\\) we can combine it with \\(\\sigma\\) to create estimates of height rather than just the mean. sim.height &lt;- sim(m4.3, data = list(weight = weight.seq)) str(sim.height) ## num [1:1000, 1:46] 142 139 138 138 145 ... height.PI &lt;- apply(sim.height, 2, PI, prob = 0.89) plot(height ~ weight, d2, type = &#39;n&#39;) shade(height.PI, weight.seq, col = &#39;grey85&#39;) shade(mu_PI, weight.seq, col = &#39;grey60&#39;) points(x = d2$weight, y = d2$height, pch = 16, col = col.alpha(rangi2, 0.5)) lines(weight.seq, mu_mean) Note: shade() seeems to be broken for me in that without specifying a colour, it is completely transparent If you want to smooth the edges of the interval, you can add more samples to the sim.height controls. sim.height &lt;- sim(m4.3, data=list(weight = weight.seq), n=1e4) height.PI &lt;- apply(sim.height, 2, PI, prob = 0.89) plot(height ~ weight, d2, type = &#39;n&#39;) #plot(height ~ weight, data = d2, col = col.alpha(rangi2, 0.5)) shade(height.PI, weight.seq, col = &#39;grey85&#39;) shade(mu_PI, weight.seq, col = &#39;grey60&#39;) points(x = d2$weight, y = d2$height, pch = 16, col = col.alpha(rangi2, 0.5)) lines(weight.seq, mu_mean) 4.5 Curves from lines There are two common methods to build curves from lines. One is Polynomial Regression and the other is B-Splines. 4.5.1 Polynomial regression This method uses powers of a variable (squares &amp; cubes) as extra pedictors. Let’s grab all the height and weight data, not just adults. data(Howell1) d &lt;- Howell1 plot(height~weight, d) This is quite the curve. We can try to model it with a parabolic model: \\[\\begin{equation} \\mu_{i} = \\alpha + \\beta_{1} x_{i} + \\beta_{2} x_{i}^{2} \\end{equation}\\] where now each weight observation \\(x_{i}\\) is used twice. Once as before as a linear fit and a second time squared to produce the curvature of the line defined as \\(\\beta_{2}\\) To fit the model to the data we will need to standardize the predictor variable. d$weight_s &lt;- (d$weight - mean(d$weight))/sd(d$weight) d$weight_s2 &lt;- d$weight_s^2 m4.5 &lt;- quap( alist( height ~ dnorm(mu, sigma), #same as before mu &lt;- a + b1*weight_s + b2*weight_s2, #adding the b2 and squared term here a ~ dnorm(178, 20), #same b1 ~ dlnorm(0, 1), #same as beta prior before b2 ~ dnorm(0, 1), #new prior that could be positive or negative sigma ~ dunif(0, 50) ), data = d ) Now try and interpret the results of the summary table precis(m4.5) ## mean sd 5.5% 94.5% ## a 146.057411 0.3689756 145.467716 146.647105 ## b1 21.733065 0.2888891 21.271364 22.194766 ## b2 -7.803264 0.2741839 -8.241463 -7.365065 ## sigma 5.774476 0.1764652 5.492450 6.056501 Not very straight forward how the numbers in the table relate to the data Let’s find the mean for \\(\\mu\\) again now with \\(\\beta_{1}\\) and \\(\\beta_{2}\\) in the equation. weight.seq &lt;- seq(from = -2.2, to = 2, length.out = 30) # grab 30 equally spaced weight values pred_dat &lt;- list(weight_s = weight.seq, weight_s2 = weight.seq^2) #new weight values mu &lt;- link(m4.5, data = pred_dat) #get new estimates from new data mu.mean &lt;- apply(mu, 2, mean) #calculate the mu mean for the line mu.PI &lt;- apply(mu , 2, PI, prob = 0.89) #calculate a 89% interval around the mean for mu sim.height &lt;- sim(m4.5, data = pred_dat) #calculate estimates for heights based on mu and sigma with new weight values height.PI &lt;- apply(sim.height, 2, PI, prob = 0.89) #calculate 89% intervals of height plot(height~weight_s, d, type = &#39;n&#39;) lines(weight.seq, mu.mean) shade(mu.PI, weight.seq) shade(height.PI, weight.seq) points(x = d$weight_s, y = d$height, col = col.alpha(rangi2, 0.5)) Now let’s try adding a cubic predictor term \\(\\beta_{3}x_{i}^3\\) d$weight_s3 &lt;- d$weight_s^3 m4.6 &lt;- quap( alist( height ~ dnorm(mu, sigma), mu &lt;- a + b1*weight_s + b2*weight_s2 + b3*weight_s3, a ~ dnorm(178, 20), b1 ~ dlnorm(0, 1), b2 ~ dnorm(0, 1), b3 ~ dnorm(0,1), sigma ~ dunif(0, 50) ), data = d ) weight.seq &lt;- seq(from = -2.2, to = 2, length.out = 30) # grab 30 equally spaced weight values pred_dat &lt;- list(weight_s = weight.seq, weight_s2 = weight.seq^2, weight_s3 = weight.seq^3) #new weight values mu &lt;- link(m4.6, data = pred_dat) #get new estimates from new data mu.mean &lt;- apply(mu, 2, mean) #calculate the mu mean for the line mu.PI &lt;- apply(mu , 2, PI, prob = 0.89) #calculate a 89% interval around the mean for mu sim.height &lt;- sim(m4.6, data = pred_dat) #calculate estimates for heights based on mu and sigma with new weight values height.PI &lt;- apply(sim.height, 2, PI, prob = 0.89) #calculate 89% intervals of height plot(height~weight_s, d, type = &#39;n&#39;) lines(weight.seq, mu.mean) shade(mu.PI, weight.seq) shade(height.PI, weight.seq) points(x = d$weight_s, y = d$height, col = col.alpha(rangi2, 0.5)) And here is how you would plot the above with the original x-axis plot(height ~ weight_s, d, col=col.alpha(rangi2, 0.5), type = &#39;n&#39;, xaxt = &#39;n&#39;) #turn off x axis text #Rcode 4.71 at &lt;- c(-2, -1, 0, 1, 2) labels &lt;- at*sd(d$weight) + mean(d$weight) axis(side = 1, at = at, labels = round(labels, 1)) lines(weight.seq, mu.mean) shade(mu.PI, weight.seq) shade(height.PI, weight.seq) points(x = d$weight_s, y = d$height, col = col.alpha(rangi2, 0.5)) 4.5.2 Splines Splines are smooth functions that are built from smaller component functions that usually take on a wiggly appearance. #load cherry blossom bloom time over a thousand years library(rethinking) data(cherry_blossoms) d &lt;- cherry_blossoms precis(d) ## mean sd 5.5% 94.5% histogram ## year 1408.000000 350.8845964 867.77000 1948.23000 ▇▇▇▇▇▇▇▇▇▇▇▇▁ ## doy 104.540508 6.4070362 94.43000 115.00000 ▁▂▅▇▇▃▁▁ ## temp 6.141886 0.6636479 5.15000 7.29470 ▁▃▅▇▃▂▁▁ ## temp_upper 7.185151 0.9929206 5.89765 8.90235 ▁▂▅▇▇▅▂▂▁▁▁▁▁▁▁ ## temp_lower 5.098941 0.8503496 3.78765 6.37000 ▁▁▁▁▁▁▁▃▅▇▃▂▁▁▁ Let’s see the bloom time over the years plot(x=d$year, y=d$doy) abline(h=mean(d$doy, na.rm = T), col = &#39;red&#39;, lty = 2) Similar to the polynomial method, B-splines will break the predictor (year) into chunks with different basis functions to fit the data in that chunk. When we stitch the chunks together, we often get a wiggly line through the data. The new linear function for \\(\\mu_{i}\\) will look a bit different. \\[\\begin{equation} \\mu_{i} = \\alpha + w_{1}\\beta_{i,1} + w_{2}\\beta_{i,2} + w_{3}\\beta_{i,3} + ... \\end{equation}\\] Here, the \\(\\beta\\) values are for each basis function and the \\(w\\) parameters are the weights for each function. For the cherry blossom data we will split the data up into 15 quantiles and have the basis functions operate within each quantile. d2 &lt;- d[complete.cases(d$doy), ] #remove any NA values num_knots &lt;- 15 #number of sections in the data knot_list &lt;- quantile(d2$year, probs = seq(0, 1, length.out = num_knots)) knot_list ## 0% 7.142857% 14.28571% 21.42857% 28.57143% 35.71429% 42.85714% 50% ## 812 1036 1174 1269 1377 1454 1518 1583 ## 57.14286% 64.28571% 71.42857% 78.57143% 85.71429% 92.85714% 100% ## 1650 1714 1774 1833 1893 1956 2015 So now that we have the break points in the year range of the data we can assign a polynomial degree. This degree determines how many basis functions will be interacting in each year range. If we choose a cubic spline, four basis functions will be active in each year range. library(splines) B &lt;- bs(d2$year, knots = knot_list[-c(1,num_knots)], degree = 3, intercept = TRUE) We just created a matrix that contains 827 rows and 17 columns. Each row is a year in d2 and each column is a basis function. Let’s see what it looks like (fig 4.13a) plot(NULL, xlim = range(d2$year), ylim=c(0,1), xlab=&#39;year&#39;, ylab=&#39;basis&#39;) for(i in 1:ncol(B)){ lines(d2$year, B[,i]) } points(x = knot_list, y = rep(0.9, length(knot_list)), col = &#39;red&#39;) Now we need to calculate the parameter weights (\\(w\\)) for each basis function. To do this we need to run the model. \\[\\begin{equation} D_{i} \\sim \\text{Normal}(\\mu_{i},\\sigma) \\mu_{i} = \\alpha + \\sum_{k=1}^{K}w_{k}B_{k,i} \\alpha \\sim \\text{Normal}(100,10) w_{j} \\sim \\text{Normal}(0,10) \\sigma \\sim \\text{Exponential}(1) \\end{equation}\\] m4.7 &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + B %*% w, a ~ dnorm(100, 10), w ~ dnorm(0, 10), sigma ~ dexp(1) ), data = list(D = d2$doy, B=B), start = list(w=rep(0, ncol(B))) ) Now we can extract some samples and look at the basis weights (fig 4.13b) post &lt;- extract.samples(m4.7) w &lt;- apply(post$w, 2, mean) plot(NULL, xlim=range(d2$year), ylim=c(-6,6), xlab = &#39;year&#39;, ylab=&#39;basis * weight&#39;) for(i in 1:ncol(B)){ lines(d2$year, w[i]*B[,i]) } points(x = knot_list, y = rep(4.9, length(knot_list)), col = &#39;red&#39;) Now we can calculate a posterior interval for \\(\\mu\\) at each year (fig 4.13c) mu &lt;- link(m4.7) mu_PI &lt;- apply(mu, 2, PI, prob = 0.97) mu_mean &lt;- apply(mu, 2, mean) plot(d2$year, d2$doy, col = col.alpha(rangi2, 0.3), pch = 16) shade(mu_PI, d2$year, col = col.alpha(&#39;black&#39;,0.5)) lines(d2$year, mu_mean) A whole other class of similar models are called GAMs (Generalized Additive Models) and are worth looking into if you have time. "],["the-many-variables-the-spurious-waffles.html", "Chapter 5 The many variables &amp; the spurious waffles 5.1 Spurious assoiciation 5.2 Masked relationship 5.3 Categorical variables", " Chapter 5 The many variables &amp; the spurious waffles Here is why we need Multiple Regression to model outcomes. Statistical control for confounds Multiple and complex causation Interactions 5.1 Spurious assoiciation Divorce rate seems to be positively correlated with marriage rate. But does higher marriage rate cause a higher divorce rate? Divorce rate is also negatively correlated with median age at marriage meaning higher divorce rates for younger couples. But does young marriage are cause more divorce? Let’s find out library(rethinking) data(WaffleDivorce) d &lt;- WaffleDivorce #standardize variables d$D &lt;- standardize(d$Divorce) d$M &lt;- standardize(d$Marriage) d$A &lt;- standardize(d$MedianAgeMarriage) Now we can use some linear modeling skills to see how median age of marriage is related to divorce rates \\[\\begin{equation} D_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\ \\mu_{i} = \\alpha + \\beta_{A}A_{i}\\ \\alpha \\sim \\text{Normal}(0, 0.2)\\ \\beta_{A} \\sim \\text{Normal}(0, 0.5)\\ \\sigma \\sim \\text{Exponential}(1)\\ \\end{equation}\\] Here \\(D_{i}\\) is the divorce rate in state \\({i}\\) and \\(A_{i}\\) is the median age of marriage in state \\(i\\). Since both the outcome and predictor are standardized in the above code, the intercept estimate (\\(\\alpha\\)) should be somewhere near 0. But how do we interpret the slope \\(\\beta_{A}\\)? Well if it were to be estimated as 1, then one sd increase in median age of marriage would be a 1 sd increase in divorce rate. To know the magnitude of a 1 sd change, you would have to calculate it sd(d$MedianAgeMarriage) ## [1] 1.24363 sd(d$Divorce) ## [1] 1.820814 So if \\(\\beta_{A}\\) was estimated to be 1, an increase of 1.2 years in median age would increase divorce by 1.82 (units?) Let’s get the posterior of this model m5.1 &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + bA * A, a ~ dnorm(0, 0.2), bA ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) Here is the simulated priors over 2 standard deviations set.seed(11) prior &lt;- extract.prior(m5.1) mu &lt;- link(m5.1, post = prior, data = list(A = c(-2, 2))) plot(NULL, xlim = c(-2,2), ylim = c(-2,2), xlab = &#39;Median age Marriage (std)&#39;, ylab = &#39;Divorce rate (std)&#39;) for(i in 1:50){ lines(c(-2,2), mu[i,], col = col.alpha(&#39;black&#39;,0.4)) } Now the posterior #calculate percentiles A_seq &lt;- seq(from = -3, to = 3.2, length.out = 30) mu &lt;- link(m5.1, data = list(A = A_seq)) mu.mean &lt;- apply(mu, 2, mean) mu.PI &lt;- apply(mu, 2, PI) #plot plot(D ~ A, data = d, col = rangi2, xlab = &#39;Median age Marriage (std)&#39;, ylab = &#39;Divorce rate (std)&#39;) lines(A_seq, mu.mean, lwd = 2) shade(mu.PI, A_seq) And now the Marriage rate model m5.2 &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + bM * M, a ~ dnorm(0, 0.2), bM ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) #calculate percentiles M_seq &lt;- seq(from = -2, to = 2.8, length.out = 30) mu &lt;- link(m5.2, data = list(M = M_seq)) mu.mean &lt;- apply(mu, 2, mean) mu.PI &lt;- apply(mu, 2, PI) #plot plot(D ~ M, data = d, col = rangi2, xlab = &#39;Marriage rate (std)&#39;, ylab = &#39;Divorce rate (std)&#39;) lines(M_seq, mu.mean, lwd = 2) shade(mu.PI, M_seq) Comparing these two models won’t yield much useful information. We need to think about how they may interact together on Divorce rates 5.1.1 Think before to regress We have to try and think about causal inference before we start fitting models. The best way is to create a DAG or Directed Acyclic Graph. In these graphs we can add direction of influence between variables of interest. From the graphs above, we know that both \\(A\\) and \\(M\\) influence our outcome variable \\(D\\). But, is there any relationship between \\(A\\) and \\(M\\)? Also note that to make these graphs, assumptions have to be made in order to make inference. library(dagitty) dag5.1 &lt;-dagitty(&quot;dag{A-&gt;D;A-&gt;M;M-&gt;D}&quot;) coordinates(dag5.1) &lt;-list(x=c(A=0,D=1,M=2),y=c(A=0,D=1,M=0)) drawdag( dag5.1) What this DAG says is: 1. \\(A\\) directly influences \\(D\\) 2. \\(M\\) directly influences \\(D\\) 3. \\(A\\) directly influences \\(M\\) So there are two pathways from \\(A\\) to \\(D\\). One is the direct effect of \\(A\\) on \\(D\\), and the other is an indirect effect of \\(A\\) \\(\\rightarrow\\) \\(M\\) \\(\\rightarrow\\) \\(D\\). We saw that there is a strong negative association of \\(A\\) and \\(D\\) above in model m5.1 but we don’t know if that association is direct or entirely working through an indirect pathway. Alternatively, the effect of \\(M\\) on \\(D\\) could be entirely from \\(A\\)’s effect on \\(M\\) like this: dag5.2 &lt;-dagitty(&quot;dag{A-&gt;D;A-&gt;M}&quot;) coordinates(dag5.2) &lt;-list(x=c(A=0,D=1,M=2),y=c(A=0,D=1,M=0)) drawdag( dag5.2) 5.1.2 Testable implications Now we have two different DAGs that we can consider their testable implications or conditional independencies. The first DAG with three arrows has every pair of variables connected by arrows. We can test this by checking their correlations with each other. cor(d$D, d$A) ## [1] -0.5972392 cor(d$D, d$M) ## [1] 0.3737314 cor(d$A, d$M) ## [1] -0.721096 Pretty strong associations all around. In the second DAG the implication is that \\(D\\) could be independent of \\(M\\) without \\(A\\). Or written in mathy language \\(D \\perp\\!\\!\\!\\perp M|A\\) which means that \\(D\\) is independent of \\(M\\) conditioned on \\(A\\). We can look for conditional independencies with the dagitty package. DMA_dag2 &lt;- dagitty(&#39;dag{D &lt;- A -&gt; M}&#39;) impliedConditionalIndependencies(DMA_dag2) ## D _||_ M | A If we run this on the first three arrow DAG we won’t see an output because there are no conditional independencies in that particular model. DMA_dag1 &lt;- dagitty(&#39;dag{D &lt;- A -&gt; M -&gt; D}&#39;) impliedConditionalIndependencies(DMA_dag1) Because \\(D \\perp\\!\\!\\!\\perp M|A\\) is the only implication that differs between the models, we need a model that will condition on \\(A\\). In other words, once we have conditioned for \\(A\\) (accounted for \\(A\\)), does knowing \\(M\\) add any additional information about \\(D\\)? The expectation from the DAG is that no there shouldn’t be. Once we fit a mutiple regression using all 3 variables we will be able to address how much knowing \\(M\\) influences \\(D\\) and also how much knowing \\(A\\) influences \\(D\\). 5.1.3 Multiple regression notation These will look a lot like the polynomial regressions of the last chapter. \\[\\begin{equation} D_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\ \\mu_{i} = \\alpha + \\beta_{M}M_{i} + \\beta_{A}A_{i}\\ \\alpha \\sim \\text{Normal}(0, 0.2)\\ \\beta_{M} \\sim \\text{Normal}(0, 0.5)\\ \\beta_{A} \\sim \\text{Normal}(0, 0.5)\\ \\sigma \\sim \\text{Exponential}(1)\\ \\end{equation}\\] 5.1.4 Approximating the posterior Like models before now, we will use the quap function to approximate the posterior m5.3 &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + bM * M + bA * A, a ~ dnorm(0, 0.2), bM ~ dnorm(0, 0.5), bA ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) precis(m5.3) ## mean sd 5.5% 94.5% ## a -9.233966e-08 0.09707600 -0.1551463 0.1551461 ## bM -6.538071e-02 0.15077301 -0.3063451 0.1755837 ## bA -6.135136e-01 0.15098354 -0.8548145 -0.3722128 ## sigma 7.851177e-01 0.07784332 0.6607090 0.9095263 Let’s see how the slopes have changed in each model coeftab_plot(coeftab(m5.1, m5.2, m5.3), par = c(&#39;bA&#39;,&#39;bM&#39;)) Here we see that the estimate for \\(\\beta_{A}\\) is relatively the same between models where the estimate for \\(\\beta_{M}\\) is much closer to 0 when considered with \\(\\beta_{A}\\). This suggests that once \\(A\\) is in the model, adding \\(M\\) doesn’t add much more information. This is congruent with our second DAG and tells us that the first DAG is not causally correct because \\(M\\) no longer has a direct effect on \\(D\\) once \\(A\\) is in the model. In case you are curious how \\(A\\) and \\(M\\) are related: m5.4 &lt;- quap( alist( M ~ dnorm(mu, sigma), mu &lt;- a + bAM * A, a ~ dnorm(0, 0.2), bAM ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) #calculate percentiles A_seq &lt;- seq(from = -2.5, to = 3.2, length.out = 30) mu &lt;- link(m5.4, data = list(A = A_seq)) mu.mean &lt;- apply(mu, 2, mean) mu.PI &lt;- apply(mu, 2, PI) #plot plot(M ~ A, data = d, col = rangi2, xlab = &#39;Median age of marriage (std)&#39;, ylab = &#39;Marriage rate (std)&#39;) lines(A_seq, mu.mean, lwd = 2) shade(mu.PI, A_seq) 5.1.5 Plotting multivariate posteriors Once there are more than one predictor variable in the model, a simple scatterplot with a regression line and confidence intervals will not convey as much information as you would want. McElreath explains three example plots to convey more information about your multivariate models Predictor residual plots. These plots will show the outcome against residual predictor values. Posterior prediction plots. These will show model-based predictions against the observations (data). Counterfactual plots. These show implied predictions from imaginary experiments. They can allow you to explore causal implications of the model by manipulating variables. 5.1.5.1 Predictor residual plots Looking again at \\(A \\sim M\\) (m5.4) above, we can calculate the residual (difference from posterior mean) for each data point mu &lt;- link(m5.4) mu_mean &lt;- apply(mu, 2, mean) mu_resid &lt;- d$M - mu_mean # rough plot (Not in book) plot(M ~ A, data = d, col = rangi2, xlab = &#39;Median age of marriage (std)&#39;, ylab = &#39;Marriage rate (std)&#39;) lines(A_seq, mu.mean, lwd = 2) for(i in 1:length(d$A)){ segments(d$A[i], d$M[i], d$A[i], mu_mean[i] ) } #plotted horizontally against Divorce rate d &lt;- cbind(d, mu_resid) m5.4b &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + bMR * mu_resid, a ~ dnorm(0, 0.2), bMR ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) reside_seq &lt;- seq(from = -1.55, to = 1.8, length.out = 30) mu &lt;- link(m5.4b, data = list(mu_resid = reside_seq)) mu.mean &lt;- apply(mu, 2, mean) mu.PI &lt;- apply(mu, 2, PI) plot(x = mu_resid, y = d$D, xlab = &#39;Marriage rate residuals&#39;, ylab = &#39;Divorce rate (std)&#39;, col = col.alpha(rangi2, 0.5)) lines(reside_seq, mu.mean) shade(mu.PI, reside_seq) abline(v = 0, col = &#39;grey50&#39;, lty = 2) text( x = -0.2, y = 2, label = &#39;slower&#39;) text(x = 0.2, y = 2, label = &#39;faster&#39;) As we already found from the model, the marriage rate has little to no effect on the divorce rate, no matter how far the observation is from the expected mean. This process can be repeated for the other predictor as well m5.4c &lt;- quap( alist( A ~ dnorm(mu, sigma), mu &lt;- a + bMA * M, a ~ dnorm(0, 0.2), bMA ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) mu &lt;- link(m5.4c) mu_mean2 &lt;- apply(mu, 2, mean) mu_resid2 &lt;- d$A - mu_mean2 #calculate percentiles M_seq &lt;- seq(from = -1.76, to = 2.85, length.out = 30) mu2 &lt;- link(m5.4c, data = list(M = M_seq)) mu.mean2 &lt;- apply(mu2, 2, mean) mu.PI2 &lt;- apply(mu2, 2, PI) #plot plot(A ~ M, data = d, col = rangi2, xlab = &#39;Marriage rate (std)&#39;, ylab = &#39;Median age of marriage (std)&#39;) lines(M_seq, mu.mean2, lwd = 2) for(i in 1:length(d$M)){ segments(d$M[i], d$A[i], d$M[i], mu_mean2[i] ) } d &lt;- cbind(d, mu_resid2) m5.4d &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + bMR * mu_resid2, a ~ dnorm(0, 0.2), bMR ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) reside_seq &lt;- seq(from = -1.3, to = 2.5, length.out = 30) mu2 &lt;- link(m5.4d, data = list(mu_resid2 = reside_seq)) mu.mean2 &lt;- apply(mu2, 2, mean) mu.PI2 &lt;- apply(mu2, 2, PI) plot(x = mu_resid2, y = d$D, xlab = &#39;Median age residuals&#39;, ylab = &#39;Divorce rate (std)&#39;, col = col.alpha(rangi2, 0.5)) lines(reside_seq, mu.mean2) shade(mu.PI2, reside_seq) abline(v = 0, col = &#39;grey50&#39;, lty = 2) text( x = -0.25, y = 2, label = &#39;younger&#39;) text(x = 0.2, y = 2, label = &#39;older&#39;) And just as the model tells us, states that have younger than the expected average age of marriage have higher divorce rates than states that have older than expected age of marriage. 5.1.5.2 Posterior prediction plots Comparing model predictions against the observations can serve a couple purposes Checking to see that the model correctly approximated the posterior Checking to see how the model fails and if it should even be retained Here’s how we could simulate predictions mu &lt;- link(m5.3) #our multivariate model of divorce rate #summarize samples mu_mean &lt;- apply(mu, 2, mean) mu_PI &lt;- apply(mu, 2, PI) #simulate observations D_sim &lt;- sim(m5.3, n = 1e4) D_PI &lt;- apply(D_sim, 2, PI) and then plot the results plot(mu_mean ~ d$D, col = rangi2, ylim = range(mu_PI), xlab = &#39;Observed divorce&#39;, ylab = &#39;Predicted divorce&#39;) abline(a = 0, b = 1, lty = 2) for(i in 1:nrow(d)){ lines(rep(d$D[i],2), mu_PI[,i], col = rangi2) } Somewhat unsurprisingly, the model is bad with extreme values (-2, 2), and does pretty well near the means. 5.1.5.3 Counterfactual plots Here McElreath makes use of the term counterfactual in the sense of some computation that that makes use of the casual model. This is done by manipulating variables to see how the outcome would change. The general recipe would be 1. pick a variable to manipulate 2. define a range of values for the manipulation 3. for each value of the manipulated variable, and for each sample in the posterior, use the causal model to simulate the other variables and outcome variable. For this example we will be looking at the first DAG we came up with with the three arrows Let’s create the model for this example data(&quot;WaffleDivorce&quot;) d &lt;- list() d$A &lt;- standardize(WaffleDivorce$MedianAgeMarriage) d$D &lt;- standardize(WaffleDivorce$Divorce) d$M &lt;- standardize(WaffleDivorce$Marriage) m5.3_A &lt;- quap( alist( ## A -&gt; D &lt;- M D ~ dnorm(mu, sigma), mu &lt;- a + bM*M + bA*A, a ~ dnorm(0,0.2), bM ~ dnorm(0, 0.5), bA ~ dnorm(0, 0.5), sigma ~ dexp(1), ## A -&gt; M M ~ dnorm(mu_M, sigma_M), mu_M &lt;- aM + bAM*A, aM ~ dnorm(0, 0.2), bAM ~ dnorm(0, 0.2), sigma_M ~ dexp(1) ), data = d ) precis(m5.3_A) ## mean sd 5.5% 94.5% ## a -0.0000194729 0.09707615 -0.1551659 0.1551270 ## bM -0.0655282376 0.15077177 -0.3064906 0.1754342 ## bA -0.6135574871 0.15098333 -0.8548580 -0.3722570 ## sigma 0.7851193004 0.07784372 0.6607100 0.9095286 ## aM -0.0000642742 0.08825440 -0.1411119 0.1409833 ## bAM -0.5783793520 0.09200708 -0.7254244 -0.4313343 ## sigma_M 0.6954216151 0.07116903 0.5816798 0.8091635 Now we will define our value range A_seq &lt;- seq(from = -2, to = 2, length.out = 30) Now we can simulate observations from this range #prep data sim_dat &lt;- data.frame(A = A_seq) #simulate M and then D, using A_seq s &lt;- sim(m5.3_A, data = sim_dat, vars = c(&#39;M&#39;, &#39;D&#39;)) and plot the predictions plot(sim_dat$A, colMeans(s$D), ylim = c(-2,2), type = &#39;l&#39;, xlab = &#39;manipulated A&#39;, ylab = &#39;counterfactual D&#39;) shade(apply(s$D, 2, PI), sim_dat$A) mtext(&quot;total counterfactual effect of A on D&quot;) This plot above shows the trend in D following both paths \\(A \\rightarrow D\\) and \\(A \\rightarrow M \\rightarrow D\\) We can also produce the effect of \\(A \\rightarrow M\\) plot(sim_dat$A, colMeans(s$M), ylim = c(-2,2), type = &#39;l&#39;, xlab = &#39;manipulated A&#39;, ylab = &#39;counterfactual M&#39;) shade(apply(s$M, 2, PI), sim_dat$A) mtext(&quot;Counterfactual effect of A on M&quot;) Let’s raise the median age of marriage from 20 to 30 and see the change in Divorce rate (in std dev units) #new data with mean 26.1 and std dev 1.24 sim2_dat &lt;- data.frame(A = (c(20,30)-26.1)/1.24) s2 &lt;- sim(m5.3_A, data = sim2_dat, vars = c(&#39;M&#39;,&#39;D&#39;)) mean(s2$D[,2] - s2$D[,1]) ## [1] -4.643447 Reducing divorce by 4.5 standard deviations is probably unlikely If we wanted to manipulate \\(M\\) instead of \\(A\\) we would have to consider a new DAG where \\(A\\) can’t influence \\(M\\) because we are controlling it. dag5.3 &lt;-dagitty(&quot;dag{A-&gt;D;M-&gt;D}&quot;) coordinates(dag5.3) &lt;-list(x=c(A=0,D=1,M=2),y=c(A=0,D=1,M=0)) drawdag(dag5.3) Let’s set \\(A\\) to equal 0 and just worry about \\(M\\) changing sim_dat &lt;- data.frame(M = seq(from = -2, to = 2, length.out = 30), A = 0) s &lt;- sim(m5.3_A, data = sim_dat, vars = &#39;D&#39;) plot(sim_dat$M, colMeans(s), ylim = c(-2,2), type = &#39;l&#39;, xlab = &#39;manipulated M&#39;, ylab = &#39;counterfactual D&#39;) shade(apply(s, 2, PI), sim_dat$M) mtext(&quot;Total counterfactual effect of M on D&quot;) You can see from this counterfactual that any value of \\(M\\) has very little effect on \\(D\\). We already expected this because we knew there was a weak effect in the model (bM) coeftab(m5.3) ## m5.3 ## a 0 ## bM -0.07 ## bA -0.61 ## sigma 0.79 ## nobs 50 5.1.5.4 Simulate counterfactuals by hand Let’s manipulate \\(A\\) again but simulate by hand A_seq &lt;- seq(from = -2, to = 2, length.out = 30) #grab samples from posterior post &lt;- extract.samples(m5.3_A) # use sapply to estimate M from values in post and A_seq M_sim &lt;- with(post, sapply(1:30, function(i) rnorm(1e3, aM + bAM * A_seq[i], sigma_M))) # use sapply to estimate D from values in post, M_sim, and A_seq D_sim &lt;- with(post, sapply(1:30, function(i) rnorm(1e3, a + bA*A_seq[i] + bM*M_sim[,i] , sigma_M))) #plot A_seq against D_sim colMeans plot(A_seq, colMeans(D_sim), ylim = c(-2,2), type = &#39;l&#39;, xlab = &#39;manipulated A&#39;, ylab = &#39;counterfactual D&#39;) shade(apply(D_sim, 2, PI), A_seq) mtext(&quot;total counterfactual effect of A on D&quot;) 5.2 Masked relationship This section is devoted to measuring multiple direct effects from multiple predictors on an outcome. This is particularly useful when the predictors seem to have little effect on the outcome on their own. We will load in some new data on milk composition from different primates library(rethinking) data(milk) d &lt;- milk str(d) ## &#39;data.frame&#39;: 29 obs. of 8 variables: ## $ clade : Factor w/ 4 levels &quot;Ape&quot;,&quot;New World Monkey&quot;,..: 4 4 4 4 4 2 2 2 2 2 ... ## $ species : Factor w/ 29 levels &quot;A palliata&quot;,&quot;Alouatta seniculus&quot;,..: 11 8 9 10 16 2 1 6 28 27 ... ## $ kcal.per.g : num 0.49 0.51 0.46 0.48 0.6 0.47 0.56 0.89 0.91 0.92 ... ## $ perc.fat : num 16.6 19.3 14.1 14.9 27.3 ... ## $ perc.protein : num 15.4 16.9 16.9 13.2 19.5 ... ## $ perc.lactose : num 68 63.8 69 71.9 53.2 ... ## $ mass : num 1.95 2.09 2.51 1.62 2.19 5.25 5.37 2.51 0.71 0.68 ... ## $ neocortex.perc: num 55.2 NA NA NA NA ... We will focus on the relationship between kilocalories per gram of milk and brain mass (neocortex percent). We will also include mass later on. d$K &lt;- standardize(d$kcal.per.g) d$N &lt;- standardize(d$neocortex.perc) d$M &lt;- standardize(log(d$mass)) First lets look at \\(K \\sim N\\) directly m5.5_draft &lt;- quap( alist( K ~ dnorm(mu, sigma), mu &lt;- a + bN*N, a ~ dnorm(0, 1), bN ~ dnorm(0, 1), sigma ~ dexp(1) ), data = d ) Ooops, too many NA values in the data dcc &lt;- d[complete.cases(d$K, d$N, d$M),] # 5.33 m5.5_draft &lt;- quap( alist( K ~ dnorm(mu, sigma), mu &lt;- a + bN*N, a ~ dnorm(0, 1), bN ~ dnorm(0, 1), sigma ~ dexp(1) ), data = dcc #update data source ) Let’s look at the validity of the priors prior &lt;- extract.prior(m5.5_draft) xseq &lt;- c(-2,2) mu &lt;- link(m5.5_draft, post = prior, data = list(N = xseq)) plot(NULL, xlim = xseq, ylim = xseq, xlab = &#39;neocortex % (std)&#39;, ylab = &#39;kilocal / g (std)&#39;) mtext(&#39;a ~ dnorm(0, 1) \\ bN ~ dnorm(0, 1)&#39;) for(i in 1:50){ lines(xseq, mu[i,], col = col.alpha(&#39;black&#39;,0.3)) } Not great… Because we know that these variables are standardized, we can constrain \\(\\alpha\\) to be a bit closer to 0. \\(\\beta_{N}\\) can also be tightened to avoid impossibly strong relationships. m5.5 &lt;- quap( alist( K ~ dnorm(mu, sigma), mu &lt;- a + bN*N, a ~ dnorm(0, 0.2), bN ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = dcc ) prior &lt;- extract.prior(m5.5) xseq &lt;- c(-2,2) mu &lt;- link(m5.5, post = prior, data = list(N = xseq)) plot(NULL, xlim = xseq, ylim = xseq, xlab = &#39;neocortex % (std)&#39;, ylab = &#39;kilocal / g (std)&#39;) mtext(&#39;a ~ dnorm(0, 0.2) \\ bN ~ dnorm(0, 0.5)&#39;) for(i in 1:50){ lines(xseq, mu[i,], col = col.alpha(&#39;black&#39;,0.3)) } Now we have lines that are still quite vague, but at least they largely fall within the observable data. Let’s turn to the posterior precis(m5.5) ## mean sd 5.5% 94.5% ## a 0.0399399 0.1544908 -0.2069662 0.2868460 ## bN 0.1332349 0.2237470 -0.2243560 0.4908257 ## sigma 0.9998209 0.1647083 0.7365852 1.2630565 And plotted xseq &lt;- seq(from = min(dcc$N)-0.15, to = max(dcc$N)+0.15, length.out = 30) mu &lt;- link(m5.5, data = list(N = xseq)) mu_mean &lt;- apply(mu, 2, mean) mu_PI &lt;- apply(mu, 2, PI) plot(K ~ N, data = dcc, col=col.alpha(rangi2, 0.5), xlab = &#39;neocortex % (std)&#39;, ylab = &#39;kilocal / g (std)&#39;) lines(xseq, mu_mean, lwd = 2) shade(mu_PI, xseq) Not a great relationship given the large residuals around the mean line. Let’s see how body mass does at explaining calorie content of milk m5.6 &lt;- quap( alist( K ~ dnorm(mu, sigma), mu &lt;- a + bM*M, a ~ dnorm(0, 0.2), bM ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = dcc #update data source ) precis(m5.6) ## mean sd 5.5% 94.5% ## a 0.04654154 0.1512801 -0.1952333 0.28831640 ## bM -0.28253570 0.1928820 -0.5907984 0.02572704 ## sigma 0.94928078 0.1570621 0.6982652 1.20029633 Let’s have a look xseq &lt;- seq(from = min(dcc$M)-0.15, to = max(dcc$M)+0.15, length.out = 30) mu &lt;- link(m5.6, data = list(M = xseq)) mu_mean &lt;- apply(mu, 2, mean) mu_PI &lt;- apply(mu, 2, PI) plot(K ~ M, data = dcc, col=col.alpha(rangi2, 0.5), xlab = &#39;log body mass (std)&#39;, ylab = &#39;kilocal / g (std)&#39;) lines(xseq, mu_mean, lwd = 2) shade(mu_PI, xseq) Negative relationship but still not great. Let’s build a model that incorporates both predictors m5.7 &lt;- quap( alist( K ~ dnorm(mu, sigma), mu &lt;- a + bN*N + bM*M, a ~ dnorm(0, 0.2), bN ~ dnorm(0, 0.5), bM ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = dcc ) precis(m5.7) ## mean sd 5.5% 94.5% ## a 0.06797532 0.1340022 -0.1461861 0.2821367 ## bN 0.67513341 0.2483041 0.2782956 1.0719713 ## bM -0.70294840 0.2207986 -1.0558272 -0.3500696 ## sigma 0.73804620 0.1324760 0.5263240 0.9497684 The interesting part here is how \\(\\beta_{M}\\) and \\(\\beta_{N}\\) have changed when they are considered together (m5.7). A plot can be made to show the comparison. coeftab_plot(coeftab(m5.5, m5.6, m5.7), pars = c(&#39;bM&#39;, &#39;bN&#39;)) Let’s see how these variables are related to better understand what happened pairs(~K + M + N, dcc) You can see where \\(M\\) and \\(N\\) interact there is some strong positive association. Now we can use our counterfactual skills to visualize the effect each predictor has on the outcome \\(K\\). xseq &lt;- seq(from = min(dcc$M)-0.15, to = max(dcc$M)+0.15, length.out = 30) mu &lt;- link(m5.7, data = data.frame(M = xseq, N=0)) mu_mean &lt;- apply(mu, 2, mean) mu_PI &lt;- apply(mu, 2, PI) plot(NULL, xlim = range(dcc$M), ylim = range(dcc$K), xlab = &#39;log body mass (std)&#39;, ylab = &#39;kilocal / g (std)&#39;) mtext(&#39;Counterfactual holding N = 0&#39;) lines(xseq, mu_mean, lwd = 2) shade(mu_PI, xseq) xseq &lt;- seq(from = min(dcc$N)-0.15, to = max(dcc$N)+0.15, length.out = 30) mu &lt;- link(m5.7, data = data.frame(N = xseq, M = 0)) mu_mean &lt;- apply(mu, 2, mean) mu_PI &lt;- apply(mu, 2, PI) plot(NULL, xlim = range(dcc$N), ylim = range(dcc$K), xlab = &#39;neocortex % (std)&#39;, ylab = &#39;kilocal / g (std)&#39;) mtext(&#39;Counterfactual holding M = 0&#39;) lines(xseq, mu_mean, lwd = 2) shade(mu_PI, xseq) OVERTHINKING BOX 5.3 Categorical variables How do we deal with non-continuous variables like categories (unordered, discrete)? First we will consider an easy binary example of categories from earlier. data(Howell1) d &lt;- Howell1 str(d) ## &#39;data.frame&#39;: 544 obs. of 4 variables: ## $ height: num 152 140 137 157 145 ... ## $ weight: num 47.8 36.5 31.9 53 41.3 ... ## $ age : num 63 63 65 41 51 35 32 27 19 54 ... ## $ male : int 1 0 0 1 0 1 0 1 0 1 ... 5.3.1 Binary categories Now we will see how sex influences height and weight in the Kalahari dataset. We can try and use the 0’s and 1’s already coded in the dataset for 1 meaning male and 0 meaning female. \\[\\begin{equation} h_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\ \\mu_{i} = \\alpha + \\beta_{m}m_{i}\\ \\alpha \\sim \\text{Normal}(178, 20)\\ \\beta_{m} \\sim \\text{Normal}(0, 10)\\ \\sigma \\sim \\text{Uniform}(0, 50) \\end{equation}\\] So by this definition, being female means \\(\\mu_{i}\\) is solely dependent on \\(\\alpha\\) where as males have both \\(\\alpha\\) and \\(\\beta_{m}m_{i}\\) influencing the height outcome. Here is a brief look at the priors for this use of category: mu_female &lt;- rnorm(1e4, 178, 20) mu_male &lt;- rnorm(1e4, 178, 20) + rnorm(1e4, 0, 10) precis(data.frame(mu_female, mu_male)) ## mean sd 5.5% 94.5% histogram ## mu_female 177.8739 20.01428 145.6530 209.7697 ▁▁▁▃▇▇▂▁▁ ## mu_male 177.9288 22.34394 142.2684 213.7899 ▁▁▁▃▇▇▂▁▁▁ As we can see here, the male prior is a bit wider because it is using both \\(\\alpha\\) and \\(\\beta_{m}\\) priors. To avoid these potentially problematic priors we can try Index variables. Index variables assign non-zero integers (in no particular order) to categorical variables. d$sex &lt;- ifelse(d$male == 1, 2, 1) str(d$sex) ## num [1:544] 2 1 1 2 1 2 1 2 1 2 ... Now we have males labeled as ‘2’ and females as ‘1.’ And now we can adjust our model to reflect this change. \\[\\begin{equation} h_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\ \\mu_{i} = \\alpha_{SEX[i]}\\ \\alpha_{j} \\sim \\text{Normal}(178, 20) \\text{for} j = 1..2\\ \\beta_{m} \\sim \\text{Normal}(0, 10)\\ \\sigma \\sim \\text{Uniform}(0, 50) \\end{equation}\\] Now our model will have \\(\\alpha\\) values for each category in the dataset. For this example we will have \\(\\alpha_{1}\\) and \\(\\alpha_{2}\\) representing our two sex variables. m5.8 &lt;- quap( alist( height ~ dnorm(mu, sigma), mu &lt;- a[sex], a[sex] ~ dnorm(178, 20), sigma ~ dunif(0, 50) ), data = d ) precis(m5.8, depth = 2) ## mean sd 5.5% 94.5% ## a[1] 134.91030 1.6069179 132.34214 137.47847 ## a[2] 142.57820 1.6974562 139.86533 145.29106 ## sigma 27.30969 0.8280218 25.98635 28.63303 Now we can pull samples from the posterior to see the expected difference between males and females or contrast post &lt;- extract.samples(m5.8) post$diff_fm &lt;- post$a[,1] - post$a[,2] precis(post, depth = 2) ## mean sd 5.5% 94.5% histogram ## sigma 27.303453 0.8343736 25.95509 28.627765 ▁▁▁▁▃▇▇▇▃▂▁▁▁ ## a[1] 134.903500 1.6175473 132.32788 137.514391 ▁▁▁▁▂▅▇▇▅▂▁▁▁▁▁ ## a[2] 142.560824 1.6936249 139.85860 145.276115 ▁▁▁▁▃▇▇▇▃▂▁▁▁▁ ## diff_fm -7.657324 2.3177677 -11.34463 -3.879652 ▁▁▁▃▇▇▃▁▁▁▁ So the variable of interest here is the diff_fm which tells the difference from \\(\\alpha_{1}\\) and \\(\\alpha_{2}\\). (or you could manually subtract the parameter means by hand). 5.3.2 Many categories data(milk) d &lt;- milk levels(d$clade) ## [1] &quot;Ape&quot; &quot;New World Monkey&quot; &quot;Old World Monkey&quot; &quot;Strepsirrhine&quot; d$clade_id &lt;- as.integer(d$clade) d$K &lt;- standardize(d$kcal.per.g) m5.9 &lt;- quap( alist( K ~ dnorm(mu, sigma), mu &lt;- a[clade_id], a[clade_id] ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) labels &lt;- paste(&#39;a[&#39;, 1:4, &#39;]:&#39; , levels(d$clade), sep = &#39;&#39;) precis_plot(precis(m5.9, depth = 2, pars = &#39;a&#39;), labels = labels) And you can easily add additional categorical variables to the same model. Here we can add a random assignment of house which can take the values of [1] Gryffindor, [2] Hufflepuff, [3] Ravenclaw, and [4] Slytherin. set.seed(11) d$house &lt;- sample(rep(1:4, each = 8), size = nrow(d)) Now we can update our model to also consider an index variable of house and look at the expected values for each house. m5.10 &lt;- quap( alist( K ~ dnorm(mu, sigma), mu &lt;- a[clade_id] + h[house], a[clade_id] ~ dnorm(0, 0.5), h[house] ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) precis(m5.10, depth = 2) ## mean sd 5.5% 94.5% ## a[1] -0.48971430 0.26371913 -0.91118840 -0.06824019 ## a[2] 0.38280264 0.26487240 -0.04051461 0.80611988 ## a[3] 0.69787422 0.29123455 0.23242516 1.16332328 ## a[4] -0.59652803 0.30310713 -1.08095177 -0.11210430 ## h[1] -0.19874066 0.27019092 -0.63055794 0.23307662 ## h[2] 0.07604072 0.28651618 -0.38186748 0.53394892 ## h[3] 0.09333180 0.26964281 -0.33760950 0.52427309 ## h[4] 0.02280471 0.27938165 -0.42370113 0.46931054 ## sigma 0.70188166 0.09466191 0.55059364 0.85316968 "],["the-haunted-dag-the-casual-terror.html", "Chapter 6 The haunted dag &amp; the casual terror 6.1 Multicollinearity 6.2 Post-treatrment bias 6.3 Collider bias 6.4 Confronting confounding", " Chapter 6 The haunted dag &amp; the casual terror If a review panel receives 200 proposals and can only fund 20 (10%) based on newsworthiness and trustworthiness, a negative correlation is bound to occur from strong selection. set.seed(11) N &lt;- 200 #number of proposals p &lt;- 0.1 #proportion to select for funding #set up random pairs of newsworthiness and trustworthiness nw &lt;- rnorm(N) tw &lt;- rnorm(N) #select top 10% of total score s &lt;- nw + tw #total score q &lt;- quantile(s, 1-p) #top 10% threshold selected &lt;- ifelse(s &gt;= q, TRUE, FALSE) # assign TRUE to selected values col.factor &lt;- ifelse(s &gt;= q, 1, 0) # plotting colour code df &lt;- data.frame(nw, tw, s, selected, as.factor(col.factor)) # stitch a df for plotting df2 &lt;- df[selected == TRUE,] # grab selected values for quick linear model plot(df$nw, df$tw, col = df$as.factor.col.factor, xlab = &#39;newsworthiness&#39;, ylab = &#39;trustworthiness&#39;) abline(lm(nw ~ tw, data = df2), col = &#39;red&#39;) mtext(cor(tw[selected], nw[selected])) Strong selection induces a negative correlation among the criteria used in the selection. Why? If the only way to corss the threshold is to score high, it is more common to score high on one item than on both. — p.161 This phenomenon is referred to as Berkson’s Paradox. But you can remember it as the selection-distortion effect. This effect can be very common inside multiple regression models. When you add a predictor to a model the model wants to statistically include that parameter at any cost. This can be referred to as collider bias. 6.1 Multicollinearity To have multicollinearity, there must be a very strong association between two or more predictors in your model. 6.1.1 Example: Try to predict height from length of a person’s legs. N &lt;- 100 #number of people set.seed(11) height &lt;- rnorm(N, 10, 2) #simulate total height leg_prop &lt;- runif(N, 0.4, 0.5) #leg as a proportion of height leg_left &lt;- leg_prop*height + rnorm(N, 0, 0.02) #left leg with error leg_right &lt;- leg_prop*height + rnorm(N, 0, 0.02) # right leg with error (only error will vary here) d &lt;- data.frame(height, leg_left, leg_right) Now we can build a model that predicts height from leg length. We set the leg_prop to be bound from 0.4 to 0.5 so on average the leg proportions should be about 45% (0.45) of a persons height. If we were to guess then what the \\(\\beta\\) coefficients would be we would ballpark somewhere around the average height (10) divided by 45% of the average height (10/0.45 = 4.5). This 10 / 4.5 is about 2.2. Let’s see if our guess is close. library(rethinking) m6.1 &lt;- quap( alist( height ~ dnorm(mu, sigma), mu &lt;- a + bl*leg_left + br*leg_right, a ~ dnorm(10, 5), #really wide priors bl ~ dnorm(2, 5), br ~ dnorm(2, 5), sigma ~ dexp(1) ), data = d) precis(m6.1) ## mean sd 5.5% 94.5% ## a 0.95222088 0.31790120 0.4441534 1.4602884 ## bl 0.06223552 1.99578411 -3.1274129 3.2518840 ## br 1.93732727 1.98698630 -1.2382606 5.1129151 ## sigma 0.59950562 0.04221453 0.5320386 0.6669726 These seem a bit wonky. What happened? Let’s plot to get a better understanding precis_plot(precis(m6.1)) No matter how many times we run the simulation the leg length appears to be unimportant. This is because the model is trying to tell us how important knowing the other leg length is once we know the length of one leg. If we only knew right leg data, the right leg length would be pretty important. If we were to look at the posterior distribution of the two leg length coefficients we would see that one is not more informative than the other post &lt;- extract.samples(m6.1) plot(bl ~ br, post, col=col.alpha(rangi2, 0.1), pch = 16) Basically this plot shows that any combination of br and bl will fall along a very thin line, suggesting any combination of these values will have equal predictive power. We fit a model that looks like this: \\[\\begin{equation} y_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\ \\mu_{i} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x_{i}\\ \\end{equation}\\] Because the leg lengths are very similar we can write \\(x_{i}\\) twice, and because of this the model golem simplifies the model to be: \\[\\begin{equation} \\mu_{i} = \\alpha + (\\beta_{1} + \\beta_{2})x_{i}\\ \\end{equation}\\] In this version \\(\\beta_{1}\\) and \\(\\beta_{2}\\) can’t be considered separately because they do not have individual influences on \\(\\mu\\). The sum of the coefficients does however influence \\(\\mu\\), so we can calculate that to get a better idea of how leg lengths influence height. sum_blbr &lt;- post$bl + post$br dens(sum_blbr, col=rangi2, lwd = 2, xlab = &#39;sum of bl and br&#39;) If you recall we did estimate the coefficient to be somewhere slightly larger than 2 which is shown here. Here is a single leg model for comparison : m6.2 &lt;- quap( alist( height ~dnorm(mu,sigma), mu &lt;-a+bl*leg_left, a ~dnorm(10,100), bl ~dnorm(2,10), sigma ~dexp(1) ), data = d ) ## Caution, model may not have converged. ## Code 1: Maximum iterations reached. precis(m6.2) ## Warning in sqrt(diag(vcov(model))): NaNs produced ## Warning in sqrt(diag(vcov(model))): NaNs produced ## Warning in sqrt(diag(vcov(model))): NaNs produced ## mean sd 5.5% 94.5% ## a 25.5498117 54.248421 -61.14964 112.24927 ## bl 0.1562758 9.168886 -14.49738 14.80993 ## sigma 479.5199897 NaN NaN NaN 6.1.2 Multicollinear milk Let’s look at some real data for an example of correlated predictors data(milk) d &lt;- milk d$K &lt;- standardize(d$kcal.per.g) d$F &lt;- standardize(d$perc.fat) d$L &lt;- standardize(d$perc.lactose) Let’s see how percent fat and percent lactose do at predicting energy content of milk #model for percent fat m6.3 &lt;- quap( alist( K ~ dnorm(mu, sigma), mu &lt;- a + bF*F, a ~ dnorm(0, 0.2), bF ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) #model for percent lactose m6.4 &lt;- quap( alist( K ~ dnorm(mu, sigma), mu &lt;- a + bL*L, a ~ dnorm(0, 0.2), bL ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) precis(m6.3) ## mean sd 5.5% 94.5% ## a -2.185005e-06 0.07725163 -0.1234652 0.1234608 ## bF 8.618959e-01 0.08426050 0.7272313 0.9965604 ## sigma 4.510157e-01 0.05870685 0.3571909 0.5448406 precis(m6.4) ## mean sd 5.5% 94.5% ## a 7.997789e-07 0.06661645 -0.1064652 0.1064668 ## bL -9.024550e-01 0.07132863 -1.0164520 -0.7884581 ## sigma 3.804660e-01 0.04958285 0.3012231 0.4597090 Pretty much polar opposites of each other. More fat = more energy, more lactose = less energy. But what happens if they are considered together? m6.5 &lt;- quap( alist( K ~ dnorm(mu, sigma), mu &lt;- a + bF*F + bL*L, a ~ dnorm(0, 0.2), bF ~ dnorm(0, 0.5), bL ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) precis(m6.5) ## mean sd 5.5% 94.5% ## a -5.951129e-06 0.06603601 -0.10554425 0.1055323 ## bF 2.435066e-01 0.18357932 -0.04988858 0.5369018 ## bL -6.780742e-01 0.18377740 -0.97178595 -0.3843624 ## sigma 3.767433e-01 0.04918443 0.29813710 0.4553495 Both coefficients have shifted much closer to 0 and the standard deviations have more than doubled. pairs(~ kcal.per.g + perc.fat + perc.lactose, data = d, col = rangi2) Pay attention to the first column of graphs where kcal.per.g is virtually mirrored across perc.fat and perc.lactose. These two predictors are so strongly correlated that knowing one is enough to predict kcal.per.g. 6.2 Post-treatrment bias Post-treatment bias comes from included variable bias and experimental design. Say you grew plants in different soil types to test anti-fungal resistance of the soil. You measure the height of the plant at the start of the trial, add in the soil treatment, measure the height of the plant at the end of the trial, and check for presence of fungus. So you have initial height, final height, treatment, and fungus. What should be in the model? If you are hypothesizing that fungus slows growth, final height is the outcome of interest and fungus presence should be discarded because it is a post-treatment effect. Let’s simulate to be sure set.seed(11) N &lt;- 100 #number of plants h0 &lt;- rnorm(N,10,2) #initial height treatment &lt;- rep(0:1, each = N/2) #assign treatments fungus &lt;- rbinom(N, size = 1, prob = 0.5 - treatment*0.4) #assign fungus h1 &lt;- h0 + rnorm(N, 5-3*fungus) #simulate growth d &lt;- data.frame(h0=h0, h1=h1, treatment=treatment, fungus=fungus) precis(d) ## mean sd 5.5% 94.5% histogram ## h0 9.752973 1.8289342 6.976258 12.74917 ▁▂▂▇▅▅▃▂▁▁ ## h1 13.784304 2.5918058 9.598218 17.75557 ▁▂▃▃▇▇▅▃▅▃▁▁▁ ## treatment 0.500000 0.5025189 0.000000 1.00000 ▇▁▁▁▁▁▁▁▁▇ ## fungus 0.350000 0.4793725 0.000000 1.00000 ▇▁▁▁▁▁▁▁▁▃ 6.2.1 A prior is born If we didn’t know how the data was created and actually conducted the plant experiment, we would be able to assume that all plants will have grown some amount from the beginning to the end of the experiment. In this way we can scale the growth to be proportion of initial height such that: \\[\\begin{equation} h_{1,i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\ \\mu_{i} = h_{0,i} \\times p \\end{equation}\\] Here \\(p\\) is a parameter that is standing in for the proportion grown from \\(h_{0}\\) to \\(h_{1}\\). In another way, \\(p = h_{1}/h_{0}\\) and \\(p = 1\\) would mean no growth and \\(p = 2\\) means the plant has doubled it’s size. Because \\(p\\) is a proportion, it must be positive, even if the plant dies (\\(p = 0\\)) so we will use the log normal distribution as a prior. sim_p &lt;- rlnorm(1e4, 0, 0.25) #simulate p values precis(data.frame(sim_p)) ## mean sd 5.5% 94.5% histogram ## sim_p 1.034501 0.2606664 0.6749326 1.49171 ▁▁▃▇▇▃▁▁▁▁▁▁ dens(sim_p) This prior distribution expects that there should be roughly no growth but anywhere from 40% shrinkage to 50% growth. Let’s put the prior in the model without any predictors. m6.6 &lt;- quap( alist( h1 ~ dnorm(mu, sigma), mu &lt;- h0*p, p ~ dlnorm(0, 0.25), sigma ~ dexp(1) ), data = d ) precis(m6.6) ## mean sd 5.5% 94.5% ## p 1.400621 0.01775487 1.372246 1.428997 ## sigma 1.762761 0.12303782 1.566123 1.959399 About 40% growth on average from the model. But what about adding some predictors? We will add both treatment and fungus. \\[\\begin{equation} h_{1,i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\ \\mu_{i} = h_{0, i} \\times p\\ p = \\alpha + \\beta_{T}T_{i} + \\beta_{F}F_{i}\\ \\alpha \\sim \\text{Log-Normal}(0, 0.25)\\ \\beta_{T} \\sim \\text{Normal}(0, 0.5)\\ \\beta_{F} \\sim \\text{Normal}(0, 0.5)\\ \\sigma \\sim \\text{Exponential}(1) \\end{equation}\\] Notice how \\(p\\) is now a linear model parameterized by the predictors treatment and fungus. m6.7 &lt;- quap( alist( h1 ~ dnorm(mu, sigma), mu &lt;- h0 * p, p &lt;- a + bt*treatment + bf*fungus, a ~ dlnorm(0, 0.2), bt ~ dnorm(0, 0.5), bf ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) precis(m6.7) ## mean sd 5.5% 94.5% ## a 1.52442223 0.02589644 1.48303472 1.5658097500 ## bt -0.04742812 0.02926555 -0.09420012 -0.0006561103 ## bf -0.30101915 0.03114703 -0.35079812 -0.2512401777 ## sigma 1.20701921 0.08461445 1.07178898 1.3422494372 So now a is like the previous p with a similar average growth of about 50%. Treatment (bt) seems to have no effect and fungus is slowing growth. How could treatment not have an effect if we built the data to ensure that it did? 6.2.2 Blocked by consequence Because treatment influenced fungus presence and was not included in the growth estimation, it adds very little information once we know if fungus was present or not. here is a model without fungus m6.8 &lt;- quap( alist( h1 ~ dnorm(mu, sigma), mu &lt;- h0 * p, p &lt;- a + bt*treatment, a ~ dlnorm(0, 0.2), bt ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) precis(m6.8) ## mean sd 5.5% 94.5% ## a 1.3410577 0.02474841 1.30150494 1.3806104 ## bt 0.1108954 0.03387262 0.05676043 0.1650304 ## sigma 1.6826776 0.11753297 1.49483725 1.8705180 Now we have a strong positive coefficient (bt) for treatment on growth of the plants. This is good news. It means that the hypothesized mechanism (fungus slowing growth) appears to be true because including fungus zeros the treatment effect. 6.2.3 Fungus and d-separation Let’s make a DAG for this plant model library(dagitty) plant_dag &lt;- dagitty(&quot;dag { H_0 -&gt; H_1 F -&gt; H_1 T -&gt; F }&quot;) coordinates(plant_dag) &lt;- list(x = c(H_0 = 0, T = 2, F = 1.5, H_1 = 1), y = c(H_0 = 0, T = 0, F = 0, H_1 = 0)) drawdag(plant_dag) In this framework treatment \\(T\\) influences fungus \\(F\\) which influences final plant height \\(H_{1}\\). And the initial plant height \\(H_{0}\\) has a separate influence on \\(H_{1}\\). When we include fungus in the model we are blocking treatment from influencing the outcome \\(H_{1}\\). When a parameter is blocking the effect of another, it is called **d-separation* or directional separation. When we condition the model on \\(F\\), we create an independence between \\(T\\) and \\(H_{1}\\). Here is how we can gather all conditional independencies for the above DAG: impliedConditionalIndependencies(plant_dag) ## F _||_ H_0 ## H_0 _||_ T ## H_1 _||_ T | F So we have three independencies. The first two simply separate the left side \\(H_{0}\\) from the right side. The third one is the one of interest where \\(H_{1}\\) is independent of \\(T\\) when conditioned on \\(F\\). There is no additional information gained by including \\(T\\) after \\(F\\). Observational experiments have additional trouble with unobserved variables. What if there was something else that was influencing both \\(H_{1}\\) and \\(F\\) and broke their dependence? Let’s say moisture is that missing link. So now we have broken the path from \\(T\\) to \\(H_{1}\\) so there shouldn’t be an influence of \\(T\\) on \\(H_{1}\\). Let’s update the data set.seed(11) N &lt;- 1000 h0 &lt;- rnorm(N, 10, 2) treatment &lt;- rep(0:1, each = N/2) M &lt;- rbern(N) fungus &lt;- rbinom(N, size = 1, prob = 0.5 - 0.4*treatment + 0.4*M) h1 &lt;- h0 + rnorm(N, 5 + 3*M) d2 &lt;- data.frame(h0=h0, h1=h1, treatment=treatment, fungus=fungus) So \\(M\\) was used to influence \\(F\\) and \\(H_{1}\\) but was not included in the new data. Let’s see how this changes our models m6.7b &lt;- quap( alist( h1 ~ dnorm(mu, sigma), mu &lt;- h0 * p, p &lt;- a + bt*treatment + bf*fungus, a ~ dlnorm(0, 0.2), bt ~ dnorm(0, 0.5), bf ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d2 ) m6.8b &lt;- quap( alist( h1 ~ dnorm(mu, sigma), mu &lt;- h0 * p, p &lt;- a + bt*treatment, a ~ dlnorm(0, 0.2), bt ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d2 ) precis(m6.7b) ## mean sd 5.5% 94.5% ## a 1.51625146 0.01313354 1.49526152 1.53724139 ## bt 0.06862459 0.01398489 0.04627403 0.09097516 ## bf 0.16527116 0.01400138 0.14289426 0.18764807 ## sigma 2.06601352 0.04612638 1.99229465 2.13973239 precis(m6.8b) ## mean sd 5.5% 94.5% ## a 1.628343101 0.009667407 1.61289272 1.64379348 ## bt 0.001884175 0.013636836 -0.01991012 0.02367847 ## sigma 2.203247647 0.049185180 2.12464023 2.28185506 Now when fungus is in the model, bt seems to have a positive effect. The model without fungus shows bt to be useless. How did \\(M\\) have this effect to make \\(T\\) seem important when we know that it shouldn’t? 6.3 Collider bias Returning now to the selection bias example with trustworthiness and newsworthiness, let’s see Collider bias in action Any parameter with two arrows pointing at it is called a collider. For this example, if we knew \\(S\\) and \\(T\\) for example, then adding \\(N\\) would offer no additional information. If a proposal was accepted (\\(S = \\text{True}\\)) and we knew it had a high value of trustworthiness (\\(T\\)) we would already have an idea that the newsworthiness (\\(N\\)) would be low. 6.3.1 Collider for false sorrow Is age associated with happiness? For hypothetical reasons, we will consider that happiness in part is determined at birth and is generally static through time. Additionally, happy people tend to get married more than unhappy people. Marriage is also influenced by age with people who live longer eventually get married. Together: c2 &lt;- dagitty(&quot;dag{ H -&gt; M A -&gt; M }&quot;) coordinates(c2) &lt;- list(x = c(H = 0, M = 1, A = 2), y = c(H = 0, M = 0, A = 0)) drawdag(c2) So we have set up marriage to be a collider. If we were to include marriage as a predictor in a regression model, it will artificially create an association between happiness and age. There is a algorithm built in the rethinking package that can simulate data for this example. each year 20 people are born with uniformly distributed happiness each year age increases but happiness does not after age 18 marriage is possible and odds of marriage are based on happiness once married, individuals remain married after age 65 individuals leave the system d &lt;- sim_happiness(seed = 1977, N_years = 1000) precis(d) ## mean sd 5.5% 94.5% histogram ## age 3.300000e+01 18.768883 4.000000 62.000000 ▇▇▇▇▇▇▇▇▇▇▇▇▇ ## married 3.007692e-01 0.458769 0.000000 1.000000 ▇▁▁▁▁▁▁▁▁▃ ## happiness -1.000070e-16 1.214421 -1.789474 1.789474 ▇▅▇▅▅▇▅▇ d now contains 1000 years of age, happiness, and marriage status. To avoid unmarried children from altering the results we can remove them. d2 &lt;- d[d$age &gt; 17,] #pick out adults d2$A &lt;- (d2$age - 18) / (65 - 18) #set age back to zero and bound between 0 and 1 plot(d2$age, d2$happiness, col = as.factor(d2$married)) Now we can create the model of happiness from age and marriage status d2$mid &lt;- d2$married + 1 #have marriage status either 1 (not) or 2 (married) m6.9 &lt;- quap( alist( happiness ~ dnorm(mu, sigma), mu &lt;- a[mid] + bA*A, a[mid] ~ dnorm(0,1), bA ~ dnorm(0, 2), sigma ~ dexp(1) ), data = d2 ) precis(m6.9, depth = 2) ## mean sd 5.5% 94.5% ## a[1] -0.2350877 0.06348986 -0.3365568 -0.1336186 ## a[2] 1.2585517 0.08495989 1.1227694 1.3943340 ## bA -0.7490274 0.11320112 -0.9299447 -0.5681102 ## sigma 0.9897080 0.02255800 0.9536559 1.0257600 Our model believes that there is a strong negative relationship between age and happiness. The alpha values for intercepts make sense though with a[1] identifying non-married folks and a[2] identifying married people. What about if we ignore marriage? m6.10 &lt;- quap( alist( happiness ~ dnorm(mu, sigma), mu &lt;- a + bA*A, a ~ dnorm(0,1), bA ~ dnorm(0, 2), sigma ~ dexp(1) ), data = d2 ) precis(m6.10) ## mean sd 5.5% 94.5% ## a 1.649248e-07 0.07675015 -0.1226614 0.1226617 ## bA -2.728620e-07 0.13225976 -0.2113769 0.2113764 ## sigma 1.213188e+00 0.02766080 1.1689803 1.2573949 And now the once negative association is gone. 6.3.2 The haunted DAG Sometimes colliders are created from unobserved variables Here we have a DAG that describes grandparents \\(G\\) and parents \\(P\\) influence on their children’s academic achievement \\(C\\). But what if there were influeneces that couldn’t be measured like neighbourhood effects or something that would effect the household of the parents and children but not grandparents. We will call this \\(U\\). Even if \\(U\\) remains unmeasured, it can still introduce bias in the model. Our set up: 1. \\(P\\) is a function of \\(G\\) and \\(U\\) 2. \\(C\\) is a function of \\(G\\), \\(P\\), and \\(U\\) 3. \\(G\\) and \\(U\\) are not influenced by any other known variables N &lt;- 200 #number of families b_GP &lt;- 1 #effect of G on P b_GC &lt;- 0 #effect of G on C b_PC &lt;- 1 #effect of P on C b_U &lt;- 2 #effect of U on P and C #random observations set.seed(11) U &lt;- 2*rbern(N, 0.5) - 1 G &lt;- rnorm(N) P &lt;- rnorm(N, b_GP*G + b_U*U) C &lt;- rnorm(N, b_PC*P + b_GC*G + b_U*U) d &lt;- data.frame(C=C, P=P, G=G, U=U) So if we wanted to know the influence of \\(G\\) on \\(C\\) we would also have to include \\(P\\) because some effect passes from \\(G\\) to \\(C\\) through \\(P\\). m6.11 &lt;- quap( alist( C ~ dnorm(mu, sigma), mu &lt;- a + b_PC*P + b_GC*G, a ~ dnorm(0, 1), c(b_PC, b_GC) ~ dnorm(0, 1), sigma ~ dexp(1) ), data = d ) precis(m6.11) ## mean sd 5.5% 94.5% ## a 0.04096459 0.09759488 -0.1150109 0.1969401 ## b_PC 1.84994960 0.04376129 1.7800106 1.9198886 ## b_GC -0.83312218 0.10787908 -1.0055338 -0.6607106 ## sigma 1.37707320 0.06850756 1.2675849 1.4865615 The estimated effect of parents on children seems to be too large, likely from the input of \\(U\\) which the model doesn’t know about. But the negative effect of grandparents on their grandchildren is absurd. If we were to plot the grandparent education against grandchild education we would see positive relationships in each neighbourhood. But if we subsampled by middle percentiles of parent education we would see the negative relationship from the above model. This is possible because of how we constructed the data using \\(U\\) as an influence on \\(P\\) and \\(C\\). If the model knows \\(P\\) then the \\(G\\) parameter secretly tells the model about \\(U\\) even though it isn’t included in the model. This is a confusing concept to explain so consider the graph above. The filled in dots are family groups where parents have roughly the same education level. So here when the model conditions on \\(P\\) (imagine only the filled in dots exist) the outcome of \\(G\\) on \\(C\\) appears to be negative where we are selecting the top of the ‘bad neighbourhood’ families, and the bottom of the ‘good neighbourhood’ families. So measuring \\(U\\) and including it in the model is the only real way to get around this. m6.12 &lt;- quap( alist( C ~ dnorm(mu, sigma), mu &lt;- a + b_PC*P + b_GC*G + b_U*U, a ~ dnorm(0, 1), c(b_PC, b_GC, b_U) ~ dnorm(0, 1), sigma ~ dexp(1) ), data = d ) precis(m6.12) ## mean sd 5.5% 94.5% ## a 0.12195789 0.07084901 0.008727491 0.2351883 ## b_PC 1.04847302 0.06712619 0.941192398 1.1557536 ## b_GC -0.06698041 0.09661348 -0.221387419 0.0874266 ## b_U 2.04106421 0.15035130 1.800773788 2.2813546 ## sigma 0.99391904 0.04952629 0.914766468 1.0730716 Once we account for \\(U\\), we are able to recover our slopes that we set previously. (0, 1, 2) 6.4 Confronting confounding Let’s see if we can tie it all together to deal with the multiple ways condfounding appears. Suppose we are interested in the relationship between education \\(E\\) and wages \\(W\\). Unfortunately there are multiple unobserved variables that can influence both \\(E\\) and \\(W\\) such as neighbourhood, parents, friends, etc. We will call these \\(U\\). So if we are interested in \\(W\\) as the outcome and \\(E\\) as the predictor, it is confounded by the two paths that lead connect \\(E\\) and \\(W\\). \\(E \\rightarrow W\\) and \\(E \\leftarrow U \\rightarrow W\\) are both considered paths as the direction of the arrows doesn’t matter in this sense. What matters is that \\(E\\) and \\(W\\) have two different connections, one of which has an unobserved piece. If we were able to have absolute control \\(E\\) it would break the second path by blocking the influence of \\(U\\) on \\(E\\). When we are unable to manipulate \\(E\\) we have to condition on \\(U\\) so that knowing \\(U\\) disrupts any information gained by knowing \\(E\\). If \\(U\\) in this case was regions and different regions had different average wealth levels. Higher wealth regions \\(U\\) would have higher wages contributing to wealth \\(W\\) and to get high \\(W\\) you need higher education \\(E\\). So once you condition on \\(U\\) knowing \\(E\\) doesn’t add any useful information to the model. But knowing \\(E\\) still informs us of \\(W\\). 6.4.1 Shutting the backdoor Blocking confounding paths from a predictor \\(X\\) to an outcome of interest \\(Y\\) is called shutting the backdoor. Above, the \\(E \\leftarrow U \\rightarrow W\\) path would be considered a backdoor path. Here we will run through the four types of paths that can crop up and possibly create backdoors FORK: \\(X \\leftarrow Z \\rightarrow Y\\) ; \\(Z\\) influences both \\(X\\) and \\(Y\\) and conditioning on \\(Z\\) will make \\(X\\) and \\(Y\\) independent PIPE: \\(X \\rightarrow Z \\rightarrow Y\\) ; \\(X\\) influences \\(Z\\) which influences \\(Y\\) and conditioning on \\(Z\\) will make \\(X\\) and \\(Y\\) independent COLLIDER: \\(X \\rightarrow Z \\leftarrow Y\\) ; No relation exists between \\(X\\) and \\(Y\\) UNLESS you condition on \\(Z\\) which will open the path but doesn’t contain any causation DESCENDENT: \\(X \\rightarrow Z \\leftarrow Y\\) &amp; \\(Z \\rightarrow D\\) ; It is like the collider but \\(Z\\) is a parent of descendent \\(D\\) and conditioning on \\(D\\) will be a partial condition on \\(Z\\). Note that descendents aren’t only present in colliders. They could crop up in many places And here is the proposed recipe on how to open or close these potential backdoors 1. List all paths from \\(X\\) to \\(Y\\) 2. Classify paths from step 1 as open (fork or pipe) or closed (collider) 3. Classify paths as backdoor paths (arrow entering \\(X\\)) 4. If there are any open backdoor paths, decide which variable to condition on to close it 6.4.2 Two roads As an example, we have an exposure of interest \\(X\\) on outcome \\(Y\\). \\(A\\), \\(B\\), and \\(C\\) are observed with \\(U\\) being unobserved. Step 1: list all paths from \\(X\\) to \\(Y\\) \\(X \\rightarrow Y\\) \\(X \\leftarrow U \\leftarrow A \\rightarrow C \\rightarrow Y\\) \\(X \\leftarrow U \\rightarrow B \\leftarrow C \\rightarrow Y\\) Step 2: classify all paths as open or closed Open (direct) Open Closed on B Step 3: classify backdoor paths NA (direct) backdoor backdoor (closed) Step 4: Find which variable to condition on to make \\(X\\) and \\(Y\\) independent NA \\(A\\) or \\(C\\) should work as \\(U\\) is unobserved NA We can also ask our computer to do this for us if we know the DAG dag_6.1 &lt;- dagitty(&quot;dag { U [unobserved] X -&gt; Y X &lt;- U &lt;- A -&gt; C -&gt; Y U -&gt; B &lt;- C }&quot;) adjustmentSets(dag_6.1, exposure = &quot;X&quot;, outcome = &quot;Y&quot;) ## { C } ## { A } 6.4.3 Backdoor waffles BW_dag &lt;- dagitty(&quot;dag{ A -&gt; M A -&gt; D S -&gt; A S -&gt; M S -&gt; W W -&gt; D M -&gt; D }&quot;) coordinates(BW_dag) &lt;- list(x = c(A = 0, S = 0, W = 2, M = 1, D = 2), y = c(A = 0, S = -1, W = -1, M = -0.5, D = 0)) drawdag(BW_dag) Let’s go back to the waffle divorce rate example in ch. 5. So what we want to know is if the number of Waffle House restaurants \\(W\\) actually influences divorce rate \\(D\\) in each state. \\(A\\) is median age of marriage, \\(M\\) is the marriage rate of the state and \\(S\\) tells us if it is a southern state or not. So from the graph, \\(S\\) has low age \\(A\\) high marriage rate \\(M\\) (through \\(S \\rightarrow M\\) and \\(S \\rightarrow A \\rightarrow M\\)) and more waffle houses \\(W\\) (\\(S \\rightarrow W\\)). Both \\(A\\) and \\(M\\) influence \\(D\\). Step 1: list paths from \\(W\\) to \\(D\\) \\(W \\rightarrow D\\) \\(W \\leftarrow S \\rightarrow M \\rightarrow D\\) \\(W \\leftarrow S \\rightarrow A \\rightarrow M \\rightarrow D\\) \\(W \\leftarrow S \\rightarrow A \\rightarrow D\\) Step 2: classify open or closed direct open open open Step 3: classify backdoors direct backdoor backdoor backdoor Step 4: where do we condition? NA \\(S\\) \\(S\\) \\(S\\) We can ask the computer to confirm dag_6.2 &lt;- dagitty(&quot;dag { A -&gt; D A -&gt;M -&gt; D A &lt;- S -&gt; M S -&gt; W -&gt; D }&quot;) adjustmentSets(dag_6.2, exposure = &quot;W&quot;, outcome = &quot;D&quot;) ## { A, M } ## { S } We could do \\(A\\) and \\(M\\) together or just condition on \\(S\\). What about implied conditonal independencies? impliedConditionalIndependencies(dag_6.2) ## A _||_ W | S ## D _||_ S | A, M, W ## M _||_ W | S From this output we can say that: (i) \\(A\\) is independent of \\(W\\) when conditioned on \\(S\\) (ii) \\(D\\) is independent of \\(S\\) when conditioned on \\(A\\), \\(M\\), and \\(W\\) (iii) \\(M\\) is independent of \\(W\\) when conditioned on \\(S\\). "],["ulysses-compass.html", "Chapter 7 Ulysses’ compass 7.1 The problem with parameters 7.2 Entropy and accuracy 7.3 Golem taming: regularization 7.4 Predicting predictive accuracy 7.5 Model comparison", " Chapter 7 Ulysses’ compass The hero of Homer’s Odyssey, Ulysses, had two navigate between two dire consequences to complete his voyage. On one side, a beast with many heads and a taste for men, on the other side, a sea monster that sank boats. In statistical modelling, we are navigating equally treacherous waters while trying to avoid two things Overfitting Underfitting Both dangers provide the same result, poor predictions. All this while also avoiding confounds as we’ve seen. 7.1 The problem with parameters Most people will describe the performance of a model by the \\(R^2\\) term or amount of variance explained \\[R^{2} = \\frac{\\text{var(outcome)} - \\text{var(residuals)}} {\\text{var(outcome)}} = 1 - \\frac{\\text{var(residuals)}}{\\text{var(outcome)}}\\] But \\(R^2\\) is misleading because adding in more predictor variables almost always improves model fit, even if they are a bunch of random numbers that aren’t related to the outcome. 7.1.1 More parameters (almost) always improve fit Unfortunately overfitting happens automatically. To see this in action, we will look at seven hominin species’ average body mass and brain volume. sppnames &lt;- c(&quot;afarensis&quot;, &quot;aricanus&quot;, &quot;habilis&quot;,&quot;boisei&quot;,&quot;rudolfensis&quot;, &quot;ergaster&quot;,&quot;sapiens&quot;) brainvolcc &lt;- c(438, 452, 612, 521, 752, 871, 1350) masskg &lt;- c(37.0, 35.5, 34.5, 41.5, 55.5, 61, 53.5) d &lt;- data.frame(species=sppnames, brain=brainvolcc, mass=masskg) If we were interested in brain volume as a function of body mass, we could fit a polynomial regression (see ch. 4). But first we would have to standardize the parameters. Here we do the same as we normally do with the predictor, body mass. The outcome, brain volume, we have in proportion of the largest brain. This will avoid negative brain values. d$mass_std &lt;- (d$mass - mean(d$mass))/sd(d$mass) d$brain_std &lt;- d$brain / max(d$brain) Let’s build the linear model first. We will consider \\(\\sigma\\) to be log-normal to keep it positive. We will also include rather vague priors for \\(\\alpha\\) and \\(\\beta\\). library(rethinking) m7.1 &lt;- quap( alist( brain_std ~ dnorm(mu, exp(log_sigma)), mu &lt;- a + b*mass_std, a ~ dnorm(0, 0.5), b ~ dnorm(0, 10), log_sigma ~ dnorm(0, 1) ), data = d ) Rethinking OLS and Bayesian m7.1_OLS &lt;- lm(brain_std ~ mass_std, data = d) post &lt;- extract.samples(m7.1_OLS) Now let’s calculate the \\(R^{2}\\) value for our linear model set.seed(11) s &lt;- sim(m7.1) r &lt;- apply(s, 2, mean) - d$brain_std resid_var &lt;- var2(r) outcome_var &lt;- var2(d$brain_std) 1 - resid_var/outcome_var ## [1] 0.4731086 And let’s write a function that we can quickly apply to other models R2_is_bad &lt;- function(quap_fit){ s &lt;- sim(quap_fit, refresh = 0) r &lt;- apply(s, 2, mean) - d$brain_std 1 - var2(r)/var2(d$brain_std) } This is our second degree polynomial. It is just adding a secondary predictor that is based on the same data as m7.1. m7.2 &lt;- quap( alist( brain_std ~ dnorm(mu, exp(log_sigma)), mu &lt;- a + b[1]*mass_std + b[2]*mass_std^2, a ~ dnorm(0.5,1), b ~ dnorm(0, 10), log_sigma ~ dnorm(0, 1) ), data = d, start = list(b=rep(0,2)) ) Note: quap() needs to know how many \\(\\beta\\) values you have so we set this with a start list Now we will write additinal higher order polynomials, each increasing in degree m7.3 &lt;- quap( alist( brain_std ~ dnorm(mu, exp(log_sigma)), mu &lt;- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3, a ~ dnorm(0.5,1), b ~ dnorm(0, 10), log_sigma ~ dnorm(0, 1) ), data = d, start = list(b = rep(0,3)) ) m7.4 &lt;- quap( alist( brain_std ~ dnorm(mu, exp(log_sigma)), mu &lt;- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3 + b[4]*mass_std^4, a ~ dnorm(0.5,1), b ~ dnorm(0, 10), log_sigma ~ dnorm(0, 1) ), data = d, start = list(b = rep(0,4)) ) m7.5 &lt;- quap( alist( brain_std ~ dnorm(mu, exp(log_sigma)), mu &lt;- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3 + b[4]*mass_std^4 + b[5]*mass_std^5, a ~ dnorm(0.5,1), b ~ dnorm(0, 10), log_sigma ~ dnorm(0, 1) ), data = d, start = list(b = rep(0, 5)) ) \\(\\sigma\\) here needs to be replaced with a constant to get an output. m7.6 &lt;- quap( alist( brain_std ~ dnorm(mu,0.001), mu &lt;- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3 + b[4]*mass_std^4 + b[5]*mass_std^5 + b[6]*mass_std^6, a ~ dnorm(0.5,1), b ~ dnorm(0, 10) ), data = d, start = list(b = rep(0,6)) ) Now we can plot all our models. Below is a sample of plot code for a single model. You can also use brain_plot from the rethinking package. #7.1 post &lt;- extract.samples(m7.1) mass_seq &lt;- seq(from = min(d$mass_std), to = max(d$mass_std), length.out = 100) l &lt;- link(m7.1, data = list(mass_std=mass_seq)) mu &lt;- apply(l, 2, mean) ci &lt;- apply(l, 2, PI) plot(brain_std ~ mass_std, data = d, xlab = &#39;body mass (std)&#39;, ylab = &#39;brain volume (cc)&#39;) lines(mass_seq, mu) shade(ci, mass_seq) mtext(paste(&quot;m7.1: R^2 = &quot;, round(R2_is_bad(m7.1), 2))) To get an output for m7.6, sigma had to be a constant small number because the model passes through each point without variance. You can also notice that these high \\(R^2\\) models do very poorly in data gaps. Between 41 and 54 kg, the higher order models go wildly positive, and then steeply dip between 55 and 60 kg. 7.1.2 Too few parameters hurts, too Underfitting does a poor job at describing existing data and also lacks predictive power. To see if a model is underfit, you could remove data points one at a time and see how sensitive the model is to change. Overfit models will be very sensitive to change. Let’s plot the linear model and the 3rd degree polynomial for comparison. par(mfrow=c(1,2)) brain_loo_plot(m7.1) brain_loo_plot(m7.4) The (underfit) linear model doesn’t change much compared to the wild lines in model m7.3. 7.2 Entropy and accuracy 7.2.1 Firing the weatherperson Accuracy starts with picking the target. We should be worried about:\\ 1. Cost-benefit analysis. What is the cost of being wrong? How much do we win by if we are right? 2. Accuracy in context. How do we judge accuracy in a way that accounts for how much a model could imrpove prediction. Suppose in Vancouver, CBC puts out a 10 day forecast that has uncertain predictions of rain. And say CTV says theirs is better and only predicts sunshine for the 10 days. CBC forecast Day 1 2 3 4 5 6 7 8 9 10 Prediction 1 1 1 0.6 0.6 0.6 0.6 0.6 0.6 0.6 Observed 1 1 1 0 0 0 0 0 0 0 CTV forecast Day 1 2 3 4 5 6 7 8 9 10 Prediction 0 0 0 0 0 0 0 0 0 0 Observed 1 1 1 0 0 0 0 0 0 0 If we are define a metric of accuracy to be hit rate to be the average chance of a correct prediction, CBC would have a score of 5.8 hits (\\(3 \\times 1 + 7 \\times 0.4\\)) across 10 days (5.8 / 10 = 0.58) and CTV would have 7 hits (\\(3 \\times 0 + 7 \\times 1\\)) for a score of 0.7. CTV wins by this metric. 7.2.1.1 Cost and benefits As a member of the public you may choose your own happiness as the metric to score these forecasts. If you don’t like getting caught in the rain but also dislike carrying umbrellas, you may have a different scoring system. Say getting caught in the rain is -5 happiness points, and carrying an umbrella is -1 points. The probability of you carrying an umbrella is the same as the probability of rain. CBC &lt;- rep(c(-1, -0.6), times = c(3,7)) CTV &lt;- rep(c(-5, 0), times = c(3,7)) d &lt;- rbind(Day, Observed, CBC, CTV) knitr::kable(d, &quot;html&quot;) Day 1 2 3 4 5 6 7 8 9 10 Observed 1 1 1 0 0 0 0 0 0 0 CBC -1 -1 -1 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 -0.6 CTV -5 -5 -5 0 0 0 0 0 0 0 So now the CBC weather forecast is costing you -7.2 happiness points where CTV’s forecast is double that at -15. 7.2.1.2 Measuring accuracy The hit rate of the forecasts calculated above are not the only way to determine accuracy. If we instead wanted to compute the probability of correctly predicting the whole sequence of days, this would be similar to the joint likelihood. So if we look at the CBC forecast, \\(1^{3} \\times 0.4^{7} \\approx 0.002\\) doesn’t seem like a great probability. But when compared to the CTV forecast \\(0^{3} \\times 1^{7} = 0\\) there is no way for the forecast to be correct for the whole sequence because the CTV forecast doesn’t allow any prediction for rain. 7.2.2 Information and uncertainty Now that we have a way to measure accuracy, we want to use it to compare competing models. To do this we can measure the distance of a model’s prediction from the true value. But how do we measure distance from the true model? The answer comes from Information Theory. Information: The reduction in uncertainty when we learn an outcome Now we need a way to measure uncertainty. Our measure should include a few things:\\ It should be continuous in scale It should increase as the number of possible events increases It should be additive (i.e. combinations of different events should be the sum of their parts; weather and temp, rain/cold, rain/hot, sun/cold, sun/hot) Information Entropy: The uncertainty contained in a probability distribution is the average log-probability of an event. \\[H(p) = -E\\text{log}(p_{i}) = -\\sum_{i = 1}^{n} p_{i}\\text{log}(p_{i})\\] Here \\(n\\) is the number of events, \\(i\\) is an individual event with the probability of \\(p_{i}\\). To use in an example, let’s say that the true probability of rain is \\(p_{1} = 0.3\\) and sun is \\(p_{2} = 0.7\\). Then we can plug into the above equation: \\[H(p) = -(p_{1}\\text{log}(p_{1}) + p_{2}\\text{log}(p_{2})) \\approx 0.61\\] in R: p &lt;- c(0.3, 0.7) -sum(p*log(p)) ## [1] 0.6108643 So if we were in Abu Dhabi, the chance of rain would be about 0.01 and sunshine would be about 0.99 (1 - rain). So the uncertainty drops way down to $$0.06. The uncertainty is low because we know rain is rare. p &lt;- c(0.01, 0.99) -sum(p*log(p)) ## [1] 0.05600153 and when sun (0.7), rain (0.15), and snow (0.15) are all in the mix, uncertainty increases. p &lt;- c(0.7, 0.15, 0.15) -sum(p*log(p)) ## [1] 0.8188085 7.2.3 From entropy to accuracy Divergence: The additional uncertainty induced by using probabilities from one distribution to describe another distribution. Now we can measure how much uncertainty is introduced if our model uncertainty \\(q\\) is different from our true uncertainty \\(p\\) \\[D_{KL}(p, q) = \\sum_{i} p_{i}(\\text{log}(p_{i}) - \\text{log}(q_{i})) = \\sum_{i} p_{i} \\text{log} \\left(\\frac{p_{i}}{q_{i}}\\right)\\] Say our true distribution is \\(p = {0.3, 0.7}\\), and we have our model distributions ranging from \\(q = {0.01,0.99}\\) to \\(q = 0.99, 0.01\\). p1 &lt;- 0.3 p2 &lt;- 0.7 q1 &lt;- seq(from = 0.01, to = 0.99, by = 0.01) q2 &lt;- (1 - q1) d_kl &lt;- (p1 * log(p1/q1)) + (p2 * log(p2/q2)) d &lt;- data.frame(p1, p2, q1, q2, d_kl) plot(x = d$q1, y = d$d_kl, type = &#39;n&#39;) lines(d$q1, d$d_kl, col = &#39;blue&#39;, lwd = 2) abline(v = 0.3, lty = 2) text(x = 0.35, y = 2.2, label = &#39;q = p&#39;) In the plot we can see where \\(q_{1} = p_{1}\\) (0.3), the divergence is at its lowest value (0) and only increases as you move away from the true value. Now we can use divergence to compare models estimation of events (observations) to the probability of the events themselves. 7.2.4 Estimating divergence Because we don’t know \\(p\\) (especially if we are forecasting weather), we need to estimate our model distance from \\(p\\) by comparing models to each other. This way we can infer which model is closer to \\(p\\) without knowing it directly. To do this we will utilize the middle of the above Information Entropy equation, \\(E\\text{log}(p_{i})\\). What we can do is sum the likelihoods over all observations for each model by using: \\[S(q) = \\sum_{i} \\text{log}(q_{i})\\] for model \\(q\\) and : \\[S(r) = \\sum_{i} \\text{log}(r_{i})\\] for model \\(r\\). We use all observations so that we don’t throw away information that is hidden in the distributions of each log-likelihood. For Bayesian models, we use Log-Pointwise-Predictive-Densities (lppd) to score model accuracy. There is a function inside the rethinking package that will compute this for models, but we will do it by hand below as well. set.seed(11) lppd(m7.1, n = 1e4) ## [1] 0.6232812 0.6540091 0.5198331 0.6381867 0.4796153 0.4417278 -0.9146812 The equation for lppd consists of data \\(y\\) and a distribution \\(\\Theta\\) such that: \\[\\text{lppd}(y, \\Theta) = \\sum_{i}\\text{log}\\frac{1}{S}\\sum_{s}p(y_{i}|\\Theta_{s})\\] and \\(S\\) is the number of samples and \\(\\Theta_{s}\\) is the s-th set of sampled parameter values. set.seed(11) logprob &lt;- sim(m7.1, ll=TRUE, n = 1e4) #sampled LL as rows and observations as columns n &lt;- ncol(logprob) # seven observations ns &lt;- nrow(logprob) # 1e4 samples f &lt;- function(i){ log_sum_exp(logprob[,i]) - log(ns) # exp(i) then sums them, then takes log of sum and subtracts the log of sample number } (lppd &lt;- sapply(1:n, f)) ## [1] 0.6232812 0.6540091 0.5198331 0.6381867 0.4796153 0.4417278 -0.9146812 They match (yay!) 7.2.5 Scoring the right data One problem still exists. lppd also increases as models get more complex, just like \\(R^2\\). set.seed(11) sapply(list(m7.1,m7.2,m7.3,m7.4,m7.5,m7.6), function(m) sum(lppd(m))) ## [1] 2.478986 2.584124 3.634849 5.325457 14.052121 39.535568 Here is the way around this problem: 1. suppose there is a training sample of size \\(N\\) 2. compute the posterior distribution of a model for the training sample, then compute the score (\\(D_{train}\\)) 3. Suppose another sample of size \\(N\\). This is the test sample 4. compute the score on the test sample using the training posterior. (\\(D_{test}\\)) As an example we will train and test models of the form: \\[y_{i} \\sim \\text{Normal}(\\mu_{i}, 1) \\\\ \\mu_{i} = (0.15)x_{1,i} = (0.4)x_{2,i}\\] Here, \\(\\sigma\\) is static, \\(\\alpha = 0\\), and our \\(\\beta\\) coefficients are known. How it works: n_sim &lt;- 10 #number of simulations kseq &lt;- 1:5 #number of parameters in the model #simulation function my_sim &lt;- function(k) { r &lt;- replicate(n_sim, sim_train_test(N=n, k = k)); c(mean(r[1,]), mean(r[2,]), sd(r[1,]), sd(r[2,])) } # run for 20 cases n &lt;- 20 dev_20 &lt;- sapply(kseq, my_sim) # this will take a while # run for 100 cases #n &lt;- 100 #dev_100 &lt;- sapply(kseq, my_sim) # run if interested but with more data, adding parameters will not largely affect the deviance #plot plot(1:5, dev_20[1,], ylim=c(min(dev_20[1:2,])-5, max(dev_20[1:2,])+10), xlim = c(1, 5.1), xlab = &#39;number of parameters&#39;, ylab=&#39;deviance&#39;, pch = 16, col = rangi2) mtext(&#39;N = 20&#39;) points((1:5)+0.1, dev_20[2,]) for(i in kseq){ pts_in &lt;- dev_20[1,i] + c(-1,+1)*dev_20[3,i] pts_out &lt;- dev_20[2,i] + c(-1, +1)*dev_20[4,i] lines(c(i,i), pts_in, col=rangi2) lines(c(i,i)+0.1, pts_out) } Notice that deviance always decreases as you add parameters (similar to \\(R^2\\) increases with parameters) but the black out of sample simulations do best at 3 parameters, which is the correct amount of parameters for the data. 7.3 Golem taming: regularization Tuning the priors to improve model performance Standard model: \\[y_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\\\ \\mu_{i} = \\alpha + \\beta x_{i}\\\\ \\alpha \\sim \\text{Normal}(0,100)\\\\ \\beta \\sim \\text{Normal}(0,1)\\\\ \\sigma \\sim \\text{Exponential}(1)\\] So here we have the prior of \\(\\alpha\\) quite flat providing no real effect on the inference. The \\(\\beta\\) prior though is rather narrow and doesn’t expect any increase of 1 unit of \\(x\\) to create any change outside of 2 units in \\(y\\). 7.4 Predicting predictive accuracy 7.4.1 Cross-validation LOO and PSIS are common tools for cross-validatiting your models. LOO can be computationally expensive for large datasets so PSIS (Pareto-smoothed Importance Sampling Cross-validation) is used as a shortcut by using observational importance of each data point. 7.4.2 Information criteria AIC (Akaike Information Criteria) provides a simple estimate for out-of-sample deviance: \\[\\text{AIC} = D_{train} + 2p = -2\\text{lppd} +2p\\] with \\(p\\) representing free parameters in the posterior distribution. Unfortunately, AIC is not great with skeptical priors or multilevel models, so instead we will use the Widely Applicable Information Criterion (WAIC). It is a bit more complicated: \\[\\text{WAIC}(y, \\Theta) = -2\\left(\\text{lppd} - \\sum_{i}\\text{var}_{\\theta}\\text{log}p(y_{i}|\\theta)\\right)\\] Where the lppd equation is written above in section 7.2 and the term to the right of lppd is a penalty term proportional to the variance in the posterior predictions. Here is how you can do it in R in case it will help understand the process: #standard linear model data(cars) m &lt;- quap( alist( dist ~ dnorm(mu, sigma), mu &lt;- a + b*speed, a ~ dnorm(0, 100), b ~ dnorm(0, 10), sigma ~ dexp(1) ), data = cars ) set.seed(11) post &lt;- extract.samples(m, n = 1000) #pull samples from posterior # need to calculate likelihood for each observation n_samples &lt;- 1000 logprob &lt;- sapply(1:n_samples, function(s){ mu &lt;- post$a[s] + post$b[s]*cars$speed dnorm(cars$dist, mu, post$sigma[s], log = TRUE) }) The output of the above will be 1000 samples in columns for the 50 observations in rows #calculate the lppd n_cases &lt;- nrow(cars) #number of observations lppd &lt;- sapply(1:n_cases, function(i) log_sum_exp(logprob[i,]) - log(n_samples)) #penalty term in WAIC pWAIC &lt;- sapply(1:n_cases, function(i) var(logprob[i,])) -2*(sum(lppd) - sum(pWAIC)) ## [1] 423.0532 #compare to automatic function rethinking::WAIC(m) ## WAIC lppd penalty std_err ## 1 422.4586 -206.8399 4.38937 17.32067 #calculate the standard error of the WAIC for yourself waic_vec &lt;- -2*(lppd - pWAIC) sqrt(n_cases*var(waic_vec)) ## [1] 17.83286 7.4.3 Comparing CV, PSIS, and WAIC Regularizing priors have lower deviance WAIC tends to be a bit better than CV and PSIS 7.5 Model comparison You may see people select the best model and discard the others, but this is a mistake. 7.5.1 Model mis-selection Let’s start looking at some models. Recall the plant growth models from section 6.2. set.seed(11) compare(m6.6, m6.7, m6.8, func=WAIC) ## WAIC SE dWAIC dSE pWAIC weight ## m6.7 330.9678 16.15098 0.00000 NA 4.054545 1.000000e+00 ## m6.8 395.6257 11.23436 64.65787 14.23936 2.901013 9.114282e-15 ## m6.6 402.7142 11.05810 71.74634 16.16689 1.697835 2.633180e-16 set.seed(11) #compute dSE pointwise waic_m6.7 &lt;- WAIC(m6.7, pointwise = TRUE)$WAIC waic_m6.8 &lt;- WAIC(m6.8, pointwise = TRUE)$WAIC n &lt;- length(waic_m6.7) diff_m6.7_6.8 &lt;- waic_m6.7 - waic_m6.8 sqrt(n*var(diff_m6.7_6.8)) ## [1] 14.19359 99% interval between models (dWAIC ; m6.7 - m6.8) 65 + c(-1,1)*14.2*2.9 #dWAIC (+ or -) dSE * pWAIC ## [1] 23.82 106.18 plot(x = compare(m6.6, m6.7, m6.8)[,1], y=c(1:3) ) #Fix this m6.6 (intercept only) and m6.8 pointwise dSE set.seed(11) waic_m6.6 &lt;- WAIC(m6.6, pointwise=TRUE)$WAIC diff_m6.6_m6.8 &lt;- waic_m6.6 - waic_m6.8 sqrt(n*var(diff_m6.6_m6.8)) ## [1] 6.790192 only 7 points of deviance apart looking at dSE for all three set.seed(11) compare(m6.6, m6.7, m6.8)@dSE ## m6.6 m6.7 m6.8 ## m6.6 NA 16.16689 6.807944 ## m6.7 16.166887 NA 14.239356 ## m6.8 6.807944 14.23936 NA Notice the dSE for m6.6-m6.8 isn’t in the above table. above only has comparisons to best model (m6.7) 7.5.2 Outliers and other illustions Lets refit the waffle - divroce models data(WaffleDivorce) d &lt;- WaffleDivorce d$A &lt;- standardize(d$MedianAgeMarriage) d$D &lt;- standardize(d$Divorce) d$M &lt;- standardize(d$Marriage) m5.1 &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + bA*A, a ~ dnorm(0, 0.2), bA ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) m5.2 &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + bM*M, a ~ dnorm(0, 0.2), bM ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) m5.3 &lt;- quap( alist( D ~ dnorm(mu, sigma), mu &lt;- a + bM*M + bA*A, a ~ dnorm(0, 0.2), bM ~ dnorm(0,0.5), bA ~ dnorm(0, 0.5), sigma ~ dexp(1) ), data = d ) precis(m5.3) #remember marriage rate not useful with age ## mean sd 5.5% 94.5% ## a -3.141054e-05 0.09707997 -0.1551840 0.1551211 ## bM -6.536477e-02 0.15078018 -0.3063406 0.1756111 ## bA -6.135209e-01 0.15099044 -0.8548328 -0.3722090 ## sigma 7.851597e-01 0.07785368 0.6607345 0.9095849 Look at PSIS function comparison set.seed(11) compare(m5.1, m5.2, m5.3, func=PSIS) ## Some Pareto k values are very high (&gt;1). Set pointwise=TRUE to inspect individual points. ## Some Pareto k values are very high (&gt;1). Set pointwise=TRUE to inspect individual points. ## PSIS SE dPSIS dSE pPSIS weight ## m5.1 127.6505 15.03249 0.000000 NA 4.767972 0.880458424 ## m5.3 131.6777 16.57977 4.027213 1.903536 7.038831 0.117546749 ## m5.2 139.8303 10.84369 12.179770 11.146999 3.352505 0.001994827 When we get the high Pareto \\(k\\) warning, we may want to inspect the data set.seed(11) PSIS_m5.3 &lt;- PSIS(m5.3, pointwise = TRUE) ## Some Pareto k values are high (&gt;0.5). Set pointwise=TRUE to inspect individual points. set.seed(11) WAIC_m5.3 &lt;- WAIC(m5.3, pointwise = TRUE) labels &lt;- list(x = PSIS_m5.3[PSIS_m5.3$k &gt; 0.4,5], labels = d$Loc[c(9, 13, 20)], y = WAIC_m5.3[c(9,13,20),3]) plot(PSIS_m5.3$k, WAIC_m5.3$penalty, xlab = &quot;PSIS Pareto k&quot;, ylab = &quot;WAIC penalty&quot;, col=rangi2, lwd=2) text(x = labels$x-0.05, y = labels$y, labels = labels$labels) Robust regressing to deal with outliers Change the distribution from Gaussian (normal) to Student-T and add a \\(\\nu\\) term to adjust the frequency of rare events. m5.3t &lt;- quap( alist( D ~ dstudent(2, mu, sigma), mu &lt;- a + bM*M + bA*A, a ~ dnorm(0,0.2), bM ~ dnorm(0,0.5), bA ~ dnorm(0,0.5), sigma ~ dexp(1) ), data = d ) PSIS(m5.3t) #no error for high k ## PSIS lppd penalty std_err ## 1 134.1568 -67.07842 7.138106 11.87741 precis(m5.3) ## mean sd 5.5% 94.5% ## a -3.141054e-05 0.09707997 -0.1551840 0.1551211 ## bM -6.536477e-02 0.15078018 -0.3063406 0.1756111 ## bA -6.135209e-01 0.15099044 -0.8548328 -0.3722090 ## sigma 7.851597e-01 0.07785368 0.6607345 0.9095849 precis(m5.3t) ## mean sd 5.5% 94.5% ## a 0.015066723 0.09943806 -0.1438545 0.1739880 ## bM 0.003731365 0.21184966 -0.3348453 0.3423080 ## bA -0.701153916 0.13045818 -0.9096513 -0.4926565 ## sigma 0.552696770 0.08005500 0.4247534 0.6806401 The age of marriage effect has been increased (farther from zero) slightly and the marriage rate has gotten closer to zero with a wider sd. "],["conditional-manatees.html", "Chapter 8 Conditional manatees 8.1 Building an interaction 8.2 Symmetry of interactions 8.3 Continuous interactions", " Chapter 8 Conditional manatees 8.1 Building an interaction \\(R\\) = ruggedness, \\(G\\) = GDP, \\(C\\) = continent, \\(U\\) = unobserved variables. library(dagitty) library(rethinking) dag_8.1 &lt;- dagitty(&quot;dag{ R -&gt; G C -&gt; G U -&gt; G U -&gt; R }&quot;) coordinates(dag_8.1) &lt;- list(y = c(R = 0, G = 0, C = 0, U = 1), x = c(R = 0, G = 1, U = 1, C = 2)) drawdag(dag_8.1) \\(G = f(R,C)\\) 8.1.1 Making a rugged model library(rethinking) data(rugged) d &lt;- rugged #log transform GDP d$log_gdp &lt;- log(d$rgdppc_2000) #only include countries with GDP data dd &lt;- d[complete.cases(d$rgdppc_2000),] #rescale variables dd$log_gdp_std &lt;- (dd$log_gdp) / mean(dd$log_gdp) # values of 1 is average dd$rugged_std &lt;- (dd$rugged) / max(dd$rugged) # values range from 0 to max ruggedness (1) Basic model \\[\\text{log}(y_{i}) \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\\\ \\mu_{i} = \\alpha + \\beta(r_{i} - \\overline{r})\\\\ \\alpha \\sim \\text{Normal}|(1,1)\\\\ \\beta \\sim \\text{Normal}(0, 1)\\\\ \\sigma \\sim \\text{Exponential}(1)\\] In R: m8.1 &lt;- quap( alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- a + b*(rugged_std - 0.215), a ~ dnorm(1, 1), b ~ dnorm(0, 1), sigma ~ dexp(1) ), data = dd ) Sample priors: set.seed(11) prior &lt;- extract.prior(m8.1) #set plot plot(NULL, xlim=c(0,1), ylim=c(0.5, 1.5), xlab = &quot;ruggedness&quot;, ylab = &quot;log GDP&quot;) abline(h=min(dd$log_gdp_std), lty = 2) abline(h=max(dd$log_gdp_std), lty = 2) #draw lines from prior rugged_seq &lt;- seq(from = -0.1, to = 1.1, length.out=30) mu &lt;- link(m8.1, post = prior, data = data.frame(rugged_std=rugged_seq)) for(i in 1:50){ lines(rugged_seq, mu[i,], col=col.alpha(&#39;black&#39;,0.3)) } \\(\\alpha\\) is too wild. intercept should be somewhere around where the mean of ruggedness hits 1 on the log GDP scale so adjust to Normal(1, 0.1). \\(\\beta\\) is also out of control. we need something (positive or negative) that spans the difference between the dashed lines Slope should be \\(\\pm 0.6\\) which is the differece between the maximum and minimum values of GDP max(dd$log_gdp_std) - min(dd$log_gdp_std) ## [1] 0.5658058 #proportion of slopes greater than 0.6 sum(abs(prior$b) &gt; 0.6) / length(prior$b) ## [1] 0.54 Let’s fix the model m8.1 &lt;- quap( alist( log_gdp_std ~dnorm(mu,sigma), mu &lt;-a+b*(rugged_std-0.215), a ~dnorm(1,0.1), b ~dnorm(0,0.3), sigma ~dexp(1) ), data = dd ) precis(m8.1) ## mean sd 5.5% 94.5% ## a 0.999998578 0.010412457 0.98335746 1.01663970 ## b 0.001994904 0.054795958 -0.08557962 0.08956943 ## sigma 0.136503830 0.007397023 0.12468196 0.14832570 No association seen yet 8.1.2 Adding an indicator isn’t enough Update \\(\\mu\\) \\[\\mu_{i} = \\alpha_{CID[i]} + \\beta(r_{i} - \\overline{r})\\] #make an index variable for Africa (1) and other continents (2) dd$cid &lt;- ifelse(dd$cont_africa == 1, 1, 2) Now update the model m8.2 &lt;- quap( alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;-a[cid] + b * (rugged_std - 0.215), a[cid] ~ dnorm(1, 0.1), b ~ dnorm(0, 0.3), sigma ~dexp(1) ), data = dd ) compare(m8.1, m8.2) ## WAIC SE dWAIC dSE pWAIC weight ## m8.2 -252.1508 15.25508 0.00000 NA 4.291725 1.000000e+00 ## m8.1 -188.9664 13.28913 63.18439 15.11018 2.574284 1.904075e-14 precis(m8.2, depth = 2) ## mean sd 5.5% 94.5% ## a[1] 0.88040848 0.015938408 0.8549358 0.90588113 ## a[2] 1.04916209 0.010186473 1.0328821 1.06544204 ## b -0.04651791 0.045690786 -0.1195406 0.02650479 ## sigma 0.11239762 0.006092464 0.1026607 0.12213455 post &lt;- extract.samples(m8.2) diff_a1_a2 &lt;- post$a[,1] - post$a[,2] PI(diff_a1_a2) ## 5% 94% ## -0.1988472 -0.1384092 rugged.seq &lt;- seq(from = -0.1, to = 1.1, length.out = 30) mu.NotAfrica &lt;- link(m8.2, data = data.frame(cid=2, rugged_std=rugged.seq)) mu.Africa &lt;- link(m8.2, data = data.frame(cid = 1, rugged_std = rugged.seq)) mu.NotAfrica_mu &lt;- apply(mu.NotAfrica, 2, mean) mu.NotAfrica_ci &lt;- apply(mu.NotAfrica, 2, PI, prob = 0.97) mu.Africa_mu &lt;- apply(mu.Africa, 2, mean) mu.Africa_ci &lt;- apply(mu.Africa, 2, PI) plot(NULL, xlim=c(0,1), ylim=c(0.5, 1.5), xlab = &quot;ruggedness&quot;, ylab = &quot;log GDP&quot;) points(dd$rugged_std, dd$log_gdp_std, col = dd$cid, pch = 16) lines(rugged.seq, mu.Africa_mu, lwd = 2, col = 1) shade(mu.Africa_ci, rugged.seq) lines(rugged.seq, mu.NotAfrica_mu, lwd = 2, col = 2) shade(mu.NotAfrica_ci, rugged.seq, col = col.alpha(2, 0.3)) 8.1.3 Adding an interaction does work \\[\\mu_{i} = \\alpha_{CID[i]} + \\beta_{CID[i]}(r_{i} - \\overline{r})\\] m8.3 &lt;- quap( alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;-a[cid] + b[cid] * (rugged_std - 0.215), a[cid] ~ dnorm(1, 0.1), b[cid] ~ dnorm(0, 0.3), sigma ~dexp(1) ), data = dd ) precis(m8.3, depth = 2) ## mean sd 5.5% 94.5% ## a[1] 0.8865632 0.015675727 0.86151037 0.91161605 ## a[2] 1.0505709 0.009936627 1.03469025 1.06645155 ## b[1] 0.1325019 0.074204597 0.01390861 0.25109517 ## b[2] -0.1425818 0.054749512 -0.23008206 -0.05508147 ## sigma 0.1094944 0.005935331 0.10000855 0.11898016 compare(m8.1, m8.2, m8.3, func=PSIS) ## Some Pareto k values are high (&gt;0.5). Set pointwise=TRUE to inspect individual points. ## PSIS SE dPSIS dSE pPSIS weight ## m8.3 -258.8749 15.35435 0.000000 NA 5.314384 9.688047e-01 ## m8.2 -252.0033 15.30167 6.871589 6.936621 4.354277 3.119533e-02 ## m8.1 -188.4136 13.40307 70.461243 15.674795 2.856543 4.850334e-16 plot(PSIS(m8.3, pointwise = TRUE)$k) ## Some Pareto k values are high (&gt;0.5). Set pointwise=TRUE to inspect individual points. 8.1.4 Plotting the interaction par(mfrow=c(1,2)) # plot Africa - cid = 1 d.A1 &lt;-dd[dd$cid == 1,] plot(d.A1$rugged_std, d.A1$log_gdp_std, pch=16, col=rangi2, xlab=&quot;ruggedness (standardized)&quot;,ylab=&quot;log GDP (as proportion of mean)&quot;, xlim=c(0,1) ) mu &lt;-link(m8.3,data=data.frame(cid=1,rugged_std=rugged_seq)) mu_mean &lt;-apply(mu,2,mean) mu_ci &lt;-apply(mu,2,PI,prob=0.97) lines( rugged_seq,mu_mean,lwd=2) shade( mu_ci,rugged_seq,col=col.alpha(rangi2,0.3)) mtext(&quot;African nations&quot;) # plotnon-Africa-cid=2 d.A0 &lt;-dd[dd$cid==2,] plot( d.A0$rugged_std,d.A0$log_gdp_std,pch=1,col=&quot;black&quot;, xlab=&quot;ruggedness (standardized)&quot;,ylab=&quot;logGDP(asproportionofmean)&quot;, xlim=c(0,1) ) mu &lt;-link(m8.3,data=data.frame(cid=2,rugged_std=rugged_seq)) mu_mean &lt;-apply(mu,2,mean) mu_ci &lt;-apply(mu,2,PI,prob=0.97) lines( rugged_seq,mu_mean,lwd=2) shade( mu_ci,rugged_seq) mtext(&quot;Non-African nations&quot;) 8.2 Symmetry of interactions You can break an interaction into 2 identical phrasings 1. GDP ~ ruggedness depending on Africa 2. Africa ~ GDP depending on rugedness \\[\\mu_{i} = (2 - CID_{i})(\\alpha_{1} + \\beta_{1}(r_{i} - \\overline{r})) + (CID_{i} - 1)(\\alpha_{2} + \\beta_{2}(r_{i} - \\overline{r}))\\] rugged_seq &lt;- seq(from = -0.2, to = 1.2, length.out = 30) muA &lt;- link(m8.3, data=data.frame(cid=1, rugged_std=rugged_seq)) muN &lt;- link(m8.3, data=data.frame(cid=2, rugged_std=rugged_seq)) delta &lt;- muA - muN mu.delta &lt;- apply(delta, 2, mean) PI.delta &lt;- apply(delta, 2, PI) plot(x=rugged_seq, type = &#39;n&#39;, xlim = c(0,1), ylim = c(-0.3, 0.2), xlab = &#39;ruggedness (std)&#39;, ylab = &#39;expected difference log GDP&#39;) shade(PI.delta, rugged_seq, col=&#39;grey&#39;) abline(h = 0, lty = 2) text(x = 0.2, y = 0, label = &quot;Africa higher GDP\\nAfrica lower GDP&quot;) lines(rugged_seq, mu.delta) At high ruggedness, being in Africa gives higher than expected GDP. 8.3 Continuous interactions 8.3.1 A winter flower data(tulips) d &lt;- tulips str(d) ## &#39;data.frame&#39;: 27 obs. of 4 variables: ## $ bed : Factor w/ 3 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ water : int 1 1 1 2 2 2 3 3 3 1 ... ## $ shade : int 1 2 3 1 2 3 1 2 3 1 ... ## $ blooms: num 0 0 111 183.5 59.2 ... 8.3.2 the models Water and Shade work together to create Blooms; \\(W \\rightarrow B \\leftarrow S ; B = f(W,S)\\) water \\[\\beta_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\\\ \\mu_{i} = \\alpha + \\beta_{W}(W_{i} - \\overline{W})\\\\ \\alpha \\sim \\text{Normal}(0.5,1)\\\\ \\beta_{W} \\sim \\text{Normal}(0,1)\\\\ \\sigma \\sim \\text{Exponential}(1)\\] shade \\[\\beta_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\\\ \\mu_{i} = \\alpha + \\beta_{S}(S_{i} - \\overline{S})\\\\ \\alpha \\sim \\text{Normal}(0.5,1)\\\\ \\beta_{S} \\sim \\text{Normal}(0,1)\\\\ \\sigma \\sim \\text{Exponential}(1)\\] water + shade \\[\\beta_{i} \\sim \\text{Normal}(\\mu_{i}, \\sigma)\\\\ \\mu_{i} = \\alpha + \\beta_{W}(W_{i} - \\overline{W}) + \\beta_{S}(S_{i} - \\overline{S})\\\\ \\alpha \\sim \\text{Normal}(0.5,1)\\\\ \\beta_{W} \\sim \\text{Normal}(0,1)\\\\ \\beta_{S} \\sim \\text{Normal}(0,1)\\\\ \\sigma \\sim \\text{Exponential}(1)\\] water * shade $$$$ #center predictors and scale outcome d$blooms_std &lt;- d$blooms / max(d$blooms) d$water_cent &lt;- d$water - mean(d$water) d$shade_cent &lt;- d$shade - mean(d$shade) The \\(\\alpha\\) prior is likely too broad. We need it to be between 0 and 1. How much is outside that? a &lt;- rnorm(1e4, 0.5, 1); sum(a &lt; 0 | a &gt; 1) / length(a) ## [1] 0.623 Let’s tighten it a &lt;- rnorm(1e4, 0.5, 0.25); sum(a &lt; 0 | a &gt; 1) / length(a) ## [1] 0.047 range of water and shade are each 2 units. range of blooms is one unit. max slopes = 2/1 (0.5) so we can set the prior to 0 with 0.25 sd to get values ranging from -0.5 to 0.5. #water m8.4a &lt;- quap( alist( blooms_std ~ dnorm(mu, sigma), mu &lt;- a + bw*water_cent, a ~ dnorm(0.5,0.25), bw ~ dnorm(0, 0.25), sigma ~ dexp(1) ), data = d ) #shade m8.4b &lt;- quap( alist( blooms_std ~ dnorm(mu, sigma), mu &lt;- a + bs*shade_cent, a ~ dnorm(0.5,0.25), bs ~ dnorm(0, 0.25), sigma ~ dexp(1) ), data = d ) # water + shade m8.4c &lt;- quap( alist( blooms_std ~ dnorm(mu, sigma), mu &lt;- a + bw*water_cent + bs*shade_cent, a ~ dnorm(0.5,0.25), bw ~ dnorm(0, 0.25), bs ~ dnorm(0, 0.25), sigma ~ dexp(1) ), data = d ) #water * shade m8.4d &lt;- quap( alist( blooms_std ~ dnorm(mu, sigma), mu &lt;- a + bw*water_cent + bs*shade_cent + bws*water_cent*shade_cent, a ~ dnorm(0.5,0.25), bw ~ dnorm(0, 0.25), bs ~ dnorm(0, 0.25), bws ~ dnorm(0, 0.25), sigma ~ dexp(1) ), data = d ) prior simulations set.seed(11) prior_a &lt;- extract.prior(m8.4a) #water prior_b &lt;- extract.prior(m8.4b) #shade prior_c &lt;- extract.prior(m8.4c) #water + shade prior_d &lt;- extract.prior(m8.4d) #water * shade #set plot plot(NULL, xlim=c(0,2), ylim=c(0, 1.25), xlab = &quot;Water / shade&quot;, ylab = &quot;blooms&quot;) abline(h=min(d$blooms_std), lty = 2) abline(h=max(d$blooms_std), lty = 2) #draw lines from prior water_seq &lt;- seq(from = -1.1, to = 1.1, length.out=30) shade_seq &lt;- seq(from = -1.1, to = 1.1, length.out=30) mu_a &lt;- link(m8.4a, post = prior_a, data = data.frame(water_cent=water_seq)) mu_b &lt;- link(m8.4b, post = prior_b, data = data.frame(shade_cent=shade_seq)) mu_c &lt;- link(m8.4c, post = prior_c, data = data.frame(water_cent=water_seq, shade_cent=shade_seq)) mu_d &lt;- link(m8.4d, post = prior_d, data = data.frame(water_cent=water_seq, shade_cent=shade_seq)) #set plot plot(NULL, xlim=c(-1,1), ylim=c(0, 1.25), xlab = &quot;Water / shade (centered)&quot;, ylab = &quot;blooms (std)&quot;) abline(h=min(d$blooms_std), lty = 2) abline(h=max(d$blooms_std), lty = 2) for(i in 1:50){ lines(water_seq, mu_a[i,], col=col.alpha(&#39;blue&#39;,0.3)) } for(i in 1:50){ lines(water_seq, mu_b[i,], col=col.alpha(&#39;black&#39;,0.3)) } for(i in 1:50){ lines(water_seq, mu_c[i,], col=col.alpha(&#39;green&#39;,0.3)) } for(i in 1:50){ lines(water_seq, mu_d[i,], col=col.alpha(&#39;red&#39;,0.3)) } text(x = -0.75, y = 1.2, label = &quot;Water&quot;, col = &quot;blue&quot;) text(x = -0.45, y = 1.2, label = &quot;Shade&quot;, col = &quot;black&quot;) text(x = 0, y = 1.2, label = &quot;Water + Shade&quot;, col = &quot;green&quot;) text(x = 0.5, y = 1.2, label = &quot;Water * Shade&quot;, col = &#39;red&#39;) precis(m8.4a) ## mean sd 5.5% 94.5% ## a 0.3594883 0.03502089 0.3035181 0.4154584 ## bw 0.2034854 0.04270338 0.1352371 0.2717336 ## sigma 0.1837433 0.02489828 0.1439510 0.2235355 precis(m8.4b) ## mean sd 5.5% 94.5% ## a 0.3611171 0.04403635 0.2907385 0.43149566 ## bs -0.1097643 0.05351857 -0.1952973 -0.02423127 ## sigma 0.2323701 0.03143809 0.1821260 0.28261427 precis(m8.4c) ## mean sd 5.5% 94.5% ## a 0.3587452 0.03022116 0.3104459 0.40704441 ## bw 0.2050352 0.03689240 0.1460741 0.26399641 ## bs -0.1125324 0.03687853 -0.1714714 -0.05359337 ## sigma 0.1581668 0.02144796 0.1238888 0.19244481 precis(m8.4d) ## mean sd 5.5% 94.5% ## a 0.3579980 0.02391747 0.31977332 0.39622278 ## bw 0.2067288 0.02923282 0.16000910 0.25344849 ## bs -0.1134595 0.02922579 -0.16016795 -0.06675103 ## bws -0.1431791 0.03567746 -0.20019860 -0.08615967 ## sigma 0.1248375 0.01693790 0.09776742 0.15190750 8.3.3 Plotting posterior predictions par(mfrow = c(1,3)) for (s in -1:1){ idx &lt;- d[d$shade_cent == s,] plot(x = idx$water_cent, y = idx$blooms_std, xlim = c(-1,1), ylim = c(0,1), xlab = &quot;water&quot;, ylab = &quot;blooms&quot;, pch = 16, col = rangi2) mu &lt;- link(m8.4c, data = data.frame(shade_cent=s, water_cent = -1:1)) for(i in 1:20){ lines(-1:1, mu[i,], col = col.alpha(&#39;black&#39;,0.3)) } mtext(concat(&quot;m8.4c post: shade = &quot;, s)) } par(mfrow = c(1,3)) for (s in -1:1){ idx &lt;- d[d$shade_cent == s,] plot(x = idx$water_cent, y = idx$blooms_std, xlim = c(-1,1), ylim = c(0,1), xlab = &quot;water&quot;, ylab = &quot;blooms&quot;, pch = 16, col = rangi2) mu &lt;- link(m8.4d, data = data.frame(shade_cent=s, water_cent = -1:1)) for(i in 1:20){ lines(-1:1, mu[i,], col = col.alpha(&#39;black&#39;,0.3)) } mtext(concat(&quot;m8.4d post: shade = &quot;, s)) } par(mfrow = c(1,3)) for (s in -1:1){ idx &lt;- d[d$water_cent == s,] plot(x = idx$shade_cent, y = idx$blooms_std, xlim = c(-1,1), ylim = c(0,1), xlab = &quot;shade&quot;, ylab = &quot;blooms&quot;, pch = 16, col = rangi2) mu &lt;- link(m8.4c, data = data.frame(water_cent=s, shade_cent = -1:1)) for(i in 1:20){ lines(-1:1, mu[i,], col = col.alpha(&#39;black&#39;,0.3)) } mtext(concat(&quot;m8.4c post: water = &quot;, s)) } par(mfrow = c(1,3)) for (s in -1:1){ idx &lt;- d[d$water_cent == s,] plot(x = idx$shade_cent, y = idx$blooms_std, xlim = c(-1,1), ylim = c(0,1), xlab = &quot;shade&quot;, ylab = &quot;blooms&quot;, pch = 16, col = rangi2) mu &lt;- link(m8.4d, data = data.frame(water_cent=s, shade_cent = -1:1)) for(i in 1:20){ lines(-1:1, mu[i,], col = col.alpha(&#39;black&#39;,0.3)) } mtext(concat(&quot;m8.4c post: water = &quot;, s)) } 8.3.4 Plotting prior predictions par(mfrow = c(1,3)) for (s in -1:1){ idx &lt;- d[d$shade_cent == s,] plot(x = idx$water_cent, y = idx$blooms_std, type = &#39;n&#39;, xlim = c(-1,1), ylim = c(-0.5,1.5), xlab = &quot;water&quot;, ylab = &quot;blooms&quot;) mu &lt;- link(m8.4c, post = prior_c, data = data.frame(shade_cent=s, water_cent = -1:1)) for(i in 1:20){ lines(-1:1, mu[i,], col = col.alpha(&#39;black&#39;,0.3)) } lines(-1:1, mu[11,], lwd = 2, col = rangi2) abline(h = 0, lty = 2) abline(h = 1, lty = 2) mtext(concat(&quot;m8.4c post: shade = &quot;, s)) } par(mfrow = c(1,3)) for (s in -1:1){ idx &lt;- d[d$shade_cent == s,] plot(x = idx$water_cent, y = idx$blooms_std, type = &#39;n&#39;, xlim = c(-1,1), ylim = c(-0.5,1.5), xlab = &quot;water&quot;, ylab = &quot;blooms&quot;) mu &lt;- link(m8.4d, post = prior_d, data = data.frame(shade_cent=s, water_cent = -1:1)) for(i in 1:20){ lines(-1:1, mu[i,], col = col.alpha(&#39;black&#39;,0.3)) } lines(-1:1, mu[11,], lwd = 2, col = rangi2) abline(h = 0, lty = 2) abline(h = 1, lty = 2) mtext(concat(&quot;m8.4c post: shade = &quot;, s)) } par(mfrow = c(1,3)) for (s in -1:1){ idx &lt;- d[d$water_cent == s,] plot(x = idx$shade_cent, y = idx$blooms_std, type = &#39;n&#39;, xlim = c(-1,1), ylim = c(-0.5,1.5), xlab = &quot;shade&quot;, ylab = &quot;blooms&quot;) mu &lt;- link(m8.4c, post = prior_c, data = data.frame(water_cent=s, shade_cent = -1:1)) for(i in 1:20){ lines(-1:1, mu[i,], col = col.alpha(&#39;black&#39;,0.3)) } lines(-1:1, mu[11,], lwd = 2, col = rangi2) abline(h = 0, lty = 2) abline(h = 1, lty = 2) mtext(concat(&quot;m8.4c post: water = &quot;, s)) } par(mfrow = c(1,3)) for (s in -1:1){ idx &lt;- d[d$water_cent == s,] plot(x = idx$shade_cent, y = idx$blooms_std, type = &#39;n&#39;, xlim = c(-1,1), ylim = c(-0.5,1.5), xlab = &quot;shade&quot;, ylab = &quot;blooms&quot;) mu &lt;- link(m8.4d, post = prior_d, data = data.frame(water_cent=s, shade_cent = -1:1)) for(i in 1:20){ lines(-1:1, mu[i,], col = col.alpha(&#39;black&#39;,0.3)) } lines(-1:1, mu[11,], lwd = 2, col = rangi2) abline(h = 0, lty = 2) abline(h = 1, lty = 2) mtext(concat(&quot;m8.4c post: water = &quot;, s)) } "],["markov-chain-monte-carlo.html", "Chapter 9 Markov Chain Monte Carlo 9.1 Good King Markov and his island kingdom 9.2 Metropolis algorithims 9.3 Hamiltonian Monte Carlo 9.4 Easy HMC: ulam (brm) 9.5 Care and feeding of your Markov chain", " Chapter 9 Markov Chain Monte Carlo 9.1 Good King Markov and his island kingdom Ring of 10 islands increasing in size and population as you go around the ring. How do we visit each in proportion to population without too much time at see? Metropolis Algorithm flip a coin each week to decide stay or leave Heads - consider clockwise ; Tails - Counterclockwise ; proposal Count seashells proportionate to proposal population. Count stones of current island. If seashells &gt; stones then go to proposal. If stones &gt; seashells then discard stones equal to the number of seashells and mix remaining seashells and stones together. If he draws a seashell then move to proposal. If he draws a stone, stays for a week. This draw probability of moving is equal to seashells / stones. In code: num_weeks &lt;- 1e5 positions &lt;- rep(0, num_weeks) current &lt;- 10 for(i in 1:num_weeks){ #record current island positions[i] &lt;- current #flip a coin proposal &lt;- current + sample(c(-1,1), size = 1) #link the loop between 10 and 1 if(proposal &lt; 1) proposal &lt;- 10 if(proposal &gt; 10) proposal &lt;- 1 # move ? prob_move &lt;- proposal/current current &lt;- ifelse(runif(1) &lt; prob_move, proposal, current) } Visualized: par(mfrow = c(1,2)) plot(1:100, positions[1:100], col = rangi2, xlab = &quot;week&quot;, ylab = &quot;island&quot;) plot(table(positions), col = rangi2, xlab = &quot;island&quot;, ylab = &quot;# of weeks&quot;) 9.2 Metropolis algorithims We can equate the King Markov example to estimating a posterior probability distribution. Islands are like parameters that can take on any value Population sizes are like posterior probabilities at each parameter value Weeks are like the samples from the joint posterior between the parameters 9.2.1 Gibbs sampling Gibbs sampling uses adaptive proposals that adjusts proposed parameter values in an intelligent manner. It does this by using conjugate pairs of prior distributions and likelihoods. 9.2.2 High-dimensional problems Gibbs can get stuck in small regions of the posterior in models with thousands of parameters or highly correlated parameters. This arises from the Concentration of measure. Basically the mode is not the most likely to be sampled. d &lt;- c(1, 10, 100, 1000) concentration_sim &lt;- function(D, T = 1e3){ Y &lt;- rmvnorm(T, rep(0, D), diag(D)) rad_dist &lt;- function(Y) sqrt(sum(Y^2)) Rd &lt;- sapply(1:T, function(i) rad_dist(Y[i,])) } Rd_a &lt;- lapply(d, concentration_sim) Rd_b &lt;- unlist(Rd_a) dens(Rd_b) text(5, 0.10, &quot;1 &amp; 10&quot;) text(11, 0.10, &quot;100&quot;) text(33, 0.10, &quot;1000&quot;) 9.3 Hamiltonian Monte Carlo 9.3.1 Another parable King Monty is Markov’s cousin on the mainland. Monty’s kingdom lays in a narrow valley that runs North-south. The population is inversely related to elevation with most in the bottom of the valley and fewer on the mountainsides. King Monty picks a random direction and takes off with a random momentum. It will travel up and down as far as the momentum will carry it in a determined amount of time. When the time is reached, they stop. They then repeat. This removes any autocorrelation between neighbours. This is the basis of HMC 9.3.2 Particles in space If King Monty’s vehicle was like a marble that contained the current parameter values, and the parameter space was a bowl, HMC would be like randomly flicking the marble in the bowl and taking a new position sample at a random amount of time has passed. The marble must follow laws of physics and so too does HMC. Suppose there are 100 \\(x\\) values and 100 \\(y\\) values all sampled from Normal(0, 1). \\[x_{i} \\sim \\text{Normal}(\\mu_{x}, 1)\\\\ y_{i} \\sim \\text{Normal}(\\mu_{y}, 1)\\\\ \\mu_{x} \\sim \\text{Normal}(0, 0.5)\\\\ \\mu_{y} \\sim \\text{Normal}(0, 0.5)\\] Computing the log-probability of parameters and data: \\[\\sum_{i}\\text{log}p(y_{i}|\\mu_{y}, 1) + \\sum_{i}\\text{log}p(x_{i}|\\mu_{x}, 1) + \\text{log}p(\\mu_{y}|0, 0.5) + \\text{log}p(\\mu_{x}|0, 0.5)\\] Compute the gradient or slope in all directions from current position (same for \\(y\\)): \\[\\frac{\\partial M}{\\partial\\mu_{x}}=\\frac{\\partial\\text{log}N(x|\\mu_{x},1)}{\\partial\\mu_{x}}+\\frac{\\partial\\text{log}N(\\mu_{x}|0, 0.5)}{\\partial\\mu_{x}}=\\sum_{i}\\frac{x_{i}-\\mu_{x}}{1^2}+\\frac{0-\\mu_{x}}{0.5^2}\\] Set leapfrog steps and step size. This is largely done for you. If the leapfrog steps and step size were to be the right combination you could run into U turns where the samples look a lot like the starting position. This is avoided in newer samplers with a no-U-Turn Sampler or NUTS. Write out HMC in raw code here** 9.3.3 Limitations As always, there are some limitations. HMC requires continuous parameters. It can’t glide through a discrete parameter. In practice, this means that certain techniques, like the imputation of discrete missing data, have to be done differently with HMC. HMC can certainly sample from such models, often much more efficiently than a Gibbs sampler could. But you have to change how you code them. (p. 278) 9.4 Easy HMC: ulam (brm) Revisiting the African ruggedness example from the last chapter. data(rugged) d &lt;- rugged d$log_gdp &lt;- log(d$rgdppc_2000) dd &lt;- d[complete.cases(d$rgdppc_2000),] dd$log_gdp_std &lt;- dd$log_gdp / mean(dd$log_gdp) dd$rugged_std &lt;- dd$rugged / max(dd$rugged) dd$cid &lt;- ifelse(dd$cont_africa == 1, 1, 2) The old way: m8.3 &lt;- quap( alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- a[cid] + b[cid]*(rugged_std - 0.215), a[cid] ~ dnorm(1, 0.1), b[cid] ~ dnorm(0, 0.3), sigma ~ dexp(1) ), data = dd ) precis(m8.3, depth = 2) ## mean sd 5.5% 94.5% ## a[1] 0.8865703 0.015675587 0.86151766 0.91162289 ## a[2] 1.0505696 0.009936529 1.03468907 1.06645006 ## b[1] 0.1324943 0.074203905 0.01390215 0.25108650 ## b[2] -0.1425928 0.054748970 -0.23009225 -0.05509339 ## sigma 0.1094933 0.005935183 0.10000769 0.11897882 9.4.1 Preperation Things to do before using HMC: 1. Preprocess any variable transformations. You don’t want to waste computing power by having these transformations in your model. 2. create a trimmed data frame of only parameters of interest. Also, remove NA values. Create a slim list: dat_slim &lt;- list( log_gdp_std = dd$log_gdp_std, rugged_std = dd$rugged_std, mean_rugged_std = mean(dd$rugged_std), cid = as.integer(dd$cid) ) str(dat_slim) ## List of 4 ## $ log_gdp_std : num [1:170] 0.88 0.965 1.166 1.104 0.915 ... ## $ rugged_std : num [1:170] 0.138 0.553 0.124 0.125 0.433 ... ## $ mean_rugged_std: num 0.215 ## $ cid : int [1:170] 1 2 2 2 2 2 2 2 2 1 ... 9.4.2 Sampling from the posterior m9.1 &lt;- ulam( alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- a[cid] + b[cid]*(rugged_std - 0.215), a[cid] ~ dnorm(1, 0.1), b[cid] ~ dnorm(0, 0.3), sigma ~ dexp(1) ), data = dat_slim, chains = 1 ) ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d71202462e.stan&#39;, line 5, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Running MCMC with 1 chain, with 1 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue: ## Chain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d71202462e.stan&#39;, line 20, column 4 to column 39) ## Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine, ## Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified. ## Chain 1 ## Chain 1 finished in 0.1 seconds. precis(m9.1, depth = 2) ## mean sd 5.5% 94.5% n_eff Rhat4 ## a[1] 0.8869532 0.015830713 0.86261676 0.91190040 454.1181 0.9992850 ## a[2] 1.0505183 0.009935779 1.03498795 1.06636110 683.5363 0.9979989 ## b[1] 0.1337524 0.075649421 0.01257881 0.25452763 512.2719 0.9997851 ## b[2] -0.1410113 0.053844208 -0.22725398 -0.05794813 708.8812 1.0028886 ## sigma 0.1117050 0.005804116 0.10340290 0.12112133 591.0803 0.9980074 n_eff is a crude estimate of independent samples. Rhat (\\(\\hat{R}\\)) is a measure of model convergence. 9.4.3 Sampling again, in parallel To save time and maximize computing power you can run chains at the same time. m9.1 &lt;- ulam( alist( log_gdp_std ~ dnorm(mu, sigma), mu &lt;- a[cid] + b[cid]*(rugged_std - 0.215), a[cid] ~ dnorm(1, 0.1), b[cid] ~ dnorm(0, 0.3), sigma ~ dexp(1) ), data = dat_slim, chains = 4, cores = 4 ) ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d714e8ef537.stan&#39;, line 5, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Running MCMC with 4 parallel chains, with 1 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue: ## Chain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d714e8ef537.stan&#39;, line 20, column 4 to column 39) ## Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine, ## Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified. ## Chain 1 ## Chain 2 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue: ## Chain 2 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d714e8ef537.stan&#39;, line 20, column 4 to column 39) ## Chain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine, ## Chain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified. ## Chain 2 ## Chain 3 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue: ## Chain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d714e8ef537.stan&#39;, line 20, column 4 to column 39) ## Chain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine, ## Chain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified. ## Chain 3 ## Chain 4 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 4 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 4 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 4 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 4 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 4 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 4 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 4 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue: ## Chain 4 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d714e8ef537.stan&#39;, line 20, column 4 to column 39) ## Chain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine, ## Chain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified. ## Chain 4 ## Chain 1 finished in 0.3 seconds. ## Chain 2 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 4 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 4 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 4 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 4 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2 finished in 0.3 seconds. ## Chain 3 finished in 0.3 seconds. ## Chain 4 finished in 0.3 seconds. ## ## All 4 chains finished successfully. ## Mean chain execution time: 0.3 seconds. ## Total execution time: 0.6 seconds. show(m9.1) ## Hamiltonian Monte Carlo approximation ## 2000 samples from 4 chains ## ## Sampling durations (seconds): ## warmup sample total ## chain:1 0.10 0.18 0.28 ## chain:2 0.22 0.11 0.34 ## chain:3 0.24 0.09 0.33 ## chain:4 0.18 0.08 0.26 ## ## Formula: ## log_gdp_std ~ dnorm(mu, sigma) ## mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215) ## a[cid] ~ dnorm(1, 0.1) ## b[cid] ~ dnorm(0, 0.3) ## sigma ~ dexp(1) precis(m9.1, depth = 2) ## mean sd 5.5% 94.5% n_eff Rhat4 ## a[1] 0.8866043 0.015086816 0.86307619 0.91070816 2496.039 0.9983660 ## a[2] 1.0504062 0.010079394 1.03455670 1.06664110 3540.261 0.9986465 ## b[1] 0.1321143 0.071303286 0.02013114 0.24660710 2596.586 1.0010825 ## b[2] -0.1402688 0.055262930 -0.22792168 -0.05498101 2530.129 1.0000719 ## sigma 0.1114732 0.006281128 0.10178552 0.12225914 2453.089 0.9994101 How did we get more than 1000 independent samples? Stan is so good at sampling that it out performs random. It creates anit-correlated samples that increases the crude estimate of independent samples. 9.4.4 Visualization Everything is still pretty Gaussian pairs(m9.1) 9.4.5 Checking the chain traceplot(m9.1) Things to look for in a traceplot: 1. stationarity - staying in the same region 2. good mixing - good strong zigzags around the parameter space 3. convergence - independent chains end up in the same region There are also trace rank plots that show histograms of ranked samples for each chain. You want them to be largely similar and overlapping. trankplot(m9.1) 9.5 Care and feeding of your Markov chain 9.5.1 How many samples do you need? 9.5.2 How many chains do you need? Start with 1 for debugging purposes. After that 1 chain works as expected, run multiples to make sure they all behave the same. Then you can draw your inference from the multiple chains. Example: warmup = 1000, iter = 10000. You could do 3 chains of warmup = 1000, iter = 4000 but then you end up throwing away 3000 samples. It is really up to you and your hardware. 9.5.3 Taming a wild chain y &lt;- c(-1,1) set.seed(11) m9.2 &lt;- ulam( alist( y ~ dnorm(mu, sigma), mu &lt;- alpha, alpha ~ dnorm(0, 1000), sigma ~ dexp(0.0001) ), data = list(y=y), chains = 3 ) ## Running MCMC with 3 sequential chains, with 1 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1 finished in 0.1 seconds. ## Chain 2 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2 finished in 0.1 seconds. ## Chain 3 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3 finished in 0.1 seconds. ## ## All 3 chains finished successfully. ## Mean chain execution time: 0.1 seconds. ## Total execution time: 0.5 seconds. ## Warning: 108 of 1500 (7.0%) transitions ended with a divergence. ## See https://mc-stan.org/misc/warnings for details. precis(m9.2) ## mean sd 5.5% 94.5% n_eff Rhat4 ## alpha -27.98188 280.3293 -485.10613 291.7374 121.2064 1.006568 ## sigma 390.82029 993.0716 1.83045 1870.8598 169.2490 1.025695 pairs(m9.2@stanfit) traceplot(m9.2) trankplot(m9.2) set.seed(11) m9.3 &lt;- ulam( alist( y ~ dnorm(mu, sigma), mu &lt;- alpha, alpha ~ dnorm(0, 10), sigma ~ dexp(1) ), data = list(y=y), chains = 3 ) ## Running MCMC with 3 sequential chains, with 1 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1 finished in 0.0 seconds. ## Chain 2 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2 finished in 0.0 seconds. ## Chain 3 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3 finished in 0.0 seconds. ## ## All 3 chains finished successfully. ## Mean chain execution time: 0.0 seconds. ## Total execution time: 0.5 seconds. precis(m9.3) ## mean sd 5.5% 94.5% n_eff Rhat4 ## alpha 0.135228 1.1001034 -1.566137 1.895650 606.2608 1.003484 ## sigma 1.607333 0.8904931 0.669326 3.117943 592.4768 1.001492 pairs(m9.3@stanfit) par(mfrow=c(2,2)) traceplot(m9.3) trankplot(m9.3) 9.5.4 Non-identifiable parameters set.seed(11) y &lt;- rnorm(100, mean = 0, sd = 1) \\[ y_{i} \\sim \\text{Normal}(\\mu, \\sigma)\\\\ \\mu = \\alpha_{1} + \\alpha_{2}\\\\ \\alpha_{1} \\sim \\text{Normal}(0, 1000)\\\\ \\alpha_{2} \\sim \\text{Normal}(0, 1000)\\\\ \\sigma \\sim \\text{Exponential}(1) \\] Here \\(\\alpha_{1}\\) and \\(\\alpha_{2}\\) are unknown but we know that they will sum to nearly 0 (the mean of the simulated \\(y\\)) set.seed(11) m9.4 &lt;- ulam( alist( y ~ dnorm(mu, sigma), mu &lt;- a1 + a2, a1 ~ dnorm(0, 1000), a2 ~ dnorm(0, 1000), sigma ~ dexp(1) ), data = list(y = y), chains = 3 ) #this will take a few minutes ## Running MCMC with 3 sequential chains, with 1 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1 finished in 1.6 seconds. ## Chain 2 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2 finished in 1.5 seconds. ## Chain 3 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3 finished in 1.5 seconds. ## ## All 3 chains finished successfully. ## Mean chain execution time: 1.5 seconds. ## Total execution time: 4.7 seconds. ## Warning: 1166 of 1500 (78.0%) transitions hit the maximum treedepth limit of 10. ## See https://mc-stan.org/misc/warnings for details. precis(m9.4) ## mean sd 5.5% 94.5% n_eff Rhat4 ## a1 174.6107422 608.12646691 -681.9274950 1036.605700 1.574017 6.011029 ## a2 -174.7320368 608.12642123 -1036.7634500 681.796330 1.574013 6.011069 ## sigma 0.8961799 0.06550096 0.7913783 1.001362 4.268243 1.330069 traceplot(m9.4) m9.5 &lt;- ulam( alist( y ~ dnorm(mu, sigma), mu &lt;- a1 + a2, a1 ~ dnorm(0, 10), a2 ~ dnorm(0, 10), sigma ~ dexp(1) ), data = list(y = y), chains = 3 ) ## Running MCMC with 3 sequential chains, with 1 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue: ## Chain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d715e6f455d.stan&#39;, line 15, column 4 to column 29) ## Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine, ## Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified. ## Chain 1 ## Chain 1 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1 finished in 0.6 seconds. ## Chain 2 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2 finished in 0.7 seconds. ## Chain 3 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue: ## Chain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d715e6f455d.stan&#39;, line 15, column 4 to column 29) ## Chain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine, ## Chain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified. ## Chain 3 ## Chain 3 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3 finished in 0.9 seconds. ## ## All 3 chains finished successfully. ## Mean chain execution time: 0.8 seconds. ## Total execution time: 2.4 seconds. ## Warning: 12 of 1500 (1.0%) transitions hit the maximum treedepth limit of 10. ## See https://mc-stan.org/misc/warnings for details. precis(m9.5) ## mean sd 5.5% 94.5% n_eff Rhat4 ## a1 -0.3084331 6.44184205 -10.7135755 10.130659 422.4934 1.002721 ## a2 0.1855495 6.43880572 -10.2473870 10.618578 422.6697 1.002665 ## sigma 0.9279288 0.06630305 0.8234673 1.035225 330.4001 1.010305 traceplot(m9.5) "],["big-entropy-and-the-generalized-linear-model.html", "Chapter 10 Big entropy and the generalized linear model 10.1 Maximum entropy 10.2 Generalized linear models", " Chapter 10 Big entropy and the generalized linear model 10.1 Maximum entropy Recall the information theory function from Chapter 7 \\[H(p) = - \\sum_{i}p_{i}\\text{log}p_{i}\\] This is also known as information entropy Maximum entropy: &gt;&gt; The distribution that can happen the most ways is also the distribution with the biggest information entropy. The distribution with the biggest entropy is the most conservative distribution that obeys its constraints. (p. 301) 5 buckets and 10 pebbles. The 10 pebbles can go into any bucket with the same probability. p &lt;- list() p$A &lt;- c(0,0,10,0,0) p$B &lt;- c(0,1,8,1,0) p$C &lt;- c(0,2,6,2,0) p$D &lt;- c(1,2,4,2,1) p$E &lt;- c(2,2,2,2,2) Turn into probability distribution p_norm &lt;- lapply(p, function(q) q/sum(q)) Calculate information entropy (H &lt;- sapply(p_norm, function(q) -sum(ifelse(q==0, 0, q*log(q))))) ## A B C D E ## 0.0000000 0.6390319 0.9502705 1.4708085 1.6094379 Log(ways) per pebble ways &lt;- c(1,90,1260,37800,113400) logwayspp &lt;- log(ways)/10 plot(x = logwayspp, y = H, xlab = &quot;log(ways) per pebble&quot;, ylab = &quot;entropy&quot;, pch = 16) abline(a = 0, b = 1.38, lty = 2) text(x = logwayspp[1], y = H[1] +0.1, labels = &quot;A&quot;) text(x = logwayspp[2:5], y = H[2:5] - 0.1, labels = c(&quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;)) 10.1.1 Gaussian A generalized normal distribtuion is defined as the probability density: \\[\\text{Pr}(y|\\mu, \\alpha, \\beta) = \\frac{\\beta}{2\\alpha\\Gamma(1/\\beta)}e^{(-\\frac{|y -\\mu|}{\\alpha})^{\\beta}}\\] with \\(\\mu\\) as the location, \\(\\alpha\\) as the scale, and \\(\\beta\\) defining the shape. There is no code in the text to dive into this but here is an example from Solomon Kurz https://bookdown.org/content/4857/big-entropy-and-the-generalized-linear-model.html#gaussian. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✔ tibble 3.1.8 ✔ forcats 0.5.1 ## ✔ readr 2.0.1 ## Warning: package &#39;tibble&#39; was built under R version 4.1.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ tidyr::extract() masks rstan::extract() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ purrr::map() masks rethinking::map() alpha_per_beta &lt;- function(beta, variance = 1) { sqrt((variance * gamma(1 / beta)) / gamma(3 / beta)) } crossing(value = seq(from = -5, to = 5, by = .1), # I arrived at these values by trial and error beta = c(1, 1.5, 2, 4)) %&gt;% mutate(mu = 0, alpha = alpha_per_beta(beta)) %&gt;% # behold the formula for the generalized normal distribution in code! mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta)) %&gt;% # plot ggplot(aes(x = value, y = density, group = beta)) + geom_line(aes(color = beta == 2, size = beta == 2)) + scale_color_manual(values = c(&#39;blue&#39;,&#39;black&#39;)) + scale_size_manual(values = c(1/4, 1.25)) + labs(subtitle = &quot;Guess which color denotes the Gaussian.&quot;) + coord_cartesian(xlim = c(-4, 4)) + theme(legend.position = &quot;none&quot;, panel.background = element_rect(fill = &#39;white&#39;), panel.grid = element_blank()) The \\(\\beta\\) value of 2 gives the GLD a Gaussian appearance. This distribution also has the highest entropy of any of the 4 distributions. These 4 distributions also all have the same variance of 1. crossing(value = -8:8, # this time we need a more densely-packed sequence of `beta` values beta = seq(from = 1, to = 4, length.out = 100)) %&gt;% mutate(mu = 0, alpha = alpha_per_beta(beta)) %&gt;% mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta)) %&gt;% group_by(beta) %&gt;% # this is just an abbreviated version of the formula we used in our first code block summarise(entropy = -sum(density * log(density))) %&gt;% ggplot(aes(x = beta, y = entropy)) + geom_vline(xintercept = 2, color = &quot;black&quot;, lty = 2) + geom_line(size = 2, color = &#39;blue&#39;) + xlab(expression(beta(i.e.*&quot;, &quot;*shape))) + coord_cartesian(ylim = c(1.34, 1.42)) + theme(panel.background = element_rect(fill = &#39;white&#39;), panel.grid = element_blank()) 10.1.2 Binomial Recall long ago the binomial example of blue and white marbles from chanpter 2. The probability of observing \\(y\\) events of outcome 1 and \\(n-y\\) events of outcome 2 is : \\[\\text{Pr}(y_{1}, y_{2}, ..., y_{n}|n, p) = p^{y}(1-p)^{n-y}\\] The constraints of this distribution are: 1. Only 2 unorderd outcomes 2. Constant expected outcome Example 1: Bag of unknown amount of blue and white marbles. We draw 2 marbles (with replacement), so there are \\(2^{2}\\) possibilities: BB, BW, WB, and WW. Suppose we know that the expected number of blue marbles is 1 over two draws (n=2). Lets propose 4 distributions Distribution ww bw wb bb A 1/4 1/4 1/4 1/4 B 2/6 1/6 1/6 2/6 C 1/6 2/6 2/6 1/6 D 1/8 4/8 2/8 1/8 So \\(A\\) here is the usual Binomial distribution with \\(n = 2\\) and \\(p = 0.5\\) \\(B\\), \\(C\\), and \\(D\\) are different distributions with the same expected outcome (1 blue marble). p &lt;- list() p[[1]] &lt;- c(rep(1/4,4)) p[[2]] &lt;- c(2/6, 1/6, 1/6, 2/6) p[[3]] &lt;- c(1/6, 2/6, 2/6, 1/6) p[[4]] &lt;- c(1/8, 4/8, 2/8, 1/8) sapply(p, function(p) sum(p*c(0,1,1,2))) #calculate the sum of 0 blue, 1 blue, or 2 blues for each distribution ## [1] 1 1 1 1 But the entropies are not equal. sapply(p, function(p) -sum(p*log(p))) ## [1] 1.386294 1.329661 1.329661 1.213008 Because even spread of the probability increases information entropy, Distribution A is favoured. par(mfrow = c(2,2)) for(i in 1:4){ plot(x = c(1:4), y = p[[i]], pch = 1, col = rangi2, ylim = c(0,0.5), type = &#39;l&#39;, xaxt = &#39;n&#39;, xlab = &#39;&#39;, ylab = &#39;&#39;) points(x = c(1:4), y = p[[i]], pch = 16, col = rangi2) axis(side = 1, at = 1:4, labels = c(&quot;ww&quot;,&quot;bw&quot;,&quot;wb&quot;,&quot;bb&quot;)) mtext(paste0(LETTERS[i])) } What if the expected value was 1.4? (\\(p = 1.4/2\\)) p &lt;- 0.7 (A &lt;- c((1-p)^2, p*(1-p), (1-p)*p, p^2)) ## [1] 0.09 0.21 0.21 0.49 Hmm, not flat. What is the entropy? -sum(A*log(A)) ## [1] 1.221729 Let’s make sure that no other distribution has more entropy by simulating 100000 distributions. sim.p &lt;- function(G=1.4){ x123 &lt;- runif(3) #three random numbers 0-1 x4 &lt;- ((G)*sum(x123)-x123[2]-x123[3])/(2-G) #solves for relative value of 4th,using G #crate a probability distribution z &lt;- sum(c(x123,x4)) p &lt;- c(x123, x4)/z list(H = -sum(p*log(p)), p=p) } H &lt;- replicate(1e5, sim.p(1.4)) dens(as.numeric(H[1,]), adj=0.1) Let’s see what the highest entropy distribution looks like entropies &lt;- as.numeric(H[1,]) distributions &lt;- H[2,] max(entropies) ## [1] 1.221728 distributions[which.max(entropies)] ## [[1]] ## [1] 0.09014834 0.20993332 0.20976999 0.49014834 Nearly identical to our binomial before. 10.2 Generalized linear models Before we assumed Gaussian distribution and placed the linear model in the mean (\\(\\mu\\)) of that distribution. For discrete or bounded outcomes, this normal distribution won’t work. For example, if you were counting blue marbles, you can’t have a negative count. So we need to use something other than Gaussian. \\[y_{i} \\sim \\text{Binomial}(n,p_{i})\\\\ f(p_{i}) = \\alpha + \\beta(x_{i} - \\overline{x})\\] here the count of blue marbles is of the number of draws \\(n\\) and expected value \\(np\\). In the second line \\(f\\) represents the link function that bounds the probability between 0 and 1. 10.2.1 Meet the family The Exponential Family is a group of maximum entropy distributions (with constraints) PLOT distributions here** x &lt;- seq(from = 0, to = 5, length.out = 100) g &lt;- seq(from = -5, to = 5, length.out = 100) Gamma &lt;- dgamma(x, 2, 2) Exponential &lt;- dexp(x) G_density &lt;- dnorm(seq(from = -5, to = 5, length.out = 100)) P_density &lt;- dpois(0:20, lambda = 2.5) B_density &lt;- dbinom(0:10, size = 10, prob = 0.85) par(mfrow=c(3, 2)) plot(x = x, y = Gamma , type = &#39;l&#39;, col = &#39;blue&#39;) plot(x = x, y = Exponential , type = &#39;l&#39;, col = &#39;blue&#39;) plot(x = g, y = G_density , type = &#39;l&#39;, col = &#39;blue&#39;) plot(x = 0:20, y = P_density , type = &#39;l&#39;, col = &#39;blue&#39;) plot(x = 0:10, y = B_density , type = &#39;l&#39;, col = &#39;blue&#39;) 10.2.2 Linking linear models to distributions Lets get back to the Link functions. Link functions work to avoid mathematicla mistakes like negative counts or probabilities greater than 1. There are two common types of link functions. The Logit Link Made for probability masses that have to be between 0 and 1. Here is what it could look like: \\[y_{i} \\sim \\text{Binomial}(n, p_{i})\\\\ \\text{logit}(p_{i}) = \\alpha + \\beta x_{i}\\] Where the logit is the log-odds \\[\\text{logit}(p_{i}) = \\text{log}\\frac{p_{i}}{1 - p_{i}}\\] so we can write \\[\\text{log}\\frac{p_{i}}{1 - p_{i}} = \\alpha + \\beta x_{i}\\] and solve for \\(p_{i}\\) \\[p_{i} = \\frac{\\text{exp}(\\alpha + \\beta x_{i})}{1 + \\text{exp}(\\alpha + \\beta x_{i})}\\] This allows you to transform a linear model to a non-linear probability density. x &lt;- seq(from = -1, to = 1, by = 0.25) alpha &lt;- 0 beta &lt;- 4 log_odds &lt;- alpha + x*beta probability &lt;- exp(alpha + x*beta)/(1 + exp(alpha + x*beta)) lines &lt;- cbind(x, log_odds, probability) beta &lt;- 2 x &lt;- seq(from = -1.5, to = 1.5, length.out = 50) log_odds &lt;- alpha + x*beta probability &lt;- exp(alpha + x*beta)/(1 + exp(alpha + x*beta)) df &lt;- cbind(x, log_odds, probability) par(mfrow=c(1,2)) plot(x = df[,1], y = df[,2], xlim = c(-1,1), ylim = c(-4,4), type = &#39;l&#39;, xlab = &quot;x&quot;, ylab = &quot;log-odds&quot;, col = rangi2, lwd = 2) abline(h = lines[,2], col = col.alpha(&#39;grey50&#39;, 0.5)) plot(x = df[,1], y = df[,3], xlim = c(-1,1), ylim = c(0,1), type = &#39;l&#39;, xlab = &quot;x&quot;, ylab = &quot;probability&quot;, col = rangi2, lwd = 2) abline(h = lines[,3], col = col.alpha(&#39;grey50&#39;, 0.5)) Here the two ends of the linear line of the log-odds gets compressed in the logit link. You can imagine that steeper lines would have a sharper ‘S’ shape to them. Just remember that here the \\(\\beta\\) term no longer creates a constant rate of change in the outcome. Events at the extremes (-1,1) have very little affect on the change in probability mass. Log Link This function is for positive real numbers in a linear model. Suppose instead of modeling \\(\\mu\\) in a gaussian distribution, we wanted to model \\(\\sigma\\) such that: \\[y_{i} \\sim \\text{Normal}(\\mu, \\sigma_{i})\\\\ \\text{log}(\\sigma_{i}) = \\alpha + \\beta x_{i}\\] This is helpful to avoid something impossible like negative \\(\\sigma\\). So we can change our \\(\\sigma\\) to: \\[\\sigma_{i} = \\text{exp}(\\alpha + \\beta x_{i})\\] alpha &lt;- 0 beta &lt;- 2 log_measurement &lt;- -3:3 measurement &lt;- exp(-3:3) lines &lt;- cbind(log_measurement, measurement) x &lt;- seq(from = -1.5, to = 1.5, length.out = 50) log_measurement &lt;- alpha + x*beta measurement &lt;- exp(alpha + x*beta) df &lt;- cbind(x, log_measurement, measurement) par(mfrow=c(1,2)) plot(x = df[,1], y = df[,2], xlim = c(-1,1), ylim = c(-4,4), type = &#39;l&#39;, xlab = &quot;x&quot;, ylab = &quot;log-measurement&quot;, col = rangi2, lwd = 2) abline(h = lines[,1], col = col.alpha(&#39;grey50&#39;, 0.5)) plot(x = df[,1], y = df[,3], xlim = c(-1,1), ylim = c(0,10), type = &#39;l&#39;, xlab = &quot;x&quot;, ylab = &quot;original measurement&quot;, col = rangi2, lwd = 2) abline(h = lines[,2], col = col.alpha(&#39;grey50&#39;, 0.5)) 10.2.3 Omitted variable bias (again) link functions can create more trouble for properly inferring what your model is telling you as even variables that aren’t confounders can bias the inference. 10.2.4 Absolute relative differences When a link function is in your model , remember that big-beta coefficients don’t mean big effects on the outcome. 10.2.5 GLMs and information criteria Remember to only compare models of the same outcome distribution with information criteria. comparing a Gaussian outcome model to a Gamma model won’t tell you anything about their differences. "],["god-spiked-the-integers.html", "Chapter 11 God spiked the integers 11.1 Binomial regression", " Chapter 11 God spiked the integers GLMs are complex machines that are hard to interpret without understanding the whole and each of the parts within. To get started on trying to understand GLMs, we will look at count data (0, 1, 2, … etc). Binomial regression will be when we have 2 defined outcomes that are both measured. (alive/dead, accept/reject) Poisson regression is for counts that have no known maximum (number of animals in a country) or: number of significance tests in an issue of Psychological Science. (p. 323) 11.1 Binomial regression Going waaaay back to the globe tossing model \\[y \\sim \\text{Binomial}(n, p)\\] here \\(y\\) is the count (0 or positive whole number), \\(p\\) is the probability any ‘trial’ is a success, and \\(n\\) is the number of trials. For the binomial to work we must have a constant expected value. There are 2 common GLMs for Binomials 1. Logistic regression - independent outcomes are 0 or 1 2. Aggregated Binomial Regression - samilar covariate trials are grouped Both of the above will make use of the Logit link function 11.1.1 Logistic regression : Prosocial chimpanzees EXPERIMENT: chimps can use levers to move food items on a table. The left lever will bring the left food item closer and the right lever will move the right food item closer. This is mirrored across the table but there is only one food item in either the left or the right (not both). Condition one (control): There is not another chimp across the table. empty social food item will randomly switch from left to right Condition two: there is another chimp across the table. Choosing to move the side with the social food item is counted as prosocial. choosing to move the empty social food dish is anti-social. Again left and right for the social dish is random. #library(rethinking) data(chimpanzees) d &lt;- chimpanzees We are going to count pulled_left (\\(y\\)) predicted by prosoc_left and condition. There are four combinations: number &lt;- 1:4 prosocial_left &lt;- c(0,1,0,1) condition &lt;- c(0,0,1,1) description &lt;- c(&quot;Two food items on the right and no partner&quot;, &quot;Two food items on the left and no partner&quot;, &quot;Two food items on the right and partner present&quot;, &quot;Two food items on the left and partner present&quot;) experiment &lt;- cbind(number, prosocial_left, condition, description) knitr::kable(experiment, &quot;html&quot;) number prosocial_left condition description 1 0 0 Two food items on the right and no partner 2 1 0 Two food items on the left and no partner 3 0 1 Two food items on the right and partner present 4 1 1 Two food items on the left and partner present Now we can make an index to match each of the 4 outcomes above d$treatment &lt;- 1 + d$prosoc_left + 2*d$condition verify it worked xtabs(~treatment + prosoc_left + condition, data = d) ## , , condition = 0 ## ## prosoc_left ## treatment 0 1 ## 1 126 0 ## 2 0 126 ## 3 0 0 ## 4 0 0 ## ## , , condition = 1 ## ## prosoc_left ## treatment 0 1 ## 1 0 0 ## 2 0 0 ## 3 126 0 ## 4 0 126 Now lets build the model \\[L_{i} \\sim \\text{Binomial}(1, p_{i})\\\\ \\text{logit}(p_{i}) = \\alpha_{ACTOR[i]} + \\beta_{TREATMENT[i]}\\\\ \\alpha_{j} \\sim \\text{TBD}\\\\ \\beta_{k} \\sim \\text{TBD}\\] So \\(L\\) is whether the left lever was pulled. \\(\\alpha\\) has 7 parameters (for 7 chimps) and \\(\\beta\\) we know has 4 parameters for treatments. Now we can go ahead and try to define the priors for our model. We can start conservative \\[L_{i} \\sim \\text{Binomial}(1, p_{i})\\\\ \\text{logit}(p_{i}) = \\alpha\\\\ \\alpha \\sim \\text{Normal}(0, \\omega)\\] we will start with a flat prior where \\(\\omega\\) is 10 m11.1 &lt;- quap( alist( pulled_left ~ dbinom(1, p), logit(p) &lt;- a, a ~ dnorm(0, 10) ), data = d ) and sample the prior set.seed(11) prior &lt;- extract.prior(m11.1, n=1e4) now we transform the logit to probability space p &lt;- inv_logit(prior$a) dens(p, adj = 0.1) So the model (before seeing data) thinks that either its always the left lever or never the left lever. m11.1 &lt;- quap( alist( pulled_left ~ dbinom(1, p), logit(p) &lt;- a, a ~ dnorm(0, 1.5) ), data = d ) set.seed(11) prior &lt;- extract.prior(m11.1, n=1e4) p2 &lt;- inv_logit(prior$a) dens(c(p2), adj = 0.1, col=c(&#39;black&#39;, rangi2)) m11.2 &lt;- quap( alist( pulled_left ~ dbinom(1, p), logit(p) &lt;- a + b[treatment], a ~ dnorm(0, 1.5), b[treatment] ~ dnorm(0,10) ), data = d ) set.seed(11) prior &lt;- extract.prior(m11.2, n = 1e4) p &lt;- sapply(1:4, function(k) inv_logit(prior$a + prior$b[,k])) p now holds the prior probability for each of the 4 treatments. Let’s investigate the difference between the first two dens(abs(p[,1]-p[,2]), adj = 0.1) So now the model thinks that these two treatments are either the same, or completely different. Let’s tighten it up. m11.3 &lt;- quap( alist( pulled_left ~ dbinom(1, p), logit(p) &lt;- a + b[treatment], a ~ dnorm(0, 1.5), b[treatment] ~ dnorm(0,0.5) ), data = d ) set.seed(11) prior &lt;- extract.prior(m11.3, n = 1e4) p &lt;- sapply(1:4, function(k) inv_logit(prior$a + prior$b[,k])) mean(abs(p[,1] - p[,2])) ## [1] 0.09866385 dens(abs(p[,1]-p[,2]), adj = 0.1) Now the model finds them rather similar with a mean difference of about 10%. Great, now lets get ready for HMC #trimmed data list dat_list &lt;- list( pulled_left = d$pulled_left, actor = d$actor, treatment = as.integer(d$treatment) ) m11.4 &lt;- ulam( alist( pulled_left ~ dbinom(1, p), logit(p) &lt;- a[actor] + b[treatment], a[actor] ~ dnorm(0, 1.5), b[treatment] ~ dnorm(0, 0.5) ), data = dat_list, chains = 4, log_lik = TRUE ) ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d717daa4f58.stan&#39;, line 2, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d717daa4f58.stan&#39;, line 3, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d717daa4f58.stan&#39;, line 4, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Running MCMC with 4 sequential chains, with 1 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1 finished in 1.0 seconds. ## Chain 2 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2 finished in 1.1 seconds. ## Chain 3 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3 finished in 1.0 seconds. ## Chain 4 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 4 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 4 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 4 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 4 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 4 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 4 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 4 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 4 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 4 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 4 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 4 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 4 finished in 0.9 seconds. ## ## All 4 chains finished successfully. ## Mean chain execution time: 1.0 seconds. ## Total execution time: 4.3 seconds. precis(m11.4, depth = 2) ## mean sd 5.5% 94.5% n_eff Rhat4 ## a[1] -0.43321908 0.3247576 -0.94113808 0.09200583 691.1621 1.0046920 ## a[2] 3.90252674 0.7348453 2.81297680 5.18888460 1466.4227 0.9999074 ## a[3] -0.72721350 0.3330830 -1.26196300 -0.18413188 620.9283 1.0075010 ## a[4] -0.73085682 0.3354239 -1.26737895 -0.19186165 562.2104 1.0125086 ## a[5] -0.43810214 0.3176830 -0.91872250 0.08249774 574.5202 1.0095037 ## a[6] 0.48484521 0.3338921 -0.02784441 1.00530055 725.3433 1.0055677 ## a[7] 1.97243545 0.4032592 1.33273880 2.63883335 799.9081 1.0051345 ## b[1] -0.05643471 0.2846210 -0.50818570 0.39832506 550.2008 1.0086378 ## b[2] 0.47124792 0.2793006 0.01748083 0.90848956 615.1956 1.0087848 ## b[3] -0.39073193 0.2800441 -0.84979488 0.04572377 522.7278 1.0106370 ## b[4] 0.35079248 0.2864463 -0.11901696 0.78809074 553.9905 1.0065406 What? Yeah, me too. Let’s break this down. post &lt;- extract.samples(m11.4) p_left &lt;- inv_logit(post$a) precis_plot(precis(as.data.frame(p_left)), xlim = c(0,1)) Here each row is a chimpanzee and you can see most actually preferred the right lever. One chimp really liked the left lever. This tells us left or right, but how do we know if they were being prosocial? labs &lt;- c(&quot;R/N&quot;, &quot;L/N&quot;, &quot;R/P&quot;, &quot;L/P&quot;) precis_plot(precis(m11.4, depth = 2, pars = &quot;b&quot;), labels = labs) These coefficients are still in logit space so don’t take them at face value for each treatment yet. Let’s contrast the Right side and the Left side to see if the coefficients are very different. diffs &lt;- list( db13 = post$b[,1] - post$b[,3], db24 = post$b[,2] - post$b[,4] ) precis_plot(precis(diffs)) So now we are looking at the difference between the no partner/partner on the right (db13) and left sides (db24). The right has a bit of a stronger ‘prosocial’ signal but not important as they both have large intervals. Now we can try a posterior predictive check. Let’s calculate the proportion of left pulls for each actor in each treatment and see how it matches the posterior. pl &lt;- by(d$pulled_left, list(d$actor, d$treatment), mean) pl[1,] ## 1 2 3 4 ## 0.3333333 0.5000000 0.2777778 0.5555556 Now we can use these observed proportions and compare them to the models predictions for each combination of actor and treatment. dat &lt;- list(actor = rep(1:7, each=4), treatment = rep(1:4, times = 7)) p_post &lt;- link(m11.4, data = dat) p_mu &lt;- apply(p_post, 2, mean) p_ci &lt;- apply(p_post, 2, PI) And we can plot to see how well our model predicts the data par(mfrow=c(2,1)) #observations plot( NULL,xlim=c(1,28),ylim=c(0,1),xlab=&quot;&quot;, ylab=&quot;proportion left lever&quot;,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;) axis( 2,at=c(0,0.5,1),labels=c(0,0.5,1)) abline( h=0.5,lty=2) for (j in 1:7)abline(v=(j-1)*4+4.5,lwd=0.5) for (j in 1:7)text((j-1)*4+2.5,1.1,concat(&quot;actor&quot;,j),xpd=TRUE) for (j in (1:7)[-2]){ lines( (j-1)*4+c(1,3),pl[j,c(1,3)],lwd=2,col=rangi2) lines( (j-1)*4+c(2,4),pl[j,c(2,4)],lwd=2,col=rangi2) } points( 1:28,t(pl),pch=16,col=&quot;white&quot;,cex=1.7) points( 1:28,t(pl),pch=c(1,1,16,16),col=rangi2,lwd=2) yoff &lt;-0.01 text( 1,pl[1,1]-yoff,&quot;R/N&quot;,pos=1,cex=0.8) text( 2,pl[1,2]+yoff,&quot;L/N&quot;,pos=3,cex=0.8) text( 3,pl[1,3]-yoff,&quot;R/P&quot;,pos=1,cex=0.8) text( 4,pl[1,4]+yoff,&quot;L/P&quot;,pos=3,cex=0.8) mtext( &quot;observed proportions\\n&quot;) ##prediction plot plot( NULL,xlim=c(1,28),ylim=c(0,1),xlab=&quot;&quot;, ylab=&quot;proportion left lever&quot;,xaxt=&quot;n&quot;,yaxt=&quot;n&quot;) axis( 2,at=c(0,0.5,1),labels=c(0,0.5,1)) abline( h=0.5,lty=2) for (j in 1:7)abline(v=(j-1)*4+4.5,lwd=0.5) for (j in 1:7)text((j-1)*4+2.5,1.1,concat(&quot;actor&quot;,j),xpd=TRUE) for (j in (1:7)[-2]){ lines( (j-1)*4+c(1,3),p_mu[c( ((4*j)-3), ((4*j)-1) )],lwd=2,col=&#39;black&#39;) lines( (j-1)*4+c(2,4),p_mu[c( ((4*j)-2), (4*j) )],lwd=2,col=&#39;black&#39;) } for (j in (1:7)[-2]){ lines( (j-1)*4+c(1,1) ,p_ci[c(1,2), ((4*j)-3)],lwd=2,col=&#39;black&#39;) lines( (j-1)*4+c(3,3) ,p_ci[c(1,2), ((4*j)-1)],lwd=2,col=&#39;black&#39;) lines( (j-1)*4+c(2,2) ,p_ci[c(1,2), ((4*j)-2)],lwd=2,col=&#39;black&#39;) lines( (j-1)*4+c(4,4) ,p_ci[c(1,2), (4*j)],lwd=2,col=&#39;black&#39;) } points( 1:28,t(p_mu),pch=16,col=&quot;white&quot;,cex=1.7) points( 1:28,t(p_mu),pch=c(1,1,16,16),col=&#39;black&#39;,lwd=2) mtext( &quot;posterior predictions\\n&quot;) Now we see that there is almost no expected change from the model between partner present or absent. Also, there doesn’t appear to be any affect of left vs right. But we could always check d$side &lt;- d$prosoc_left + 1 #right = 1, left = 2 d$cond &lt;- d$condition + 1 #no partner = 1, partner = 2 dat_list2 &lt;- list( pulled_left = d$pulled_left, actor = d$actor, side = d$side, cond = d$cond ) m11.5 &lt;- ulam( alist( pulled_left ~ dbinom(1, p), logit(p) &lt;- a[actor] + bs[side] + bc[cond], a[actor] ~ dnorm(0, 1.5), bs[side] ~ dnorm(0, 0.5), bc[cond] ~ dnorm(0, 0.5) ), data = dat_list2, chains = 4, log_lik = TRUE ) ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d7174db45dc.stan&#39;, line 2, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d7174db45dc.stan&#39;, line 3, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d7174db45dc.stan&#39;, line 4, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d7174db45dc.stan&#39;, line 5, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Running MCMC with 4 sequential chains, with 1 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1 finished in 1.9 seconds. ## Chain 2 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2 finished in 1.7 seconds. ## Chain 3 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3 finished in 1.8 seconds. ## Chain 4 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 4 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 4 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 4 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 4 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 4 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 4 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 4 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 4 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 4 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 4 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 4 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 4 finished in 1.8 seconds. ## ## All 4 chains finished successfully. ## Mean chain execution time: 1.8 seconds. ## Total execution time: 7.5 seconds. and then we can compare with PSIS compare(m11.5, m11.4, func=PSIS) ## PSIS SE dPSIS dSE pPSIS weight ## m11.5 530.7368 19.15741 0.000000 NA 7.791580 0.660974 ## m11.4 532.0721 18.89705 1.335275 1.278312 8.383866 0.339026 11.1.2 Relative shark and absolute deer What was described above was the absolute effect on the outcome. We can also calculate the relative effects or proportional odds. Here is the switch from treatment 2 \\(\\rightarrow\\) 4 (adding a partner, left side food). post &lt;- extract.samples(m11.4) mean(exp(post$b[,4]-post$b[,2])) ## [1] 0.9198223 So we would multiply the odds of pulling the left lever by 0.92 which is a 8% reduction in pulling the left lever. This isn’t enough to make any big picture inferences though. 11.1.3 Aggregated binomial: Chimps condensed Now we can analyze the data with the sets of variables aggregated or group in similar scenarios. Like how many left hand pulls across all trials data(chimapnzees) ## Warning in data(chimapnzees): data set &#39;chimapnzees&#39; not found d &lt;- chimpanzees d$treatment &lt;- 1 + d$prosoc_left + 2*d$condition d$side &lt;- d$prosoc_left + 1 #right 1, left 2 d$cond &lt;- d$condition + 1 # no partner 1, partner 2 d_aggregated &lt;- aggregate( d$pulled_left, list( treatment = d$treatment, actor = d$actor, side = d$side, cond = d$cond ), sum ) colnames(d_aggregated)[5] &lt;- &quot;left_pulls&quot; Now we can use the aggregated data to get the same results dat &lt;- with(d_aggregated, list( left_pulls = left_pulls, treatment = treatment, actor = actor, cond = cond )) m11.6 &lt;- ulam( alist( left_pulls ~ dbinom(18, p), logit(p) &lt;- a[actor] + b[treatment], a[actor] ~ dnorm(0, 1.5), b[treatment] ~ dnorm(0, 0.5) ), data = dat, chains = 4, log_lik = TRUE ) ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d7122627bf1.stan&#39;, line 2, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d7122627bf1.stan&#39;, line 3, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d7122627bf1.stan&#39;, line 4, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d7122627bf1.stan&#39;, line 5, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Running MCMC with 4 sequential chains, with 1 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1 finished in 0.1 seconds. ## Chain 2 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2 finished in 0.1 seconds. ## Chain 3 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3 finished in 0.1 seconds. ## Chain 4 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 4 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 4 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 4 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 4 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 4 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 4 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 4 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 4 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 4 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 4 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 4 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 4 finished in 0.1 seconds. ## ## All 4 chains finished successfully. ## Mean chain execution time: 0.1 seconds. ## Total execution time: 0.6 seconds. compare(m11.6, m11.4, func=PSIS) ## Warning in compare(m11.6, m11.4, func = PSIS): Different numbers of observations found for at least two models. ## Model comparison is valid only for models fit to exactly the same observations. ## Number of observations for each model: ## m11.6 28 ## m11.4 504 ## Some Pareto k values are high (&gt;0.5). Set pointwise=TRUE to inspect individual points. ## PSIS SE dPSIS dSE pPSIS weight ## m11.6 115.4243 8.689378 0.0000 NA 9.029483 1.000000e+00 ## m11.4 532.0721 18.897046 416.6478 9.686421 8.383866 3.358034e-91 The distribution of the aggregated data is larger because the model knows the total number of trials and the number of successes. Here is a comparison of 6 successes in 9 trials aggregated vs unaggregated #deviance of aggregate -2*dbinom(6, 9, 0.2, log = TRUE) ## [1] 11.79048 #deviance of unaggregated -2*sum(dbern(c(1,1,1,1,1,1,0,0,0), 0.2, log = TRUE)) ## [1] 20.65212 This doesn’t mean anything for the posterior though. It will be the same across the two methods. If you are interested in the output of WAIC of PSIS, you should use the unaggregated model. 11.1.4 Aggregated admissions data(UCBadmit) d &lt;- UCBadmit d ## dept applicant.gender admit reject applications ## 1 A male 512 313 825 ## 2 A female 89 19 108 ## 3 B male 353 207 560 ## 4 B female 17 8 25 ## 5 C male 120 205 325 ## 6 C female 202 391 593 ## 7 D male 138 279 417 ## 8 D female 131 244 375 ## 9 E male 53 138 191 ## 10 E female 94 299 393 ## 11 F male 22 351 373 ## 12 F female 24 317 341 Is there gender bias? \\[A_{i} \\sim \\text{Binomial}(N_{i}, p_{i})\\\\ \\text{logit}(p_{i}) = \\alpha_{GID[i]}\\\\ \\alpha_{j} \\sim \\text{Normal}(0, 1.5)\\] dat_list &lt;- list( admit = d$admit, applications = d$applications, gid = ifelse(d$applicant.gender == &quot;male&quot;, 1, 2) ) m11.7 &lt;- ulam( alist( admit ~ dbinom(applications, p), logit(p) &lt;- a[gid], a[gid] ~ dnorm(0, 1.5) ), data = dat_list, chains = 4 ) ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d713e3cc3b8.stan&#39;, line 2, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d713e3cc3b8.stan&#39;, line 3, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d713e3cc3b8.stan&#39;, line 4, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Running MCMC with 4 sequential chains, with 1 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1 finished in 0.0 seconds. ## Chain 2 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2 finished in 0.0 seconds. ## Chain 3 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3 finished in 0.0 seconds. ## Chain 4 Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 4 Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 4 Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 4 Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 4 Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 4 Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 4 Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 4 Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 4 Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 4 Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 4 Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 4 Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 4 finished in 0.0 seconds. ## ## All 4 chains finished successfully. ## Mean chain execution time: 0.0 seconds. ## Total execution time: 0.7 seconds. precis(m11.7, depth = 2) ## mean sd 5.5% 94.5% n_eff Rhat4 ## a[1] -0.2203097 0.03885931 -0.2827576 -0.1573031 1344.449 0.9999338 ## a[2] -0.8297067 0.05104611 -0.9098031 -0.7481747 1371.791 1.0022231 post &lt;- extract.samples(m11.7) diff_a &lt;- post$a[,1] - post$a[,2] diff_p &lt;- inv_logit(post$a[,1]) - inv_logit(post$a[,2]) precis(list(diff_a=diff_a, diff_p=diff_p)) ## mean sd 5.5% 94.5% histogram ## diff_a 0.6093971 0.06372659 0.5022138 0.7068903 ▁▁▃▇▇▅▂▁▁ ## diff_p 0.1413496 0.01434578 0.1173636 0.1631168 ▁▁▂▃▇▇▅▂▁▁ The log-odds difference (diff_a) is positive which indicates a higher admit prob for males. the outcome probability is 12-16% higher for males. Visualize the posterior postcheck(m11.7) for(i in 1:6){ x &lt;- 1 + 2*(i-1) y1 &lt;- d$admit[x]/d$applications[x] y2 &lt;- d$admit[x+1]/d$applications[x+1] lines(c(x, x+1), c(y1, y2), col=rangi2, lwd=2) text(x+0.5, (y1+y2)/2 + 0.05, d$dept[x], cex = 0.8, col=rangi2) } Women overall have less probability of getting admitted, but there is within deparrtment variation. Let’s account for this \\[A_{i} \\sim \\text{Binomial}(N_{i}, p_{i})\\\\ \\text{logit}(p_{i}) = \\alpha_{GID[i]} + \\delta_{DEPT[i]}\\\\ \\alpha_{j} \\sim \\text{Normal}(0, 1.5)\\\\ \\delta_{k} \\sim \\text{Normal}(0, 1.5)\\] dat_list$dept_id &lt;- rep(1:6, each = 2) m11.8 &lt;- ulam( alist( admit ~ dbinom(applications, p), logit(p) &lt;- a[gid] + delta[dept_id], a[gid] ~ dnorm(0, 1.5), delta[dept_id] ~ dnorm(0, 1.5) ), data = dat_list, chains = 4, iter = 4000 ) ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d712c56c72c.stan&#39;, line 2, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d712c56c72c.stan&#39;, line 3, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d712c56c72c.stan&#39;, line 4, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Warning in &#39;/var/folders/mm/1f07t6b52_5cp3r1k03vcnvh0000gn/T/RtmpfntIhr/model-d712c56c72c.stan&#39;, line 5, column 4: Declaration ## of arrays by placing brackets after a variable name is deprecated and ## will be removed in Stan 2.32.0. Instead use the array keyword before the ## type. This can be changed automatically using the auto-format flag to ## stanc ## Running MCMC with 4 sequential chains, with 1 thread(s) per chain... ## ## Chain 1 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 1 Iteration: 100 / 4000 [ 2%] (Warmup) ## Chain 1 Iteration: 200 / 4000 [ 5%] (Warmup) ## Chain 1 Iteration: 300 / 4000 [ 7%] (Warmup) ## Chain 1 Iteration: 400 / 4000 [ 10%] (Warmup) ## Chain 1 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 1 Iteration: 600 / 4000 [ 15%] (Warmup) ## Chain 1 Iteration: 700 / 4000 [ 17%] (Warmup) ## Chain 1 Iteration: 800 / 4000 [ 20%] (Warmup) ## Chain 1 Iteration: 900 / 4000 [ 22%] (Warmup) ## Chain 1 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 1 Iteration: 1100 / 4000 [ 27%] (Warmup) ## Chain 1 Iteration: 1200 / 4000 [ 30%] (Warmup) ## Chain 1 Iteration: 1300 / 4000 [ 32%] (Warmup) ## Chain 1 Iteration: 1400 / 4000 [ 35%] (Warmup) ## Chain 1 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 1 Iteration: 1600 / 4000 [ 40%] (Warmup) ## Chain 1 Iteration: 1700 / 4000 [ 42%] (Warmup) ## Chain 1 Iteration: 1800 / 4000 [ 45%] (Warmup) ## Chain 1 Iteration: 1900 / 4000 [ 47%] (Warmup) ## Chain 1 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 1 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 1 Iteration: 2100 / 4000 [ 52%] (Sampling) ## Chain 1 Iteration: 2200 / 4000 [ 55%] (Sampling) ## Chain 1 Iteration: 2300 / 4000 [ 57%] (Sampling) ## Chain 1 Iteration: 2400 / 4000 [ 60%] (Sampling) ## Chain 1 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 1 Iteration: 2600 / 4000 [ 65%] (Sampling) ## Chain 1 Iteration: 2700 / 4000 [ 67%] (Sampling) ## Chain 1 Iteration: 2800 / 4000 [ 70%] (Sampling) ## Chain 1 Iteration: 2900 / 4000 [ 72%] (Sampling) ## Chain 1 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 1 Iteration: 3100 / 4000 [ 77%] (Sampling) ## Chain 1 Iteration: 3200 / 4000 [ 80%] (Sampling) ## Chain 1 Iteration: 3300 / 4000 [ 82%] (Sampling) ## Chain 1 Iteration: 3400 / 4000 [ 85%] (Sampling) ## Chain 1 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 1 Iteration: 3600 / 4000 [ 90%] (Sampling) ## Chain 1 Iteration: 3700 / 4000 [ 92%] (Sampling) ## Chain 1 Iteration: 3800 / 4000 [ 95%] (Sampling) ## Chain 1 Iteration: 3900 / 4000 [ 97%] (Sampling) ## Chain 1 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 1 finished in 0.5 seconds. ## Chain 2 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 2 Iteration: 100 / 4000 [ 2%] (Warmup) ## Chain 2 Iteration: 200 / 4000 [ 5%] (Warmup) ## Chain 2 Iteration: 300 / 4000 [ 7%] (Warmup) ## Chain 2 Iteration: 400 / 4000 [ 10%] (Warmup) ## Chain 2 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 2 Iteration: 600 / 4000 [ 15%] (Warmup) ## Chain 2 Iteration: 700 / 4000 [ 17%] (Warmup) ## Chain 2 Iteration: 800 / 4000 [ 20%] (Warmup) ## Chain 2 Iteration: 900 / 4000 [ 22%] (Warmup) ## Chain 2 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 2 Iteration: 1100 / 4000 [ 27%] (Warmup) ## Chain 2 Iteration: 1200 / 4000 [ 30%] (Warmup) ## Chain 2 Iteration: 1300 / 4000 [ 32%] (Warmup) ## Chain 2 Iteration: 1400 / 4000 [ 35%] (Warmup) ## Chain 2 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 2 Iteration: 1600 / 4000 [ 40%] (Warmup) ## Chain 2 Iteration: 1700 / 4000 [ 42%] (Warmup) ## Chain 2 Iteration: 1800 / 4000 [ 45%] (Warmup) ## Chain 2 Iteration: 1900 / 4000 [ 47%] (Warmup) ## Chain 2 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 2 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 2 Iteration: 2100 / 4000 [ 52%] (Sampling) ## Chain 2 Iteration: 2200 / 4000 [ 55%] (Sampling) ## Chain 2 Iteration: 2300 / 4000 [ 57%] (Sampling) ## Chain 2 Iteration: 2400 / 4000 [ 60%] (Sampling) ## Chain 2 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 2 Iteration: 2600 / 4000 [ 65%] (Sampling) ## Chain 2 Iteration: 2700 / 4000 [ 67%] (Sampling) ## Chain 2 Iteration: 2800 / 4000 [ 70%] (Sampling) ## Chain 2 Iteration: 2900 / 4000 [ 72%] (Sampling) ## Chain 2 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 2 Iteration: 3100 / 4000 [ 77%] (Sampling) ## Chain 2 Iteration: 3200 / 4000 [ 80%] (Sampling) ## Chain 2 Iteration: 3300 / 4000 [ 82%] (Sampling) ## Chain 2 Iteration: 3400 / 4000 [ 85%] (Sampling) ## Chain 2 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 2 Iteration: 3600 / 4000 [ 90%] (Sampling) ## Chain 2 Iteration: 3700 / 4000 [ 92%] (Sampling) ## Chain 2 Iteration: 3800 / 4000 [ 95%] (Sampling) ## Chain 2 Iteration: 3900 / 4000 [ 97%] (Sampling) ## Chain 2 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2 finished in 0.6 seconds. ## Chain 3 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 3 Iteration: 100 / 4000 [ 2%] (Warmup) ## Chain 3 Iteration: 200 / 4000 [ 5%] (Warmup) ## Chain 3 Iteration: 300 / 4000 [ 7%] (Warmup) ## Chain 3 Iteration: 400 / 4000 [ 10%] (Warmup) ## Chain 3 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 3 Iteration: 600 / 4000 [ 15%] (Warmup) ## Chain 3 Iteration: 700 / 4000 [ 17%] (Warmup) ## Chain 3 Iteration: 800 / 4000 [ 20%] (Warmup) ## Chain 3 Iteration: 900 / 4000 [ 22%] (Warmup) ## Chain 3 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 3 Iteration: 1100 / 4000 [ 27%] (Warmup) ## Chain 3 Iteration: 1200 / 4000 [ 30%] (Warmup) ## Chain 3 Iteration: 1300 / 4000 [ 32%] (Warmup) ## Chain 3 Iteration: 1400 / 4000 [ 35%] (Warmup) ## Chain 3 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 3 Iteration: 1600 / 4000 [ 40%] (Warmup) ## Chain 3 Iteration: 1700 / 4000 [ 42%] (Warmup) ## Chain 3 Iteration: 1800 / 4000 [ 45%] (Warmup) ## Chain 3 Iteration: 1900 / 4000 [ 47%] (Warmup) ## Chain 3 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 3 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 3 Iteration: 2100 / 4000 [ 52%] (Sampling) ## Chain 3 Iteration: 2200 / 4000 [ 55%] (Sampling) ## Chain 3 Iteration: 2300 / 4000 [ 57%] (Sampling) ## Chain 3 Iteration: 2400 / 4000 [ 60%] (Sampling) ## Chain 3 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 3 Iteration: 2600 / 4000 [ 65%] (Sampling) ## Chain 3 Iteration: 2700 / 4000 [ 67%] (Sampling) ## Chain 3 Iteration: 2800 / 4000 [ 70%] (Sampling) ## Chain 3 Iteration: 2900 / 4000 [ 72%] (Sampling) ## Chain 3 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 3 Iteration: 3100 / 4000 [ 77%] (Sampling) ## Chain 3 Iteration: 3200 / 4000 [ 80%] (Sampling) ## Chain 3 Iteration: 3300 / 4000 [ 82%] (Sampling) ## Chain 3 Iteration: 3400 / 4000 [ 85%] (Sampling) ## Chain 3 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 3 Iteration: 3600 / 4000 [ 90%] (Sampling) ## Chain 3 Iteration: 3700 / 4000 [ 92%] (Sampling) ## Chain 3 Iteration: 3800 / 4000 [ 95%] (Sampling) ## Chain 3 Iteration: 3900 / 4000 [ 97%] (Sampling) ## Chain 3 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 3 finished in 0.5 seconds. ## Chain 4 Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 4 Iteration: 100 / 4000 [ 2%] (Warmup) ## Chain 4 Iteration: 200 / 4000 [ 5%] (Warmup) ## Chain 4 Iteration: 300 / 4000 [ 7%] (Warmup) ## Chain 4 Iteration: 400 / 4000 [ 10%] (Warmup) ## Chain 4 Iteration: 500 / 4000 [ 12%] (Warmup) ## Chain 4 Iteration: 600 / 4000 [ 15%] (Warmup) ## Chain 4 Iteration: 700 / 4000 [ 17%] (Warmup) ## Chain 4 Iteration: 800 / 4000 [ 20%] (Warmup) ## Chain 4 Iteration: 900 / 4000 [ 22%] (Warmup) ## Chain 4 Iteration: 1000 / 4000 [ 25%] (Warmup) ## Chain 4 Iteration: 1100 / 4000 [ 27%] (Warmup) ## Chain 4 Iteration: 1200 / 4000 [ 30%] (Warmup) ## Chain 4 Iteration: 1300 / 4000 [ 32%] (Warmup) ## Chain 4 Iteration: 1400 / 4000 [ 35%] (Warmup) ## Chain 4 Iteration: 1500 / 4000 [ 37%] (Warmup) ## Chain 4 Iteration: 1600 / 4000 [ 40%] (Warmup) ## Chain 4 Iteration: 1700 / 4000 [ 42%] (Warmup) ## Chain 4 Iteration: 1800 / 4000 [ 45%] (Warmup) ## Chain 4 Iteration: 1900 / 4000 [ 47%] (Warmup) ## Chain 4 Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 4 Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 4 Iteration: 2100 / 4000 [ 52%] (Sampling) ## Chain 4 Iteration: 2200 / 4000 [ 55%] (Sampling) ## Chain 4 Iteration: 2300 / 4000 [ 57%] (Sampling) ## Chain 4 Iteration: 2400 / 4000 [ 60%] (Sampling) ## Chain 4 Iteration: 2500 / 4000 [ 62%] (Sampling) ## Chain 4 Iteration: 2600 / 4000 [ 65%] (Sampling) ## Chain 4 Iteration: 2700 / 4000 [ 67%] (Sampling) ## Chain 4 Iteration: 2800 / 4000 [ 70%] (Sampling) ## Chain 4 Iteration: 2900 / 4000 [ 72%] (Sampling) ## Chain 4 Iteration: 3000 / 4000 [ 75%] (Sampling) ## Chain 4 Iteration: 3100 / 4000 [ 77%] (Sampling) ## Chain 4 Iteration: 3200 / 4000 [ 80%] (Sampling) ## Chain 4 Iteration: 3300 / 4000 [ 82%] (Sampling) ## Chain 4 Iteration: 3400 / 4000 [ 85%] (Sampling) ## Chain 4 Iteration: 3500 / 4000 [ 87%] (Sampling) ## Chain 4 Iteration: 3600 / 4000 [ 90%] (Sampling) ## Chain 4 Iteration: 3700 / 4000 [ 92%] (Sampling) ## Chain 4 Iteration: 3800 / 4000 [ 95%] (Sampling) ## Chain 4 Iteration: 3900 / 4000 [ 97%] (Sampling) ## Chain 4 Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 4 finished in 0.5 seconds. ## ## All 4 chains finished successfully. ## Mean chain execution time: 0.5 seconds. ## Total execution time: 2.4 seconds. precis(m11.8, depth = 2) ## mean sd 5.5% 94.5% n_eff Rhat4 ## a[1] -0.4797363 0.5285767 -1.3203164 0.3408876 729.1377 1.006207 ## a[2] -0.3830486 0.5296107 -1.2254720 0.4399153 733.4548 1.006083 ## delta[1] 1.0610778 0.5311250 0.2296213 1.9013047 735.5432 1.006321 ## delta[2] 1.0155339 0.5333167 0.1859177 1.8641042 734.7407 1.006233 ## delta[3] -0.1999438 0.5302260 -1.0196797 0.6545453 741.6319 1.005926 ## delta[4] -0.2320340 0.5309040 -1.0641837 0.6129361 736.8921 1.006138 ## delta[5] -0.6753254 0.5346639 -1.5106367 0.1861121 741.3245 1.006251 ## delta[6] -2.2335957 0.5444183 -3.1001722 -1.3744923 781.8925 1.005968 post &lt;- extract.samples(m11.8) diff_a &lt;- post$a[,1] - post$a[,2] diff_p &lt;- inv_logit(post$a[,1]) - inv_logit(post$a[,2]) precis(list(diff_a=diff_a, diff_p=diff_p)) ## mean sd 5.5% 94.5% histogram ## diff_a -0.09668761 0.07982130 -0.22331892 0.033050385 ▁▁▁▁▂▅▇▇▅▂▁▁▁ ## diff_p -0.02184294 0.01833935 -0.05147136 0.007452741 ▁▁▁▁▁▃▅▇▇▅▂▁▁▁▁ So within departments males actually have a tiny 2% reduction in admission probability. pg &lt;- with(dat_list, sapply(1:6, function(k) applications[dept_id==k]/sum(applications[dept_id==k]))) rownames(pg) &lt;- c(&quot;male&quot;, &quot;female&quot;) colnames(pg) &lt;- unique(d$dept) round(pg,2) ## A B C D E F ## male 0.88 0.96 0.35 0.53 0.33 0.52 ## female 0.12 0.04 0.65 0.47 0.67 0.48 postcheck(m11.8) ##Poisson regression "],["monsters-and-mixtures.html", "Chapter 12 Monsters and mixtures", " Chapter 12 Monsters and mixtures "],["models-with-memory.html", "Chapter 13 Models with memory", " Chapter 13 Models with memory "],["adventures-in-covariance.html", "Chapter 14 Adventures in covariance", " Chapter 14 Adventures in covariance "],["missing-data-and-other-opportunities.html", "Chapter 15 Missing data and other opportunities", " Chapter 15 Missing data and other opportunities "],["generalized-linear-madness.html", "Chapter 16 Generalized linear madness", " Chapter 16 Generalized linear madness "],["horoscopes.html", "Chapter 17 Horoscopes", " Chapter 17 Horoscopes "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
