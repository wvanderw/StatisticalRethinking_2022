[["ulysses-compass.html", "Chapter 7 Ulysses compass 7.1 The problem with parameters 7.2 Entropy and accuracy", " Chapter 7 Ulysses compass The hero of Homers Odyssey, Ulysses, had two navigate between two dire consequences to complete his voyage. On one side, a beast with many heads and a taste for men, on the other side, a sea monster that sank boats. In statistical modelling, we are navigating equally treacherous waters while trying to avoid two things Overfitting Underfitting Both dangers provide the same result, poor predictions. All this while also avoiding confounds as weve seen. 7.1 The problem with parameters Most people will describe the performance of a model by the \\(R^2\\) term or amount of variance explained \\[R^{2} = \\frac{\\text{var(outcome)} - \\text{var(residuals)}} {\\text{var(outcome)}} = 1 - \\frac{\\text{var(residuals)}}{\\text{var(outcome)}}\\] But \\(R^2\\) is misleading because adding in more predictor variables almost always improves model fit, even if they are a bunch of random numbers that arent related to the outcome. 7.1.1 More parameters (almost) always improve fit Unfortunately overfitting happens automatically. To see this in action, we will look at seven hominin species average body mass and brain volume. sppnames &lt;- c(&quot;afarensis&quot;, &quot;aricanus&quot;, &quot;habilis&quot;,&quot;boisei&quot;,&quot;rudolfensis&quot;, &quot;ergaster&quot;,&quot;sapiens&quot;) brainvolcc &lt;- c(438, 452, 612, 521, 752, 871, 1350) masskg &lt;- c(37.0, 35.5, 34.5, 41.5, 55.5, 61, 53.5) d &lt;- data.frame(species=sppnames, brain=brainvolcc, mass=masskg) If we were interested in brain volume as a function of body mass, we could fit a polynomial regression (see ch.Â 4). But first we would have to standardize the parameters. Here we do the same as we normally do with the predictor, body mass. The outcome, brain volume, we have in proportion of the largest brain. This will avoid negative brain values. d$mass_std &lt;- (d$mass - mean(d$mass))/sd(d$mass) d$brain_std &lt;- d$brain / max(d$brain) Lets build the linear model first. We will consider \\(\\sigma\\) to be log-normal to keep it positive. We will also include rather vague priors for \\(\\alpha\\) and \\(\\beta\\). library(rethinking) ## Loading required package: rstan ## Loading required package: StanHeaders ## Loading required package: ggplot2 ## rstan (Version 2.21.5, GitRev: 2e1f913d3ca3) ## For execution on a local, multicore CPU with excess RAM we recommend calling ## options(mc.cores = parallel::detectCores()). ## To avoid recompilation of unchanged Stan programs, we recommend calling ## rstan_options(auto_write = TRUE) ## Do not specify &#39;-march=native&#39; in &#39;LOCAL_CPPFLAGS&#39; or a Makevars file ## Loading required package: cmdstanr ## This is cmdstanr version 0.5.3 ## - CmdStanR documentation and vignettes: mc-stan.org/cmdstanr ## - Use set_cmdstan_path() to set the path to CmdStan ## - Use install_cmdstan() to install CmdStan ## Loading required package: parallel ## rethinking (Version 2.21) ## ## Attaching package: &#39;rethinking&#39; ## The following object is masked from &#39;package:rstan&#39;: ## ## stan ## The following object is masked from &#39;package:stats&#39;: ## ## rstudent m7.1 &lt;- quap( alist( brain_std ~ dnorm(mu, exp(log_sigma)), mu &lt;- a + b*mass_std, a ~ dnorm(0, 0.5), b ~ dnorm(0, 10), log_sigma ~ dnorm(0, 1) ), data = d ) Rethinking OLS and Bayesian m7.1_OLS &lt;- lm(brain_std ~ mass_std, data = d) post &lt;- extract.samples(m7.1_OLS) Now lets calculate the \\(R^{2}\\) value for our linear model set.seed(11) s &lt;- sim(m7.1) r &lt;- apply(s, 2, mean) - d$brain_std resid_var &lt;- var2(r) outcome_var &lt;- var2(d$brain_std) 1 - resid_var/outcome_var ## [1] 0.4731086 And lets write a function that we can quickly apply to other models R2_is_bad &lt;- function(quap_fit){ s &lt;- sim(quap_fit, refresh = 0) r &lt;- apply(s, 2, mean) - d$brain_std 1 - var2(r)/var2(d$brain_std) } This is our second degree polynomial. It is just adding a secondary predictor that is based on the same data as m7.1. m7.2 &lt;- quap( alist( brain_std ~ dnorm(mu, exp(log_sigma)), mu &lt;- a + b[1]*mass_std + b[2]*mass_std^2, a ~ dnorm(0.5,1), b ~ dnorm(0, 10), log_sigma ~ dnorm(0, 1) ), data = d, start = list(b=rep(0,2)) ) Note: quap() needs to know how many \\(\\beta\\) values you have so we set this with a start list Now we will write additinal higher order polynomials, each increasing in degree m7.3 &lt;- quap( alist( brain_std ~ dnorm(mu, exp(log_sigma)), mu &lt;- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3, a ~ dnorm(0.5,1), b ~ dnorm(0, 10), log_sigma ~ dnorm(0, 1) ), data = d, start = list(b = rep(0,3)) ) m7.4 &lt;- quap( alist( brain_std ~ dnorm(mu, exp(log_sigma)), mu &lt;- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3 + b[4]*mass_std^4, a ~ dnorm(0.5,1), b ~ dnorm(0, 10), log_sigma ~ dnorm(0, 1) ), data = d, start = list(b = rep(0,4)) ) m7.5 &lt;- quap( alist( brain_std ~ dnorm(mu, exp(log_sigma)), mu &lt;- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3 + b[4]*mass_std^4 + b[5]*mass_std^5, a ~ dnorm(0.5,1), b ~ dnorm(0, 10), log_sigma ~ dnorm(0, 1) ), data = d, start = list(b = rep(0, 5)) ) \\(\\sigma\\) here needs to be replaced with a constant to get an output. m7.6 &lt;- quap( alist( brain_std ~ dnorm(mu,0.001), mu &lt;- a + b[1]*mass_std + b[2]*mass_std^2 + b[3]*mass_std^3 + b[4]*mass_std^4 + b[5]*mass_std^5 + b[6]*mass_std^6, a ~ dnorm(0.5,1), b ~ dnorm(0, 10) ), data = d, start = list(b = rep(0,6)) ) Now we can plot all our models. Below is a sample of plot code for a single model. You can also use brain_plot from the rethinking package. #7.1 post &lt;- extract.samples(m7.1) mass_seq &lt;- seq(from = min(d$mass_std), to = max(d$mass_std), length.out = 100) l &lt;- link(m7.1, data = list(mass_std=mass_seq)) mu &lt;- apply(l, 2, mean) ci &lt;- apply(l, 2, PI) plot(brain_std ~ mass_std, data = d, xlab = &#39;body mass (std)&#39;, ylab = &#39;brain volume (cc)&#39;) lines(mass_seq, mu) shade(ci, mass_seq) mtext(paste(&quot;m7.1: R^2 = &quot;, round(R2_is_bad(m7.1), 2))) To get an output for m7.6, sigma had to be a constant small number because the model passes through each point without variance. You can also notice that these high \\(R^2\\) models do very poorly in data gaps. Between 41 and 54 kg, the higher order models go wildly positive, and then steeply dip between 55 and 60 kg. 7.1.2 Too few parameters hurts, too Underfitting does a poor job at describing existing data and also lacks predictive power. To see if a model is underfit, you could remove data points one at a time and see how sensitive the model is to change. Overfit models will be very sensitive to change. Lets plot the linear model and the 3rd degree polynomial for comparison. par(mfrow=c(1,2)) brain_loo_plot(m7.1) brain_loo_plot(m7.4) The (underfit) linear model doesnt change much compared to the wild lines in model m7.3. 7.2 Entropy and accuracy 7.2.1 Firing the weatherperson Accuracy starts with picking the target. We should be worried about:\\ 1. Cost-benefit analysis. What is the cost of being wrong? How much do we win by if we are right? 2. Accuracy in context. How do we judge accuracy in a way that accounts for how much a model could imrpove prediction. Suppose in Vancouver, CBC puts out a 10 day forecast that has uncertain predictions of rain. And say CTV says theirs is better and only predicts sunshine for the 10 days. CBC forecast Day &lt;- as.character(1:10) Prediction &lt;- rep(c(1, 0.6), times = c(3, 7)) Observed &lt;- rep(c(1, 0), times = c(3,7)) d &lt;- rbind(Day, Prediction, Observed) knitr::kable(d, &#39;html&#39;) Day 1 2 3 4 5 6 7 8 9 10 Prediction 1 1 1 0.6 0.6 0.6 0.6 0.6 0.6 0.6 Observed 1 1 1 0 0 0 0 0 0 0 CTV forecast Prediction &lt;- rep(0, 10) d &lt;- rbind(Day, Prediction, Observed) knitr::kable(d, &#39;html&#39;) Day 1 2 3 4 5 6 7 8 9 10 Prediction 0 0 0 0 0 0 0 0 0 0 Observed 1 1 1 0 0 0 0 0 0 0 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
