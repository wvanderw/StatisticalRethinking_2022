[["small-worlds-and-large-worlds.html", "Chapter 2 Small worlds and large worlds 2.1 The garden of forking data 2.2 Building a model 2.3 Components of the model", " Chapter 2 Small worlds and large worlds Every model has two parts: small world and large world. The small world is within the model itself and the large world is the broader world we want the model to be applied to. In the small world, everything is defined and there isn’t much room for pure surprises. The large world has more opportunities for unforeseen events and because the small world is an incomplete representation of the large world, mistakes are expected. The goal is to create small worlds (models) that approximate reality so that they perform well in the large world. 2.1 The garden of forking data Bayesian inference is the counting and comparing of possibilities. At each point where a ‘decision’ may split the path, bayesian inference evaluates each path and eliminates the paths that are not congruent with the data fed into the model. 2.1.1 Counting possibilities Marble Example There is a bag of four marbles of two colours (blue and white). This means that there could be 5 possibilities (conjectures); 4:0 white, 3:1 white, 2:2 split, 3:1 blue, and 4:0 blue. A sequence of three marbles is pulled from the bag, one at a time, and returned to the bag (repeated sampling) We get blue, white, blue. Considering a 3:1 white scenario, on the first draw you could get a blue marble or three white marble draws Expanding out one more draw (layer) we can expect the same possibilities because the first marble is replaced before the second draw Expanding one more time gives us the final garden of 64 possibilities (43; 4 marbles with 3 draws) Now recall our draws were blue, white, blue so we can trim the paths that are not congruent with the draws We can also trim other possibilities like all white marbles or all blue marbles because we drew both colours from the bag. Putting our 3:1 white, 2:2 split, and 3:1 blue possibilities together would look something like this You can see that there are different numbers of unique paths to get our observed result 3:1 white has 3 paths 2:2 split has 8 paths 3:1 blue has 9 paths We will call these counts our priors. 2.1.2 Combining other information Suppose we make another marble draw and it is blue. We then count the ways each of our marble possibilities could create this new result 3:1 white has 1 path 2:2 split has 2 paths 3:1 blue has 3 paths Mutiplying by the prior counts gives us: 3:1 white has (3x1) 3 paths 2:2 split has (8x2) 16 paths 3:1 blue has (9x3) 27 paths and suggests that our 3:1 blue possibility is more plausible with the new information. Note that prior data and new data don’t have to be of the same type If we knew that the marble factory made the bags of marbles at fixed rates (i.e. there are 3x more 3:1 white bags as there are 3:1 blue bags and 2x as many 2:2 split bags than 3:1 blue bags) we could update our prior knowledge 3:1 white has 3 paths x 3 factory rate = 9 2:2 split has 16 paths x 2 factory rate = 32 3:1 blue has 27 paths x 1 factory rate = 27 Now the 2:2 split bag seems to be the most plausible outcome (by a small margin) 2.1.3 From counts to probability To avoid observation counts from getting quickly out of hand (over a million possible sequences after 10 data points) we need to collapse the information in a way that is easy to manipulate with the data. Continuing our marble example The plausibility of 3:1 white after seeing blue, white, blue is proportional to the ways 3:1 white can produce blue, white, blue * prior plausibility of 3:1 white. In other words if \\(p\\) is the proportion of blue marbles then in a 3:1 white bag \\(p\\) = 0.25 (1/4) And if we call our data (blue, white, blue) \\(D~new~\\) we can write: The plausibility of \\(p\\) after \\(D~new~\\) \\(\\propto\\) ways \\(p\\) can produce \\(D~new~\\) \\(\\times\\) prior plausibility of \\(p\\) We then standardize the plausibility of \\(p\\) after \\(D~new~\\) by dividing by the sum of the products to make the sum of plausibility to equal 1 \\[\\begin{equation} \\text{The plausibility of } p \\text{ after } D~new~ = \\frac{\\text{ways } p \\text{ can produce } D~new~ \\times \\text{prior plausibility of } p}{\\text{sum of products}} \\end{equation}\\] If you recall our first count of the paths to obtain our observations we had: 3:1 white has 3 paths 2:2 split has 8 paths 3:1 blue has 9 paths To illustrate the standardization of plausibility in R: ways &lt;- c(3, 8, 9) ways/sum(ways) ## [1] 0.15 0.40 0.45 We can now think of these plausibilities as probabilities that sum to 1 New terms: Parameter : \\(p\\) or the conjectured proportion of blue marbles. (indexing ways to explain the data) Likelihood : The relative number of ways that a parameter value (\\(p\\)) can produce the data Prior probability : The prior plausibility of any \\(p\\) value Posterior probability : The new, updated plausibility of and \\(p\\) value 2.2 Building a model Globe example You have a globe and want to know how much surface is covered in water. You throw it in the air multiple times and when you catch it, what lies under your right index finger is recorded. The first nine throws are: W L W W W L W L W with W meaning water and L meaning land. The ratio of water to land is 6:3 (2:1). This will be our data Steps to design your model: Data story: Motivate the model by narrating how the data might arise Update: Educate your model by feeding it data Evaluate: All statistical models require supervision, leading to model revision 2.2.1 A data story You can be descriptive in your data story by defining associations that can be used to predict outcomes. Or you can have a causal story where some events produce others. Typically casual stories are also descriptive. A good place to start is restating the sampling process: The true proportion of water covering the globe is \\(p\\) A single toss of the globe has probability \\(p\\) of producing a water (W) observation. Land probability is \\(1 - p\\) Each toss is independent of the others 2.2.2 Bayesian updating A Bayesian model must start with one set of plausibilities for each possible scenario or Prior plausibilities. The model then updates the plausibilities using the input of the data and creates the Postierior plausibilties. For the globe example, the plausibility for each \\(p\\) value is set to be the same. So before the first draw, the model assumes an equal uniform plausibility for any proportion of water (\\(p\\)) shown as the dashed line below. After the first draw (W), the model updates its plausibility of water proportion to have a very unlikely chance that there is almost no water and a high chance there is a lot of water (solid line below) because there is no known land For each additional data point (toss) we see the model adjust its plausibility expectations. After the second toss (L), the model adjusts the plausibility of \\(p\\) to be highest at 0.5 or 1/2 as we have seen a proportion of 0.5 in the data so far As we add observations, W observations will shift the plausibility peak of \\(p\\) to the right and L observations will pull it back to the left Note that as observations are added, the height of the peak increases as fewer values of \\(p\\) gain plausibility 2.2.3 Evaluate To ensure that the model is behaving as it should to be applicable to the large world, it should be always be checked. The model’s certainty is not the same as accuracy. As you increase the amount of tosses of the globe the model will become more and more certain of the plausibility of a \\(p\\) value (a narrow and tall curve). This could be an artifact of the model and could look very different under another model. Also, be sure to supervise and critique your model’s work as it can’t supervise itself. 2.3 Components of the model Recall that we have counted a few different things already The number of ways each conjecture (mix of marbles) could produce an observation The accumulated number of ways each conjecture could produce the entire data The initial plausibility of each conjectured cause of the data Here are some components related to these things that will help us understand what is happening within the models we build 2.3.1 Variables Variables are the things we want to infer. They can be proportions, rates, even the data itself. In the globe example we had three variables \\(p\\), the proportion of water, our target of inference This value was ‘unobservable’ so it could be labled as a paramter Observed counts of water (W) Observed counts of land (L) 2.3.2 Definitions We must define our variables and how they relate to each other. 2.3.2.1 Observed variables To determine the plausibility of the data to give the target of inference (\\(p\\)), we must define a distribution function for each observed variable which we will call a likelihood. For the globe example we are only concerned with W and L as they are the only events that can occur for each toss. We also need to make assumptions: Each toss (sample) is independent of the others The probability of getting W is the same for each sample (allowing re-sampling) For instances that are either TRUE or FALSE (0 or 1) we can use something called the binomial distribution. (Also called the ‘coin tossing’ distribution). The probability of observing can be expressed as: \\[\\begin{equation} \\text{Pr}(W, L|p) = \\frac{(W + L)! {W!L!}p^W^(1 - p)^L^ \\end{equation}\\] dbinom(6, size = 9, prob = 0.5) ## [1] 0.1640625 2.3.2.2 Unobserved variables "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
