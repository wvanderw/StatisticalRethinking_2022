# Sampling the imaginary

### Probabilities vs. Frequency counts

*Vampires in the population*
_Probabilities_
There is a blood test that can correctly identify a vampire 95% of the time. Or mathematically, Pr(positive|vampire) = 0.95. 1% of the time the test gives a false positive or Pr(positive|mortal) = 0.01. Also, vampires are rare in the population only making up 0.1% of the population so, Pr(vampire) = 0.001\
If someone tests poisitve, what is the probability that they are actually a vampire?\

Using Bayes' theorem, Pr(vampire|positive) can be inverted as:

\begin{equation} 
  \text{Pr(vampire|positive)} = \frac{\text{Pr(positive|vampire)} \times \text{Pr(vampire)} {\text{Pr(positive)}}
\end{equation}

Here, Pr(positive) is the average probability of a positive test result or:

\begin{eqaution}
 \text{Pr(positive)} = \text{Pr(positive|vampire)} \times \text{Pr(vampire)} + \text{Pr(positive|mortal)} \times (1 - \text{Pr(vampire)})
 \end{equation}


```{r, Rcode 3.1}
Pr_Positive_Vampire <- 0.95
Pr_Positive_Mortal <- 0.01
Pr_Vampire <- 0.001

Pr_Positive <- Pr_Positive_Vampire * Pr_Vampire + Pr_Positive_Mortal * (1 - Pr_Vampire)

(Pr_Vampire_Positive <- Pr_Positive_Vampire * Pr_Vampire / Pr_Positive)
```
There is an 8.7% chance that a positive test result is actually a vampire.\

_Frequency counts_

1. In 100,000 people, 100 are vampires
2. of the 100 vampires, 95 will test positive
3. of the 99,900 mortals, 999 will test positive

```{r}
Pr_Positive_Vampire <- 95 / 100
Pr_Positive_Mortal <- 999 / 99900
Pr_Vampire <- 100 / 100000

Pr_Positive <- 95 + 999

(Pr_Vampire_Positive <- Pr_Positive_Vampire * 100 / Pr_Positive)
```
OR:

```{r}
Pr_Positive_Vampire <- 95 #positive results from vampires
Pr_Positive <- 95 + 999 #all positive results

(Pr_Vampire_Positive <- Pr_Positive_Vampire / Pr_Positive)
```



## Sampling from a grid-approximate posterior

Let's recreate the grid approximation for the globe tossing example:
```{r, Rcode 3.2}
p_grid <- seq(from = 0, to = 1, length.out = 1000) #create 1000 values between 0 and 1
prob_p <- rep(1, 1000) #the uniform prior of 1
prob_data <- dbinom(6, size = 9, prob = p_grid) #The observed tosses (data)
posterior <-prob_data*prob_p #calculate the posterior
posterior <-posterior/sum(posterior) #standardize by dividing by the sum
```

Let's pull some samples from our posterior (n = 10000)
```{r, Rcode 3.3}
samples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE) #notice replace set to true as there are only 1000 values in p_grid

#find the mode of your samples
getmode <- function(x) {
   uniqv <- unique(x)
   uniqv[which.max(tabulate(match(x, uniqv)))]
}
samples_mode <- getmode(samples)
```

And plot them
```{r, Rcode 3.4}
plot(samples) #left panel of figure 3.1
abline(h = samples_mode, col = 'red')
```

View the samples as a density
```{r, Rcode 3.5}

plot(density(samples), main = "Density of samples from posterior")
polygon(density(samples), col = 'black', border = 'blue')
abline(v = samples_mode, col = 'red')
#library(rethinking)
#dens(samples)
```

## Sampling to summarize

Common questions about your posterior:
1. intervals of _defined boundaries_
2. intervals of _defined probability mass_
3. _point estimates_

### Intervals of defined boundaries

What is the posterior probability that the proportion of water is >0.5?
```{r, Rcode 3.6}
sum(posterior[p_grid < 0.5])
```
about 17%

Doing the same using the samples from the posterior
```{r, Rcode 3.7}
sum(samples < 0.5) / 1e4 #divide by the number of samples you gathered
```
what about between 0.5 and 0.75?
```{r, Rcode 3.8}
sum(samples > 0.5 & samples < 0.75) / 1e4
```
Recreating figure 3.2 (upper left panel)
```{r}
library(ggplot2)
library(dplyr)

df <- tibble(p_grid, posterior)

df %>% ggplot(aes(x = p_grid)) +
  geom_line(aes(y = posterior)) +
  geom_ribbon(data = df %>% filter(p_grid < 0.5),
              aes(ymin = 0, ymax = posterior)) +
  labs(x = "proportion of water (p)",
       y = "density") +
  theme_bw()

```
upper right panel
```{r}

df %>% ggplot(aes(x = p_grid)) +
  geom_line(aes(y = posterior)) +
  geom_ribbon(data = df %>% filter(p_grid < 0.75 & p_grid > 0.5),
              aes(ymin = 0, ymax = posterior)) +
  labs(x = "proportion of water (p)",
       y = "density") +
  theme_bw()
```


### intervals of defined mass

finding the lower 80% of the probability mass using samples
```{r, Rcode 3.9}
(q_80 <- quantile(samples, 0.8))
```

finding the middle 80% (10-90%)
```{r, Rcode 3.10}
(q_10_90 <- quantile(samples, c(0.1, 0.9)))
```


Bottom panels to figure 3.2
lower left:
```{r}
df %>% ggplot(aes(x = p_grid)) +
  geom_line(aes(y = posterior)) +
  geom_ribbon(data = df %>% filter(p_grid < q_80),
              aes(ymin = 0, ymax = posterior)) +
  annotate(geom = 'text', x = 0.25, y = 0.0025,
           label = 'lower 80%') +
  labs(x = "proportion of water (p)",
       y = "density") +
  theme_bw()
```

```{r}
df %>% ggplot(aes(x = p_grid)) +
  geom_line(aes(y = posterior)) +
  geom_ribbon(data = df %>% filter(p_grid > q_10_90[1] & p_grid < q_10_90[2]),
              aes(ymin = 0, ymax = posterior)) +
  annotate(geom = 'text', x = 0.25, y = 0.0025,
           label = 'middle 80%') +
  labs(x = "proportion of water (p)",
       y = "density") +
  theme_bw()
```



example of three tosses and three water observations:
```{r, Rcode 3.11}
p_grid <- seq(from = 0, to = 1, length.out = 1000)
prior <- rep(1, 1000)
likelihood <- dbinom(3, size = 3, prob = p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
samples <- sample(p_grid, size = 1e4, replace = TRUE, prob = posterior)
```

Getting the 50% interval
```{r, Rcode 3.12}
quantile(samples, c(0.25, 0.75))

#PI(samples, prob = 0.5) #rethinking package
```

Tidybayes package:
```{r}
library(tidybayes)

median_qi(samples, .width = 0.5)
```

Notice that all methods above give us the same interval from ~70 - ~93
With tidybayes, you can also look for multiple intervals at once:
```{r}
median_qi(samples, .width = c(0.5, 0.8, 0.99))
```



Finding the *Highest Posterior Density Interval* (HPDI)
```{r, Rcode 3.13}
#HPDI(samples, prob = 0.5) #rethinking
```

```{r}
mode_hdi(samples, .width = 0.5) #tidybayes
```
**Note: you can get just the points of refernce by using qi() for quantiles and hdi() for highest density intervals. Useful for plotting**

recreate figure 3.3
left panel
```{r}
df <- tibble(p_grid, posterior)

df %>% 
  ggplot(aes(x = p_grid)) +
  geom_ribbon(data = df %>% filter(p_grid > qi(samples, .width = 0.5)[1] & 
                                     p_grid < qi(samples, .width = 0.5)[2]),
              aes(ymin = 0, ymax = posterior)) +
  geom_line(aes(y = posterior)) +
  labs(subtitle = '50% Percentile interval',
       x = 'proportion of water (p)',
       y = 'density')+
  theme_bw()
```
right panel
```{r}
df %>% 
  ggplot(aes(x = p_grid)) +
  geom_ribbon(data = df %>% filter(p_grid > hdi(samples, .width = 0.5)[1] & 
                                     p_grid < hdi(samples, .width = 0.5)[2]),
              aes(ymin = 0, ymax = posterior)) +
  geom_line(aes(y = posterior)) +
  labs(subtitle = '50% HPDI',
       x = 'proportion of water (p)',
       y = 'density')+
  theme_bw()
```

### Point estimates

How to get a single useful (?) point estimate for your parameter. First option is the _maximum a posteriori_ (MAP).

```{r, Rcode 3.14}
p_grid[which.max(posterior)] 
```
With samples:
```{r, Rcode 3.15}
Mode(samples) #tidybayes

#chainmode(samples, adj = 0.01) #rethinking
```

what about mean or median?
```{r, Rcode 3.16}
mean(samples)

median(samples)
```

Visualize the mean, median, and mode (figure 3.4)

1. create a small data frame
```{r}
(
point_estimates <- 
  bind_rows(
    mean_qi(samples),
    median_qi(samples),
    mode_qi(samples)
  ) %>% 
  select(y, .point) %>% 
  mutate(x = y + c(-0.03, 0.03, -0.03),
         z = c(0.001, 0.0015, 0.0025))
)
```

2. plot
```{r}
df %>% 
  ggplot(aes(x = p_grid)) +
  geom_ribbon(aes(ymin = 0, ymax = posterior),
              fill = "grey75") +
  geom_vline(xintercept = point_estimates$y) +
  geom_text(data = point_estimates, 
            aes(x = x, y = z, label = .point),
            angle = 90) +
  labs(x = "proportion of water (p)",
       y = "density") +
  theme(panel.grid = element_blank())
```


How do we choose between the point estimates? _Loss functions_

If we assume that p = 0.5 then the expected loss would be:
```{r, Rcode 3.17}
sum(posterior * abs(0.5 - p_grid))
```

applying this method to all values of p_grid:
```{r, Rcode 3.18}
loss <- sapply(p_grid, function(d) sum(posterior * abs(d - p_grid)))
```

Now find the p value with the lowest loss
```{r, Rcode 3.19}

p_grid[which.min(loss)]
```

Visualize the loss function
```{r}
min_loss_x <- p_grid[which.min(loss)]
min_loss_y <- loss[which.min(loss)]

df <- tibble(p_grid, loss)

df %>% 
  ggplot(aes(x = p_grid)) +
  geom_ribbon(aes(ymin = 0, ymax = loss), fill = 'grey75') +
  geom_point(aes(x = min_loss_x, y = min_loss_y), size = 3, shape = 21, color = 'blue') +
  labs(x = 'decision',
       y = 'expected proportional loss') +
  theme(panel.grid = element_blank())
```

## Sampling to simulate prediction

McElreath's 5 reasons for posterior simulation:
1. Model design - We can sample from both the posterior and the priors to see how the model behaves\
2. Model checking - simulating implied observations to check the model fit\
3. Software validation - To double check that the software is running as expected, it helps to simulate observations for a known model and try to recover the parameter values\
4. Research design - you can test observations from your hypothesis to test your design. similar to power analysis\
5. Forecasting - Estimates can be used to simulate new predictions, for new cases and future observations

### Dummy data
From the globe tossing example we can use the likelihood function to create dummy data

\begin{equation}
\text{Pr}(W|N,p) = \frac{N!}{ W!(N-W)!}p^W\left(1-p\right)^{N-W}
\end{equation}

If we had two tosses (N = 2), there are 3 possibilities: 0 W, 1W, 2W. So we can compute the probabilities of each with the p value set to 0.7
```{r, Rcode 3.20}
dbinom(0:2, size = 2, prob = 0.7)
```
So we see a 9% chance for 0 W, 42% chance of 1 W, and 49% 2 W. So we can sample from the binomial distribution. For example:
```{r, Rcode 3.21}
rbinom(1 , size = 2, prob = 0.7)
```
This is a single random draw from the described distribution. You can also sample multiples:
```{r, Rcode 3.22}
rbinom(10, size = 2, prob = 0.7)
```

So we can create a large selection of random draws as dummy data and see if the 0s, 1s, and 2s appear in the same proportions as the probabilities above (9%, 42%, and 49%)
```{r, Rcode 3.23}
dummy_w <- rbinom(1e5, size = 2, prob = 0.7)
table(dummy_w) / 1e5
```
Now, let's update the tosses to match all the previous examples (N = 9).
```{r, Rcode 3.24}
dummy_w <- rbinom(1e5, size = 9, prob = 0.7)
n <- c(1:1e5)
df_w <- tibble(n, dummy_w) 

df_w %>% ggplot(aes(x = dummy_w)) +
  geom_histogram(binwidth = 1) +
  scale_x_continuous("dummy water count", breaks = seq(from = 0, to = 9, by = 2)) +
  ylab("frequency") +
  theme(panel.grid = element_blank())

#simplehist(dummy_w, xlab = 'dummy water count') #rethinking
```

### Model checking

#### Did the software work? 
There actually is no way to check if the software is working correctly. You just have to set yourself an acceptable amount of correspondence between the observations (data) and implied predictions.

#### Is the model adequate? 

We need to incorporate the model's posterior distribution (and its uncertainty) with the implied predictions (and their uncertainty) to create a _Posterior Predictive Distribution_. To visulize this, we can recreate McElreath's figure 3.6 below.

```{r}
# number of grid point in p_grid
n <- 1001
# number of W in 9 tosses
n_w <- 6
# number of tosses
n_t <- 9

# make a table that contains the p_grid, prior, and posterior
df <- tibble(
  p_grid = seq(from = 0, to = 1, length.out = n),
  #prior is still flat
  prior = 1) %>% 
  mutate(likelihood = dbinom(n_w, size = n_t, prob = p_grid)) %>% 
  mutate(posterior = (likelihood * prior)/sum(likelihood*prior))

#visualize the posterior distribution with 9 p values to sample from
df %>% 
  ggplot(aes(x = p_grid)) +
  geom_ribbon(aes(ymin = 0, ymax = posterior),
              color = 'grey70', fill = 'grey70') +
  geom_segment(data = . %>% 
                 filter(p_grid %in% c(seq(from = 0.1, to = 0.9, by = 0.1), 3 / 10)),
               aes(xend = p_grid, y = 0, yend = posterior, size = posterior),
               color = 'grey35', show.legend = FALSE) +
  geom_point(data = . %>% 
               filter(p_grid %in% c(seq(from = 0.1, to = 0.9, by = 0.1), 3 / 10)),
             aes(y = posterior)) +
  annotate(geom = 'text', x = 0.08, y = 0.0025, label = 'Posterior probability') +
  scale_size_continuous(range = c(0,1)) +
  scale_x_continuous('probability of water', breaks = c(0:10) / 10) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())
```

```{r}
#show the sampling distributions for each p value above

#number of simulated draws
n_draws <- 1e5

#simulation function
simulate_binom <- function(probability){
  set.seed(11) #reproducible
  rbinom(n_draws, size = 9, prob = probability)
}

# make a table of simulated draws for each probability
df_small <- 
  tibble(probability = seq(from = 0.1, to = 0.9, by = 0.1)) %>% 
  mutate(draws = purrr::map(probability, simulate_binom)) %>% 
  unnest(draws) %>% 
  mutate(label = str_c("p = ", probability))

#create a histogram for each simulated p value
df_small %>% 
  ggplot(aes(x = draws)) +
  geom_histogram(binwidth = 1, center = 0, color = 'grey90', size = 1/10) +
  scale_x_continuous(NULL, breaks = seq(from = 0, to = 9, by = 3)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = 'Sampling distributions') +
  #coord_cartesian(xlim = 0:9) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ label, ncol = 9)
```

```{r}
#show the newly calculated PPD

#number of samples
n_samples <- 1e4

#make sure it can be replicated
set.seed(11)
# sample rows of the original dataframe and calculate a W value for each p_grid value
samples <- df %>% 
  sample_n(size = n_samples, weight = posterior, replace = TRUE) %>% 
  mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9))

#plot the newly created PPD
samples %>% 
  ggplot(aes(x = w)) +
  geom_histogram(binwidth = 1, center = 0, color = 'grey90', size = 1/10) +
  scale_x_continuous('number of water samples', breaks = seq(from = 0, to = 9, by = 3)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle('Posterior predictive distribution') +
  #coord_cartesian(xlim = 0:9, ylim = 0:3000) +
  theme(panel.grid = element_blank())
```

The advantage here is that the predictive distribution is still quite spread out compared to the observed data (w = 6). This is much more informative than if we were to just pick out the mode of the posterior and make implied predictions from that value. This would look like the sampling distribution of p = 0.6 above which would be overconfident.

### Practice with brms

Let's create a PPD with brms

Load the package
```{r}
library(brms)
```

```{r}
brms_3 <- brm(data = list(w = 6),
              family = binomial(link = 'identity'),
              w | trials(9) ~ 1,
              prior(beta(1, 1), class = Intercept),
              seed = 11,
              control = list(adapt_delta = 0.999))
```

Posterior summary of the probability of w
```{r}
posterior_summary(brms_3)['b_Intercept', ] %>% 
  round(digits = 2)
```

Now we can sample draws with ```fitted()``` in the brms package from the posterior
```{r}
f <- fitted(brms_3, summary = FALSE, scale = 'linear') %>% 
  as_tibble() %>% 
  set_names('p')

glimpse(f)
```

As a density:
```{r}
f %>% 
  ggplot(aes(x = p)) +
  geom_density(fill = 'grey50', color = 'grey50') +
  annotate(geom = 'text', x = 0.08, y = 2.5, label = 'Posterior probability') +
  scale_x_continuous('probability of water', breaks = c(0, 0.5, 1), limits = 0:1) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())
```
Now we can use this distribution to simulate samples 
```{r}
#make reproducible
set.seed(11)

#simulate samples
f <- f %>% 
  mutate(w = rbinom(n(), size = n_t, prob = p))

#plot PPD
f %>% 
  ggplot(aes(x = w)) +
  geom_histogram(binwidth = 1, center = 0, color = 'grey90',
                 size = 1/10) +
  scale_x_continuous('number of water samples', breaks = seq(from = 0, to = 9, by =3)) +
  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 1200)) +
  ggtitle('Posterior predictive distribution') +
  theme(panel.grid = element_blank())
```

